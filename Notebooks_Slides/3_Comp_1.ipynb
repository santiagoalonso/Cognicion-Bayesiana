{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Manejo de matrices y tablas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Estadistica y funciones matemáticas\n",
    "import scipy.stats as st\n",
    "from scipy.optimize import fmin\n",
    "from scipy import integrate\n",
    "\n",
    "#Graficas\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import altair as alt\n",
    "from altair_saver import save #ademas instalar en terminal: brew cask install chromedriver\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, HBox, VBox, Layout\n",
    "from graphviz import Source, Digraph\n",
    "import dot2tex as d2t\n",
    "from latex import build_pdf\n",
    "from mpl_toolkits.mplot3d import axes3d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Métodos computacionales para obtener la posterior\n",
    "Santiago Alonso-Díaz, PhD <br>\n",
    "Universidad Javeriana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ideal: soluciones analíticas (formulas)<br>\n",
    "Realidad: muy difícil de conseguir <br>\n",
    "Solución: métodos computacionales (algoritmos)\n",
    "\n",
    "<center><img src=\"img/3_CB/MCMC_robot.svg\" width = \"1000\" height = '1000'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Es una métafora. Muchas preguntas:<br>\n",
    "¿Qué es la montaña? <br>\n",
    "¿Qué entiende el algoritmo por \"abismo\"?<br>\n",
    "¿Dónde se inicia? <br>\n",
    "¿Tiene que ir a las otras montañas?\n",
    "\n",
    "<center><img src=\"img/3_CB/MCMC_robot.svg\" width = \"1000\" height = '1000'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intro. Markov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La data puede no ser normal. Por ejemplo, con varias modas:\n",
    "\n",
    "<center><img src=\"img/3_CB/3modes.svg\" width = \"600\" height = '600'></center>\n",
    "\n",
    "Es data hipótetica, pero ¿qué piensan que puede ser cada moda?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Qué pasa si imponemos un modelo normal no jerárquico?<br>\n",
    " \n",
    "<center><img src=\"img/3_CB/3modes_posterior.svg\" width = \"600\" height = '600'></center>\n",
    "\n",
    "No sirve. Y obtener una expresión matemática puede no ser fácil.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Métodos computacionales nos pueden ayudar a pensar modelos más elaborados.\n",
    "\n",
    "Vamos a centrarnos en algoritmos `MCMC`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Primero recordemos que significa que un proceso sea de tipo `Markov`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Proceso Markov (s: state):\n",
    "$$p(s_1,s_2, s_3, ..., s_n) = p(s_1)p(s_2|s_1)p(s_3|s_2) ... p(s_n|s_{n-1}) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "En lenguaje natural:\n",
    "\n",
    "No hay memoria de toda la secuencia. Solo hay acceso al estado actual para determinar la probabilidad del siguiente estado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Todo proceso Markoviano tiene transiciones de probabilidad de un estado al otro\n",
    "\n",
    "<center><img src=\"img/3_CB/markov1.gv.svg\" width = \"650\" height = '650'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Otro ejemplo\n",
    "\n",
    "<center><img src=\"img/3_CB/Monopolio.png\" width = \"400\" height = '400'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Otro ejemplo ... debate: ¿es el lenguaje Markoviano?\n",
    "\n",
    "<center><img src=\"img/3_CB/markov2.gv.svg\" width = \"650\" height = '650'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayes y Markov (MCMC)\n",
    "\n",
    "| <img src=\"img/2_CB/Thomas_Bayes.gif\" width = \"200\" height = '200'>  | <img src=\"img/3_CB/AAMarkov.jpg\" width = \"200\" height = '200'> |\n",
    "|:---:|:---:|\n",
    "|     |     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Qué tiene que ver esto con una posterior Bayesiana?\n",
    "\n",
    "Un modelo de desigualdad en educación en puntajes debería incluir variables latentes escasas (Quarles, 2020): recursos sociales, académicos, emocionales, y económicos. <br>\n",
    "¿Como tomar una muestra? Muestra: un puntaje, dada las variables. \n",
    "\n",
    "<center><img src=\"img/3_CB/3modes.svg\" width = \"400\" height = '400'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Problema: tomar muestras (valores) de modelos <br>\n",
    "Solución: algoritmo MCMC (las probabilidades de transición y aceptar son el algoritmo)\n",
    "<center><img src=\"img/3_CB/markov3.gv.svg\" width = \"650\" height = '650'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Acá una animación de un algoritmo MCMC en acción. <br>\n",
    "Idea básica: se propone una muestra (markoviana) y algoritmo acepta o rechaza.\n",
    "\n",
    "<center><img src=\"img/3_CB/MCMC_animation.gif\" width = \"651\" height = '650'></center>\n",
    "\n",
    "<p style = 'font-size: 15px'>Fuente: https://blog.stata.com/2016/11/15/introduction-to-bayesian-statistics-part-2-mcmc-and-the-metropolis-hastings-algorithm/ </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Aclaraciones iniciales:\n",
    "* Podemos samplear posteriors no estandarizados (sin el denominador de Bayes).\n",
    "* Usamos logaritmos para evitar over y underflows (productos son sumas en logaritmos).\n",
    "* Pensemos MCMC como una estrategia para visitar \"terreno\", proporcional a su importancia (siguiente diapositiva)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Metafora del político (Kruschke, 2014). \n",
    "\n",
    "Problema: Dar un discurso en el Estadio Campin. La candidata quiere que el estadio se llene proporcional a la población de las localidades.\n",
    "\n",
    "<center><img src=\"img/3_CB/localidades_bogota.jpg\" width = \"550\" height = '550'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Estrategia:\n",
    "\n",
    "1. Empezar a reclutar en alguna localidad aleatoria.\n",
    "\n",
    "2. Para ir a otra, lanzar una moneda norte-sur u oriente-occidente. Otro lanzamiento para subdirección (e.g. norte)\n",
    "\n",
    "3. Si la localidad propuesta tiene más población, ir con certeza. Si tiene menos, ir probabilísticamente. Por ejemplo, si tiene 50% de la población ir con 50% de probabilidad a la nueva localidad, de lo contrario seguir en la actual.\n",
    "\n",
    "4. De forma general, $Prob_{moverse} = min(\\frac{poblacion_{propuesta}}{poblacion_{actual}},1)$\n",
    "\n",
    "<center><img src=\"img/3_CB/localidades_bogota.jpg\" width = \"250\" height = '250'></center>\n",
    "\n",
    "Esta heurística es eficiente en el largo plazo: visitamos las localidades proporcional a su población."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Traducción metáfora:<br>\n",
    "Localidades -> Regiones de la función <br>\n",
    "Población localidad -> Nivel de la función en una región <br>\n",
    "Individuo confirmado -> Muestra\n",
    "\n",
    "<center><img src=\"img/3_CB/3Density.svg\" width = \"650\" height = '650'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hay muchas versiones de MCMC (Gibbs, Metropolis, Hamiltonian). A nivel general, muchas comparten la siguiente estructura:\n",
    "\n",
    "1. Posición inicial (muestra actual)\n",
    "2. Propuesta a donde moverse (muestra propuesta)\n",
    "3. Aceptar/rechazar propuesta basado en que tanto respeta la data y priors.\n",
    "4. Si se acepta, moverse a la propuesta. Si se rechaza, seguir en posición actual.\n",
    "5. Repetir desde paso 1.\n",
    "6. Repetir 1-5 por iteraciones o muestras requeridas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preambulo ejemplos\n",
    "\n",
    "Los lenguajes como Python, R, Julia ya tienen funciones que nos permiten tomar muestras. \n",
    "\n",
    "Pienselas como ladrillos que nos van a permitir tomar muestras de posteriors complicados.\n",
    "\n",
    "Actividad: busque en Internet cómo funcionan las funciones .pdf,.cdf,.rvs de scipy.stats:\n",
    "* scipy.stats.norm.pdf; scipy.stats.norm.cdf; scipy.stats.norm.rvs\n",
    "* scipy.stats.multivariate_normal.pdf; scipy.stats.multivariate_normal.cdf; scipy.stats.multivariate_normal.rvs\n",
    "* scipy.stats.beta.pdf; scipy.stats.beta.cdf; scipy.stats.beta.rvs\n",
    "* scipy.stats.dirichlet.pdf; scipy.stats.dirichlet.cdf; scipy.stats.dirichlet.rvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ejemplo 1 MCMC: Gibbs Sampler\n",
    "\n",
    "Vamos a ver dos ejemplos: Gibbs y Metropolis.\n",
    "\n",
    "En la práctica, usaremos rutinas MCMC en paquetes python (PyMC, Edward). Pero vale la pena ver estos dos ejemplos por: \n",
    "* Clásicos\n",
    "* Generar intuición de métodos MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La imagen muestra la distribución real de unas variables <br>\n",
    "El primer método MCMC a ver, para tomar muestras de espacios multidimensionales similares, es Gibbs. <br>\n",
    "Veamos el caso multivariado normal.\n",
    "\n",
    "<center><img src=\"img/3_CB/bivariate_normal.svg\" width = \"400\" height = '400'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Enfoquemonos en dos dimensiones: p(var1, var1). \n",
    "\n",
    "Pongamosle nombres a las variables: p(confianza, impaciencia). La imagen muestra que hay correlación negativa (e.g. Kidd, 2013).\n",
    "\n",
    "<center><img src=\"img/3_CB/bivariate_normal.svg\" width = \"401\" height = '400'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La idea básica detrás de Gibbs:\n",
    "* Suponga que conocemos pero es difícil tomar muestras de la distribución conjunta $p(confianza, impaciencia)$\n",
    "\n",
    "* Suponga que es fácil tomarlas de las condicionadas: $p(confianza|impaciencia)$ y de $p(impaciencia|confianza)$. Por ejemplo, ambas son normales.\n",
    "\n",
    "* El algoritmo hace lo siguiente:\n",
    "    1. Pone valores iniciales a confianza y paciencia.\n",
    "    2. Toma una muestra aleatoria de $p(confianza|impaciencia)$,\n",
    "    3. Luego una de $p(impaciencia|confianza)$, \n",
    "    4. Repite 2 y 3 hasta tener suficientes muestras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Para Gibbs necesitamos:\n",
    "* Conjunta:\n",
    "\n",
    "\\begin{equation}\n",
    "p(conf, impa) \\sim N\\left( \n",
    "\\begin{bmatrix}\n",
    "\\mu_{conf} \\\\\n",
    "\\mu_{impa} \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 & \\rho_{conf,impa} \\\\\n",
    "\\rho_{conf,impa} & 1 \n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "\\end{equation}\n",
    "\n",
    "* Condicionales <span style = 'font-size: 10px'> (no lo demostramos acá, pero esta es la formula de las condicionadas para una multivariada normal)</span> \n",
    "<br><br>\n",
    "\n",
    "\\begin{equation}\n",
    "p(conf|impa) \\sim N(\\mu_{conf} + \\rho_{conf,impa}(impa-\\mu_{impa}),1-\\rho_{conf,impa}^2))\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "p(impa|conf) \\sim N(\\mu_{impa} + \\rho_{conf,impa}(conf-\\mu_{conf}),1-\\rho_{conf,impa}^2))\n",
    "\\end{equation}\n",
    "\n",
    "Ahora solo resta aplicar el algoritmo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Ejemplo Gibbs sampler para multivariada normal bidimensional\n",
    "#Parametros joint distribution\n",
    "mean_joint = [6, 25] #[impaciencia, confianza]\n",
    "corr = -0.6\n",
    "cov = [[1, corr], [corr, 1]]\n",
    "\n",
    "#Paso 1: valores iniciales\n",
    "  = st.uniform.rvs(20,30,1)\n",
    "impa_samp = st.uniform.rvs(0,10,1)\n",
    "niter = 200 #número de iteraciones\n",
    "joint = pd.DataFrame({'impaciencia': np.repeat(float('nan'),niter),\n",
    "                     'confianza': np.repeat(float('nan'),niter)})\n",
    "for i in range(niter):\n",
    "    #Paso 2: samplear un valor aleatorio de confianza\n",
    "    conf_samp = st.norm.rvs(mean_joint[1]+corr*(impa_samp-mean_joint[0]),1-corr**2,1)\n",
    "\n",
    "    #Paso 3: samplear un valor aleatorio de impaciencia\n",
    "    impa_samp = st.norm.rvs(mean_joint[0]+corr*(conf_samp-mean_joint[1]),1-corr**2,1)\n",
    "\n",
    "    joint.loc[i,'impaciencia'] = impa_samp\n",
    "    joint.loc[i,'confianza'] = conf_samp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El algoritmo hace un buen recorrido de todo el espacio\n",
    "\n",
    "<center><img src=\"img/3_CB/Gibbs_animation-min.gif\" width = \"401\" height = '400'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "En términos generales, con Gibbs:\n",
    "\n",
    "* Se puede tomar muestras de cualquier distribución conjunta $p(\\theta_1, \\theta_2, ... \\theta_n, data)$. No solo normal, lo anterior fue un ejemplo.\n",
    "\n",
    "* Se necesita conocer las probabilidades condicionales de todas las variables, dado las demás.\n",
    "\n",
    "* Es markoviano: el siguiente \"paso\" depende del estado actual (muestra actual). \n",
    "\n",
    "* Ventaja: siempre se aceptan las propuestas. Desventaja: hay que saber los condicionales\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ejemplo 2 MCMC: Metropolis & Metropolis-Hastings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Gibbs es un caso particular de Metropolis-Hastings. La diferencia general con Gibbs:\n",
    "\n",
    "* Necesita una distribución conocida para proponer valores (muestras) que el algoritmo acepta o rechaza.\n",
    "\n",
    "* En Gibbs esa distribución eran los condicionales. \n",
    "\n",
    "* En Metropolis-Hastings pueden ser otras. De hecho, muchas versiones ni siquiera usan los condicionales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Volvamos a nuestro problema favorito: Queremos saber la probabilidad de ocurrencia de un evento binario a partir de la data (e.g. ganar/perder, categoria 1/categoria 2, culpable / inocente, etc.). \n",
    "\n",
    "Solución: Bayes.\n",
    "\n",
    "\\begin{align}\n",
    "p(\\theta|data) &\\propto p(data|\\theta)p(\\theta)\\\\\n",
    "               &\\propto Binomial(n, z, \\theta)beta(\\alpha,\\beta)\\\\\n",
    "\\end{align}\n",
    "\n",
    "Donde n es lanzamientos, z éxitos, $\\theta$ probabilidad de éxito, $\\alpha$, y $\\beta$ parámetros de la prior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "En una anterior imagen ya vimos el algoritmo Metropolis (el Metropolis-Hastings en siguientes diapositivas). \n",
    "\n",
    "Se proponen valores de una distribución normal que \"salta\". Se calcula un ratio r de la posterior de la propuesta y la actual (sin estandarizar). Se acepta o rechaza con unos criterios.\n",
    "\n",
    "<center><img src=\"img/3_CB/MCMC_animation.gif\" width = \"651\" height = '650'></center>\n",
    "\n",
    "<p style = 'font-size: 15px'>Fuente: https://blog.stata.com/2016/11/15/introduction-to-bayesian-statistics-part-2-mcmc-and-the-metropolis-hastings-algorithm/ </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Sobre la distribución de propuesta q para Metropolis:\n",
    "* Salta cuando se acepte un valor. En esta visualización, el centro de la normal se mueve a la propuesta $\\theta^{*}$ (que fue aceptada).  \n",
    "* Debe ser simétrica: $q(\\theta^{*}|\\theta) = q(\\theta|\\theta^{*})$. Evita sesgos (e.g. saltos mayores desde algunos puntos).\n",
    "\n",
    "<center><img src=\"img/3_CB/MCMC_animation2.gif\" width = \"550\" height = '550'></center>\n",
    "\n",
    "<p style = 'font-size: 15px'>Fuente: https://blog.stata.com/2016/11/15/introduction-to-bayesian-statistics-part-2-mcmc-and-the-metropolis-hastings-algorithm/ </p>\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Metropolis-Hastings (MH) es la version general. No requiere un distribución de propuesta q simétrica. Para compensar, el ratio r de posteriors p se cambia a un ratio de ratios. \n",
    "\n",
    "| Metropolis | Metropolis-Hastings |\n",
    "|:--:|:--:|\n",
    "| \\begin{equation} \\frac{p(\\theta^{*}\\|y)}{p(\\theta\\|y)} \\end{equation} |  \\begin{equation} \\frac{p(\\theta^{*}\\|y) / q(\\theta^{*}\\|\\theta)}{p(\\theta\\|y)/ q(\\theta\\|\\theta^{*})} \\end{equation}  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Para ejemplificar MH, estimemos el promedio y varianza de un proceso gaussiano (e.g. puntajes en una prueba). \n",
    "\n",
    "El estimativo frecuentista es directo: el promedio y varianza de la data. Queremos el bayesiano: la creencia en forma de distribución de posibles parámetros dado los datos.\n",
    "\n",
    "Podemos conseguir la formula dado que es gaussiano, pero tratemos con MH.\n",
    "\n",
    "Primero generemos unos datos ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Metropolis-Hastings\n",
    "# Datos \n",
    "tamano = 50000\n",
    "mu_real = 50 #parámetro a inferir, imagine que no los conocemos.\n",
    "sigma_real = 10 #parámetro a inferir, imagine que no los conocemos.\n",
    "poblacion = np.random.normal(mu_real,sigma_real,tamano)\n",
    "n = 150 #solo observamos n datos de la población\n",
    "data = pd.DataFrame({'muestra': poblacion[np.random.randint(0, tamano, n)]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Graficas usando altair (permite grabarlas a html) (correr celda anterior)\n",
    "#Tutorial en https://altair-viz.github.io/getting_started/overview.html\n",
    "bins = 20\n",
    "bar = alt.Chart(data).mark_bar().encode(\n",
    "    alt.X(\"muestra\", axis=alt.Axis(title=\"Variable\"), bin=alt.Bin(maxbins=bins)),\n",
    "    alt.Y('count()', axis=alt.Axis(title=\"Conteo\"))\n",
    ")\n",
    "\n",
    "den_data = alt.Chart(data).transform_density(#transformaciones a la data\n",
    "    density = 'muestra',\n",
    "    counts = True,\n",
    "    steps=bins\n",
    ").mark_line( #tipo de gráfica\n",
    "    color='red',\n",
    "    opacity=.35,\n",
    "    strokeWidth = 5\n",
    ").encode(#detalles de los ejes\n",
    "    alt.X('value:Q', axis=alt.Axis(title=\"Variable\")),\n",
    "    alt.Y('density:Q', axis=alt.Axis(title=\"Densidad (kde)\")),\n",
    ").properties(width=400, height=300)\n",
    "\n",
    "#both = (den_data).configure_axis(titleFontSize=20, labelFontSize = 15, grid=False) \n",
    "both = (bar).configure_axis(titleFontSize=20, labelFontSize = 15, grid=False)\n",
    "#save(both, \"img/3_CB/Metropolis_data.svg\", method='selenium', webdriver='chrome') # si no sirve, salvar con los 3 punticos\n",
    "#save(both, \"img/3_CB/Metropolis_data.html\") #salva la gráfica en html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Conteos y densidad de la data (kernel estimate)\n",
    "both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ahora recordemos paso a paso el algoritmo:\n",
    "1. Defina distribuciones likelihood y prior\n",
    "2. Defina una distribución de propuesta $q(\\theta)$\n",
    "3. Tome una muestra de $q(\\theta)$. Esa muestra es la propuesta $\\theta^{*}$\n",
    "4. Calcule el ratio de ratios $\\frac{p(\\theta^{*}|y) / q(\\theta^{*}|\\theta)}{p(\\theta|y)/ q(\\theta|\\theta^{*})}$\n",
    "5. Acepte o rechaze $\\theta^{*}$. Si acepta, mueva $q(\\theta)$. Si rechaza no se mueva.\n",
    "6. Repita 1 a 4 por la iteraciones o muestras deseadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "##### Paso 1\n",
    "#Prior & likelihood\n",
    "def log_prior(pars):\n",
    "    #Priors \n",
    "    #    mean: uniforme\n",
    "    #    std. dev: uniform (positiva)\n",
    "    #Supuesto: parámetros son independientes (multiplicar; en log sumar)\n",
    "    #Input: pars[0] = mean, pars[1] = std. dev\n",
    "    #Output: log. density  \n",
    "    log_prob_mu = st.uniform.logpdf(pars[0], -100, 200)\n",
    "    log_prob_sd = st.uniform.logpdf(pars[1], 0, 100)   \n",
    "    return log_prob_sd + log_prob_mu\n",
    "def log_lik_normal(pars,data):\n",
    "    #Input: pars[0] = mean, pars[1] = std. dev, data = muestra\n",
    "    #Output: log. density\n",
    "    return np.sum(st.norm.logpdf(data, pars[0], pars[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "##### Paso 2 & 3 \n",
    "# Distribución de propuesta\n",
    "class propuesta:\n",
    "    #Con MH, no es necesario que sea simétrica.\n",
    "    #Acá usamos diferentes std y densidades (truncada normal y normal)\n",
    "    def __init__(self):\n",
    "        self.scales = [7, 2] #desviaciones estandar   \n",
    "    def rvs(self, pars):\n",
    "        left_lim = 0 #limite para truncar la normal\n",
    "        right_lim = 50\n",
    "        a = (left_lim - pars[1]) / self.scales[1] \n",
    "        b = (right_lim - pars[1]) / self.scales[1]        \n",
    "        rand_var = [st.norm.rvs(pars[0], self.scales[0], size=1), \n",
    "                    st.truncnorm.rvs(a=a, b=b, loc=pars[1], \n",
    "                                     scale=self.scales[1], size=1)]        \n",
    "        return rand_var    \n",
    "    def log_pdf(self, x, pars):\n",
    "        mu_logpdf = st.norm.logpdf(x[0], pars[0], self.scales[0]) \n",
    "        left_lim = 0 #limite para truncar la normal\n",
    "        right_lim = 50\n",
    "        a = (left_lim - pars[1]) / self.scales[1]\n",
    "        b = (right_lim - pars[1]) / self.scales[1]\n",
    "        sd_logpdf = st.truncnorm.logpdf(x[1], a=a, b=b, loc=pars[1], \n",
    "                                        scale=self.scales[1]) \n",
    "        return sd_logpdf + mu_logpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "##### Paso 4 & 5\n",
    "# Ratio de ratios y aceptar\n",
    "def aceptar(x, x_nuevo, data):\n",
    "    # Posteriors (sin estandarizar)\n",
    "    posterior_nuevo = log_prior(x_nuevo)+log_lik_normal(x_nuevo, data) \n",
    "    posterior_actual = log_prior(x)+log_lik_normal(x, data)\n",
    "    # Propuesta\n",
    "    propuesta_nuevo = propuesta().log_pdf(x_nuevo, x)\n",
    "    propuesta_actual = propuesta().log_pdf(x, x_nuevo)\n",
    "    # Ratios\n",
    "    ratio1 = posterior_nuevo - propuesta_nuevo #Resta por que estamos con logaritmos\n",
    "    ratio2 = posterior_actual - propuesta_actual\n",
    "    ratio_de_ratios = ratio1-ratio2 #Resta por que estamos con logaritmos\n",
    "    # Aceptar\n",
    "    u = st.uniform.rvs(0, 1, size = 1)\n",
    "    acceptance_prob = np.min([1, np.exp(ratio_de_ratios)]) #exp para transformar logaritmos\n",
    "    if u<acceptance_prob:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "Metropolis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "##### Paso 6\n",
    "# Repita\n",
    "def metropolis_hastings(par_inicial, iteraciones, data):\n",
    "    x = par_inicial\n",
    "    aceptado = []\n",
    "    rechazado = []   \n",
    "    for i in range(iteraciones):\n",
    "        x_nuevo =  propuesta().rvs(x)  \n",
    "        if aceptar(x, x_nuevo, data):\n",
    "            x = x_nuevo\n",
    "            aceptado.append(x_nuevo)\n",
    "        else:\n",
    "            rechazado.append(x_nuevo)                               \n",
    "    return np.array(aceptado), np.array(rechazado) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "##### Resultados\n",
    "#Cadena 1\n",
    "par_inicial, iteraciones = [data.mean(), data.std()], 5000\n",
    "A1, R1 = metropolis_hastings(par_inicial, iteraciones, data)\n",
    "#Cadena 2\n",
    "par_inicial, iteraciones = [4, 1], 5000\n",
    "A2, R2 = metropolis_hastings(par_inicial, iteraciones, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def my_MH_plot(A1, A2):\n",
    "    print(\"Info. cadena 1.\", \n",
    "      '\\n95% HDI promedio: [', round(np.percentile(A1[:,0],[2.5])[0],2), ',', \n",
    "          round(np.percentile(A1[:,0],[97.5])[0],2),']',\n",
    "      '\\n95% HDI desv. est: [', round(np.percentile(A1[:,1],[2.5])[0],2), ',', \n",
    "          round(np.percentile(A1[:,1],[97.5])[0],2),']',\n",
    "      '\\naccept:', A1.shape[0]/iteraciones)\n",
    "    print(\"\\nInfo. cadena 2. \", \n",
    "      '\\n95% HDI promedio: [', round(np.percentile(A2[:,0],[2.5])[0],2), ',', \n",
    "          round(np.percentile(A2[:,0],[97.5])[0],2),']',\n",
    "      '\\n95% HDI desv. est: [', round(np.percentile(A2[:,1],[2.5])[0],2), ',', \n",
    "          round(np.percentile(A2[:,1],[97.5])[0],2),']',\n",
    "      '\\naccept:', A2.shape[0]/iteraciones)\n",
    "    fig, ax = plt.subplots(2,2, figsize=[7,5])\n",
    "    ax[0,0].plot(np.arange(A1.shape[0]), A1[:,0])\n",
    "    ax[0,0].set_title('Promedio \\n (cadena 1)')\n",
    "    ax[0,1].plot(np.arange(A1.shape[0]), A1[:,1])\n",
    "    ax[0,1].set_title('Desv. Estandar \\n (cadena 1)');\n",
    "    ax[1,0].plot(np.arange(A2.shape[0]), A2[:,0])\n",
    "    ax[1,0].set_title('Promedio \\n (cadena 2)')\n",
    "    ax[1,1].plot(np.arange(A2.shape[0]), A2[:,1])\n",
    "    ax[1,1].set_title('Desv. Estandar \\n (cadena 2)');\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Convergen ambas cadenas. Hay tests pero se puede ver.  \n",
    "# Las trazas gravitan alredor de los mismos valores (pero baja aceptación)\n",
    "my_MH_plot(A1, A2) #Note el \"burn-in\" al comienzo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "data_mcmc = pd.DataFrame({'promedio': np.concatenate((A1[:,0],A2[:,0]))[:,0],\n",
    "                         'desv_est':  np.concatenate((A1[:,1],A2[:,1]))[:,0]})\n",
    "def my_alt_plot(var, ext, title = ''):    \n",
    "    den = alt.Chart(data_mcmc).transform_density(#transformaciones a la data\n",
    "        density = var, extent = ext, counts = True, steps=bins\n",
    "    ).mark_line( #tipo de gráfica\n",
    "        color='green', opacity=.75, strokeWidth = 5\n",
    "    ).encode(#detalles de los ejes\n",
    "        alt.X('value:Q', axis=alt.Axis(title=var)),\n",
    "        alt.Y('density:Q', axis=alt.Axis(title=\"Densidad\")),\n",
    "    ).properties(\n",
    "        width=250, height=250, title = title\n",
    "    )\n",
    "\n",
    "    return den\n",
    "mcmc_den_p = my_alt_plot('promedio', [46,53], title = 'Posterior promedio')\n",
    "mcmc_den_e = my_alt_plot('desv_est', [6.5,12], title = 'Posterior desv. est.')\n",
    "plot = (mcmc_den_p|mcmc_den_e).configure_axis(\n",
    "    titleFontSize=20, labelFontSize = 15, grid=False\n",
    ").configure_title(\n",
    "    fontSize=24\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Resultados (en forma de distribución, ambas cadenas)\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\"Todos los modelos están mal, algunos son útiles\" (Box,1976)\n",
    "\n",
    "¿Predice nuestro modelo la data? <br>\n",
    "*Posterior predictive check* ($y^{rep}$: valor simulado; $y$: data; $\\theta$: parámetros)\n",
    "\n",
    "$$p(y^{rep}|y) = \\int p(y^{rep}|\\theta) p(\\theta|y) d\\theta $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "En lenguaje natural (casi): <br> \n",
    "En la integral, está el likelihood (1er termino) y posterior (2do). La implementamos con valores aleatorios de los parámetros $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Posterior predictive check\n",
    "n_rep = 5000 \n",
    "y_rep = []\n",
    "for n in range(n_rep):\n",
    "    #posterior (2do termino integral)\n",
    "    idx = np.random.randint(data_mcmc.shape[0])\n",
    "    prom_rep = data_mcmc.loc[idx,'promedio']\n",
    "    idx = np.random.randint(data_mcmc.shape[0])\n",
    "    desv_est_rep = data_mcmc.loc[idx,'desv_est']\n",
    "    \n",
    "    #likelihood (1er termino integral)\n",
    "    y_rep.append(st.norm.rvs(prom_rep, desv_est_rep, size=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "data_ppc = pd.DataFrame({'y_rep':np.array(y_rep)[:,0]}) \n",
    "den_ppc = alt.Chart(data_ppc).transform_density(#transformaciones a la data\n",
    "        density = 'y_rep', extent = [20,80], counts = True, steps=bins\n",
    "    ).mark_line( #tipo de gráfica\n",
    "        color='green', opacity=.75, strokeWidth = 5\n",
    "    ).encode(#detalles de los ejes\n",
    "        alt.X('value:Q', axis=alt.Axis(title='y_rep')),\n",
    "        alt.Y('density:Q', axis=alt.Axis(title=\"Densidad (kde)\")),\n",
    "    ).properties(\n",
    "        width=250, height=250, title = 'PPC'\n",
    "    )\n",
    "plot_ppc_data = (den_ppc | den_data.properties(\n",
    "    width=250, height=250, title = 'Data')\n",
    ").configure_axis(\n",
    "    titleFontSize=20, labelFontSize = 15, grid=False\n",
    ").configure_title(\n",
    "    fontSize=24\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot_ppc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Metropolis-Hastings siempre converge a la posterior?\n",
    "\n",
    "Respuesta corta: sí, el algoritmo converge cuando se cadena se hace bien (aunque puede demorarse). Respuesta larga, ver capitulo 7, Kruschke, 2da edición. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Metropolis-Hastings siempre converge a la posterior?\n",
    "\n",
    "Intuición. Tenemos la posterior $\\theta=0.15$. Esta cadena converge a la posterior.\n",
    "\n",
    "<center><img src=\"img/3_CB/mcmc_diagram.svg\" width = \"350\" height = '350'></center>\n",
    "<p style = 'font-size: 10px'>Fuente: https://people.duke.edu/~ccc14/sta-663/MCMC.html </p>\n",
    "\n",
    "Converge si la cadena es:\n",
    "* Irreducible (se puede ir a cualquier estado)\n",
    "* Aperiodica (no nos quedamos en un loop entre estados)\n",
    "* Recurrente (el tiempo de volver a un estado dado es finito)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Herramienta complementaria: gradient descent  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Objetivo: obtener el posterior\n",
    "\n",
    "Solución anterior: método estocástico (Markov + Montecarlo)\n",
    "\n",
    "Solución complementaria: seguir un gradiente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Qué es gradient descent (ascent)?<br>\n",
    "Metáfora alpinista (punto más alto) o urbanista (punto más bajo)\n",
    "\n",
    "<center><img src=\"img/3_CB/3D_panorama.svg\" width = \"500\" height = '500'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El algoritmo para descent (minimizar) se ve algo así:\n",
    "* Calcular el gradiente (pendiente) negativo de la función en la posición actual: $\\nabla f(x_k)$\n",
    "\n",
    "* Moverse en dirección del gradiente negativo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Veamos la implementación de github.com/dtnewman/ con:\n",
    "\n",
    "$$f(x) = x^3 - 2x^2+2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=[5,4.5]);\n",
    "f = lambda x: x**3-2*x**2+2  \n",
    "x = np.linspace(-1,2.5,1000)\n",
    "ax.plot(x, f(x), lw = 3);\n",
    "ax.set_title('Encontrar el mínimo (local)', fontsize=25);\n",
    "#fig.savefig(\"img/3_CB/Gradient_Desc.svg\"), plt.close();\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"img/3_CB/Gradient_Desc.svg\" width = \"400\" height = '400'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Código 2D gradient descent github.com/dtnewman/stochastic_gradient_descent/\n",
    "\n",
    "def f_prime(x): #derivada de nuestra función\n",
    "    return 3*x**2-4*x\n",
    "\n",
    "x_old,  x_new = 0, 2.5 #posición inicial\n",
    "n_k = 0.01 #tamaño del paso\n",
    "precision = 0.001\n",
    "x_list, y_list = [x_new], [f(x_new)]    \n",
    "while abs(x_new - x_old) > precision:       \n",
    "    x_old = x_new\n",
    "    s_k = -f_prime(x_old)\n",
    "    x_new = x_old + n_k * s_k #La \"fuerza\" del movimiento depende del gradiente\n",
    "    x_list.append(x_new), y_list.append(f(x_new))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Plot\n",
    "def animate(i, x_list):\n",
    "    idx = int(i)\n",
    "    ax.plot(x_list[i], f(x_list[i]),'ro')\n",
    "    if idx>0:\n",
    "        x = x_list[(idx-1):(idx+1)]\n",
    "        y = [f(x[0]),f(x[1])]\n",
    "        ax.plot(x,y,'r')\n",
    "fig, ax = plt.subplots(1,1, figsize=[7,7]);\n",
    "ax.set_title('Encontrar el mínimo (local)', fontsize=25)\n",
    "x = np.linspace(-0.75,2.5,1000)\n",
    "ax.plot(x, f(x), lw = 3);\n",
    "# call the animator.  blit=True means only re-draw the parts that have changed.\n",
    "anim = animation.FuncAnimation(fig, animate, interval = 500, \n",
    "                               frames = len(x_list), blit=False, fargs = ([x_list]))\n",
    "writer = animation.PillowWriter(fps=5)\n",
    "#anim.save('img/3_CB/Gradient_animation.gif', writer = writer); #Compress online\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"img/3_CB/Gradient_animation.gif\" width = \"350\" height = '350'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Podemos acelerar el algoritmo? \n",
    "\n",
    "Sí. Podemos hacer el paso adaptativo. \n",
    "\n",
    "En el siguiente ejemplo hacemos \"trampa\" con una función minimizadora de Python. Lo importante es ver que se puede ser creativo en cómo explorar el espacio de la función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Pasamos esta función a un minimizador (fmin)\n",
    "def f2(n,x,s):\n",
    "    #n: paso; fmin cambia este para minimizar y\n",
    "    #s: gradiente; x: coordenada x\n",
    "    x = x + n*s #desplazamiento en x\n",
    "    y = f(x) #y (a minimizar)\n",
    "    return y \n",
    "\n",
    "x_old,  x_new = 0, 2.5 #posición inicial\n",
    "n_k_init = 0.1 #tamaño del paso (valor inicial a fmin)\n",
    "x_list, y_list = [x_new], [f(x_new)]\n",
    "while abs(x_new - x_old) > precision:\n",
    "    x_old = x_new\n",
    "    s_k = -f_prime(x_old)   \n",
    "    \n",
    "    # use scipy fmin function to find ideal step size.\n",
    "    n_k = fmin(f2,n_k_init,(x_old,s_k), full_output = False, disp = False)\n",
    "    \n",
    "    x_new = x_old + n_k * s_k\n",
    "    x_list.append(x_new)\n",
    "    y_list.append(f(x_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=[7,7]);\n",
    "ax.set_title('Encontrar el mínimo (local)\\n adaptativo', fontsize=25)\n",
    "x = np.linspace(-0.75,2.5,1000)\n",
    "ax.plot(x, f(x), lw = 3);\n",
    "# call the animator.  blit=True means only re-draw the parts that have changed.\n",
    "anim = animation.FuncAnimation(fig, animate, interval = 500, \n",
    "                               frames = len(x_list), blit=False, fargs = ([x_list]))\n",
    "writer = animation.PillowWriter(fps=5)\n",
    "#anim.save('img/3_CB/Gradient_adapt_animation.gif', writer = writer); #Compress online\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "|<img src=\"img/3_CB/Gradient_animation.gif\" width = \"300\" height = '300'>|<img src=\"img/3_CB/Gradient_adapt_animation.gif\" width = \"300\" height = '300'>|\n",
    "|:-:|:-:|\n",
    "|||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Gradient descent no asegura encontrar mínimos (máximos) globales, solo locales.\n",
    "\n",
    "Podemos buscar óptimos (locales) de cualquier función. Por ejemplo, de una función de minimos cuadrados ordinarios (OLS, por sus siglas en inglés):\n",
    "$${1 \\over m} \\sum\\limits_{i=1}^m (h_\\beta(x_i)-y_i)^2$$\n",
    "\n",
    "Puede haber varios $\\beta$. Es el mismo algoritmo pero con derivadas parciales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Qué tiene que ver con Bayes?\n",
    "\n",
    "La posterior $p(\\theta_1,...\\theta_n|data)$ es una función multidimensional. Hay algoritmos MCMC que explotan gradientes, por ejemplo Hamiltonian MCMC. \n",
    "\n",
    "<center><img src=\"img/3_CB/3D_panorama.svg\" width = \"400\" height = '400'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El MCMC hamiltoniano es popular. Esta es la intuición: recorre el posterior usando gradientes y momentum.\n",
    "\n",
    "<center><img src=\"img/3_CB/Hamiltonian_MCMC_1.svg\" width = \"475\" height = '475'></center>\n",
    "\n",
    "Fuente: Betancourt (2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Variational inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Metáfora pintor (minimizar distancia)\n",
    "<center><img src=\"img/3_CB/triple_self_portrait.jpg\" width = \"400\" height = '400'></center>\n",
    "<p style = 'font-size: 15px'>Fuente: https://displate.com/tonycenteno/displates </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Problema: algunas posterior son difíciles de analizar matemáticamente o recorrer con MCMC.\n",
    "\n",
    "Solución de variational inference: Escoger una distribución lo suficientemente parecida y que sea fácil de samplear. Esa distribución la llaman variational."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Qué significa que dos distribuciones sean parecidas?\n",
    "<center><img src=\"img/3_CB/KL_1.svg\" width = \"600\" height = '600'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Varios criterios. Usaremos divergencia Kullback–Leibler (KL)\n",
    "\n",
    "$$ D_{KL} (q(z)\\|p(z|x)) = \\int_z q(z) log \\left(\\frac{q(z)}{p(z|x)} \\right)dz$$\n",
    "\n",
    "$q(z)$: Candidata (variational) <br>\n",
    "$p(z|x)$: Posterior \n",
    "\n",
    "Positiva y asimétrica: \n",
    "\n",
    "\\begin{equation} D_{KL} (q(z)\\|p(z|x)) \\ne D_{KL} (p(z|x)\\|q(z))\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Es el promedio de $log \\left(\\frac{q(z)}{p(z|x)} \\right)$, ponderando por $q(z)$\n",
    "\n",
    "$$ D_{KL} (q(z)\\|p(z|x)) = \\int_z q(z) log \\left(\\frac{q(z)}{p(z|x)} \\right)dz$$\n",
    "\n",
    "$log \\left(\\frac{q(z)}{p(z|x)} \\right) = log(p(x)) - log(p(z|x))$. Es decir, es el promedio de la diferencia de (log) probabilidades de z. \n",
    "\n",
    "Mide cuanta información se pierde cuando usamos p para aproximar q. \n",
    "\n",
    "Objetivo: minimizar esta cantidad (e.g. con gradient descent).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Ejemplo simple\n",
    "def KL(x, p, q):\n",
    "    return p(x) * np.log( p(x) / q(x) ) \n",
    "p = st.norm(10,2).pdf\n",
    "q = st.norm(18,4).pdf\n",
    "kl_int, err = integrate.quad(KL,-30,30,args=(q,p))\n",
    "print(\"La divergencia KL entre q y p es: \", round(kl_int,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"img/3_CB/KL_2.svg\" width = \"600\" height = '600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Queremos minimizar la divergencia entre la posterior $p$ y la candidata $q$, que es más fácil de samplear. \n",
    "\n",
    "Pero primero, escribamos la divergencia KL de otra forma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\\begin{aligned}\n",
    "D_{\\text{KL}} \\big( q(\\boldsymbol{z}) || p(\\boldsymbol{z} | \\boldsymbol{x}) \\big) &= \\int\\limits_{\\boldsymbol{z}} q(\\boldsymbol{z}) \\log \\Big[ \\frac{q(\\boldsymbol{z})}{p(\\boldsymbol{z} | \\boldsymbol{x})} \\Big] d\\boldsymbol{z} \\\\\n",
    "\\end{aligned}\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Apliquemos propiedad de logaritmos y dividamos la integral\n",
    "\n",
    "\\begin{aligned}\n",
    "&= \\int\\limits_{\\boldsymbol{z}} \\Big[ q(\\boldsymbol{z}) \\log q(\\boldsymbol{z}) \\Big] d\\boldsymbol{z} - \\int\\limits_{\\boldsymbol{z}} \\Big[ q(\\boldsymbol{z}) \\log p(\\boldsymbol{z} | \\boldsymbol{x}) \\Big] d\\boldsymbol{z}  \n",
    "\\end{aligned}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Son promedios, ponderados por $q(z)$\n",
    "\n",
    "\\begin{aligned}\n",
    "&= \\mathbb{E}_{q} \\big[ \\log q(\\boldsymbol{z}) \\big] - \\mathbb{E}_{q} \\big[ \\log p(\\boldsymbol{z} | \\boldsymbol{x}) \\big] \n",
    "\\end{aligned}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Apliquemos la definición de probabilidad condicional\n",
    "\n",
    "\\begin{aligned}\n",
    "&= \\mathbb{E}_{q} \\big[ \\log q(\\boldsymbol{z}) \\big] - \\mathbb{E}_{q} \\big[ \\log p(\\boldsymbol{z} | \\boldsymbol{x}) \\big] \\\\\n",
    "&= \\mathbb{E}_{q} \\big[ \\log q(\\boldsymbol{z}) \\big] - \\mathbb{E}_{q} \\bigg[ \\log \\Big[ \\frac{p(\\boldsymbol{x}, \\boldsymbol{z}) }{p(\\boldsymbol{x})} \\Big] \\bigg] \n",
    "\\end{aligned}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Propiedad de logaritmos \n",
    "\n",
    "\\begin{aligned}\n",
    "&= \\mathbb{E}_{q} \\big[ \\log q(\\boldsymbol{z}) \\big] - \\mathbb{E}_{q} \\big[ \\log p(\\boldsymbol{x}, \\boldsymbol{z}) \\big] + \\mathbb{E}_{q} \\big[ \\log p(\\boldsymbol{x}) \\big]\n",
    "\\end{aligned}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\mathbb{E}_{q} \\big[ \\log p(\\boldsymbol{x}) \\big] = \\log p(\\boldsymbol{x})$ i.e. el prior de x (i.e. data) no depende de q(z) por definición\n",
    "\n",
    "\\begin{aligned}\n",
    "&= \\mathbb{E}_{q} \\big[ \\log q(\\boldsymbol{z}) \\big] - \\mathbb{E}_{q} \\big[ \\log p(\\boldsymbol{x}, \\boldsymbol{z}) \\big] + \\log p(\\boldsymbol{x})\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Tenemos entonces que la divergencia KL es:\n",
    "\n",
    "\\begin{aligned}\n",
    "D_{\\text{KL}} \\big( q(\\boldsymbol{z}) || p(\\boldsymbol{z} | \\boldsymbol{x}) \\big) &= \\mathbb{E}_{q} \\big[ \\log q(\\boldsymbol{z}) \\big] - \\mathbb{E}_{q} \\big[ \\log p(\\boldsymbol{x}, \\boldsymbol{z}) \\big] + \\log p(\\boldsymbol{x})\n",
    "\\end{aligned}\n",
    "\n",
    "Ahora definamos evidence lower bound (ELBO) como una reorganización de los dos primeros términos:\n",
    "\n",
    "\\begin{aligned}\n",
    "ELBO &= \\mathbb{E}_{q} \\big[ \\log p(\\boldsymbol{x}, \\boldsymbol{z}) \\big] - \\mathbb{E}_{q} \\big[ \\log q(\\boldsymbol{z}) \\big] \n",
    "\\end{aligned}\n",
    "\n",
    "La divergencia KL queda así:\n",
    "\n",
    "\\begin{aligned}\n",
    "D_{\\text{KL}} \\big( q(\\boldsymbol{z}) || p(\\boldsymbol{z} | \\boldsymbol{x}) \\big) &= -ELBO + log(p(x)\n",
    "\\end{aligned}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\log p(\\boldsymbol{x})$ es una constante, no afecta la minimización. Es decir, *maximizar ELBO* es lo mismo que minimizar la divergencia KL\n",
    "\n",
    "\\begin{aligned}\n",
    "D_{\\text{KL}} \\big( q(\\boldsymbol{z}) || p(\\boldsymbol{z} | \\boldsymbol{x}) \\big) &= -ELBO + log(p(x)\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "Se llama evidence lower bound por que es un limíte inferior del logaritmo de la evidencia \n",
    "($D_{\\text{KL}}$ siempre es positivo)\n",
    "\n",
    "\\begin{aligned}\n",
    "log(p(x) &= ELBO + D_{\\text{KL}} \\big( q(\\boldsymbol{z}) || p(\\boldsymbol{z} | \\boldsymbol{x}) \\big)\\\\\n",
    "&\\ge ELBO\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La idea es maximizar ELBO respecto a los parámetros z. \n",
    "\n",
    "\\begin{aligned}\n",
    "ELBO &= \\mathbb{E}_{q} \\big[ \\log p(\\boldsymbol{x}, \\boldsymbol{z}) \\big] - \\mathbb{E}_{q} \\big[ \\log q(\\boldsymbol{z}) \\big] \n",
    "\\end{aligned}\n",
    "\n",
    "$p(x,z)$ la tenemos. ¿Cómo escogemos $q(z)$? Mitad ciencia mitad arte. \n",
    "\n",
    "<center><img src=\"img/3_CB/triple_self_portrait.jpg\" width = \"300\" height = '300'></center>\n",
    "<p style = 'font-size: 15px'>Fuente: https://displate.com/tonycenteno/displates </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Una posibilidad es con mean-field variational family. \n",
    "\n",
    "$$ q(z_1, ... z_m) = \\prod_{j=1}^{m} q(z_j)$$\n",
    "\n",
    "Es decir, cada parámetro $z_j$ tiene su propia distribución $q(z_j)$ (e.g. normal) y se asumen independientes.\n",
    "\n",
    "Recordar que $q$ es una aproximación a la posterior. En la verdadera posterior los parámetros sí se relacionan: la posterior es nuestro modelo generativo de los datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "En suma variational inference es:\n",
    "\n",
    "* Un método para encontrar parametros de una distribución fácil de samplear que se parece a la posterior\n",
    "* Necesitamos la posterior p y la candidata (variational) q\n",
    "* Podemos minimizar la divergencia con métodos computacionales (e.g. gradient descent, o expectation maximization; este último no lo vimos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusión\n",
    "\n",
    "* Obtener expresiones matemáticas para la posterior es difícil (incluso imposible)\n",
    "\n",
    "* Usamos metodos computacionales. Vimos dos:\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "|MCMC|Variational Inference|\n",
    "|:-:|:-:|\n",
    "|Insesgada <br> Lenta| Sesgada <br> Rápida|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to slides --SlidesExporter.reveal_theme='solarized' --SlidesExporter.reveal_transition='none' --SlidesExporter.reveal_scroll=True 3_Comp_1.ipynb #Saves slide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "Para salvar las diapositivas a PDF (en Chrome), correr nbconvert para que abra las diapositivas en un servidor local (la transition y el theme son opcionales):\n",
    "\n",
    "!jupyter nbconvert --to slides --SlidesExporter.reveal_theme='solarized' --SlidesExporter.reveal_transition='convex' nombre_de_mi_notebook.ipynb --post serve\n",
    "\n",
    "Luego, a la dirección añadirle ?print-pdf después del .html:\n",
    "\n",
    "http://127.0.0.1:8000/nombre_de_mi_notebook.slides.html?print-pdf\n",
    "\n",
    "Y luego, imprimir y darle salvar como pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Para salvar a pdf\n",
    "!jupyter nbconvert --to slides --SlidesExporter.reveal_theme='solarized' --SlidesExporter.reveal_transition='convex' 3_Comp_1.ipynb --post serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Anexos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "array = np.linspace(0,100,300)\n",
    "mode1 = st.norm.pdf(array, loc=10, scale = 3)\n",
    "mode2 = st.norm.pdf(array, loc=30, scale = 6)\n",
    "mode3 = st.norm.pdf(array, loc=50, scale = 4)\n",
    "\n",
    "distribution = mode1+mode2+mode3\n",
    "fig, ax = plt.subplots(1,1, figsize = [6,5])\n",
    "ax.plot(array, distribution,'k-')\n",
    "ax.set_title('Pruebas para Acceso a la Universidad', \n",
    "            fontsize = 18)\n",
    "ax.set_ylabel('Densidad', fontsize=16)\n",
    "ax.set_xlabel('Puntajes', fontsize=16)\n",
    "#fig.savefig('img/3_CB/3modes.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Correr la anterior celda\n",
    "posterior = st.norm.pdf(array, loc=40, scale = 15)\n",
    "fig, ax = plt.subplots(1,1, figsize = [6,5])\n",
    "ax.plot(array, distribution,'k-', label = 'Data')\n",
    "ax.plot(array, posterior,'r-', label = 'Posterior')\n",
    "ax.set_title('Pruebas para Acceso a la Universidad', \n",
    "            fontsize = 18)\n",
    "ax.set_ylabel('Densidad', fontsize = 16)\n",
    "ax.set_xlabel('Puntajes', fontsize = 16)\n",
    "ax.legend(fontsize = 12)\n",
    "#fig.savefig('img/3_CB/3modes_posterior.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "dot_text = 'digraph G {rankdir=LR;\\\n",
    "                       Descanso -> Estudiar [label=0.1, fontsize=12];\\\n",
    "                       Descanso -> Gimnasio [label=0.3, fontsize=12];\\\n",
    "                       Estudiar -> Descanso [label=0.5, fontsize=12];\\\n",
    "                       Estudiar -> Gimnasio [label=0.2, fontsize=12];\\\n",
    "                       Gimnasio -> Descanso [label=0.75, fontsize=12];\\\n",
    "                       Gimnasio -> Estudiar [label=0.2, fontsize=12];\\\n",
    "                       Descanso -> Descanso [label=0.6, fontsize=12];\\\n",
    "                       Estudiar -> Estudiar [label=0.3, fontsize=12];\\\n",
    "                       Gimnasio -> Gimnasio [label=0.05, fontsize=12];\\\n",
    "                       Descanso [shape=circle];\\\n",
    "                       Estudiar [shape=circle];\\\n",
    "                       Gimnasio [shape=circle];\\\n",
    "                       }' #warning: use single quote at start and end; double quotes for labels\n",
    "src = Source(dot_text)\n",
    "#src.render('img/3_CB/markov1.gv', format ='svg', view=False);\n",
    "#display(src);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "dot_text = 'digraph G {rankdir=LR;\\\n",
    "                       Tengo -> Una [label=0.65, fontsize=12];\\\n",
    "                       Tengo -> Oportunidad [label=0.345, fontsize=12];\\\n",
    "                       Una -> Tengo [label=0.05, fontsize=12];\\\n",
    "                       Una -> Oportunidad [label=0.9, fontsize=12];\\\n",
    "                       Oportunidad -> Tengo [label=0.1, fontsize=12];\\\n",
    "                       Oportunidad -> Una [label=0.85, fontsize=12];\\\n",
    "                       Tengo -> Tengo [label=0.05, fontsize=12];\\\n",
    "                       Una -> Una [label=0.05, fontsize=12];\\\n",
    "                       Oportunidad -> Oportunidad [label=0.05, fontsize=12];\\\n",
    "                       Tengo [shape=circle];\\\n",
    "                       Una [shape=circle];\\\n",
    "                       Oportunidad [shape=circle];\\\n",
    "                       }' #warning: use single quote at start and end; double quotes for labels\n",
    "src = Source(dot_text)\n",
    "#src.render('img/3_CB/markov2.gv', format ='svg', view=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "dot_text = 'digraph G {rankdir=LR;\\\n",
    "                       \"Valor 1\" -> \"Valor 2\" [label=\"??\", fontsize=12];\\\n",
    "                       \"Valor 1\" -> \"Valor n\" [label=\"??\", fontsize=12];\\\n",
    "                       \"Valor 2\" -> \"Valor n\" [label=\"??\", fontsize=12];\\\n",
    "                       \"Valor 1\" -> \"Valor 1\" [label=\"??\", fontsize=12];\\\n",
    "                       \"Valor 2\" -> \"Valor 2\" [label=\"??\", fontsize=12];\\\n",
    "                       \"Valor n\" -> \"Valor n\" [label=\"??\", fontsize=12];\\\n",
    "                       \"Valor 1\" [shape=circle];\\\n",
    "                       \"Valor 2\" [shape=circle];\\\n",
    "                       \"Valor n\" [shape=circle];\\\n",
    "                       }' #warning: use single quote at start and end; double quotes for labels\n",
    "src = Source(dot_text)\n",
    "#src.render('img/3_CB/markov3.gv', format ='svg', view=False);\n",
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[8,6])\n",
    "ax = fig.gca(projection='3d')\n",
    "X, Y, Z = axes3d.get_test_data()\n",
    "ax.plot_surface(X, Y, Z, rstride=8, cstride=8, alpha=0.3)\n",
    "cset = ax.contour(X, Y, Z, zdir='z', offset=-100, cmap=cm.coolwarm)\n",
    "cset = ax.contour(X, Y, Z, zdir='x', offset=-40, cmap=cm.coolwarm)\n",
    "cset = ax.contour(X, Y, Z, zdir='y', offset=40, cmap=cm.coolwarm)\n",
    "\n",
    "ax.set_xlabel('Emociones', fontsize = 16)\n",
    "ax.set_xlim(-40, 40)\n",
    "ax.set_xticks([])\n",
    "ax.set_ylabel('Cognición', fontsize = 16)\n",
    "ax.set_ylim(-40, 40)\n",
    "ax.set_yticks([])\n",
    "ax.set_zlabel('Densidad \\n P(puntaje>75)', fontsize = 16)\n",
    "ax.set_zlim(-100, 100)\n",
    "ax.set_zticks([])\n",
    "\n",
    "#fig.savefig('img/3_CB/3Density.svg')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Ejemplo Gibbs sampler para multivariada normal bidimensional\n",
    "#Parametros joint distribution\n",
    "mean_joint = [6, 25] #[impaciencia, confianza]\n",
    "corr = -0.6\n",
    "cov = [[1, corr], [corr, 1]]\n",
    "\n",
    "fig2 = plt.figure()\n",
    "rv = st.multivariate_normal(mean = mean_joint, cov = cov)\n",
    "samples = rv.rvs(1000)\n",
    "joint_p = sns.jointplot(samples[:,0], samples[:,1], kind=\"kde\", color = 'k')\n",
    "joint_p.set_axis_labels('Impaciencia', 'Confianza', fontsize=22)\n",
    "joint_p.savefig('img/3_CB/bivariate_normal.svg')\n",
    "\n",
    "\n",
    "#Algoritmo\n",
    "#Paso 1: valores iniciales\n",
    "conf_samp = st.uniform.rvs(20,30,1)\n",
    "impa_samp = st.uniform.rvs(0,10,1)\n",
    "niter = 300 #número de iteraciones\n",
    "joint = pd.DataFrame({'impaciencia': np.repeat(float('nan'),niter),\n",
    "                     'confianza': np.repeat(float('nan'),niter)})\n",
    "for i in range(niter):\n",
    "    #Paso 2: samplear un valor aleatorio de confianza\n",
    "    conf_samp = st.norm.rvs(mean_joint[1]+corr*(impa_samp-mean_joint[0]),1-corr**2,1)\n",
    "\n",
    "    #Paso 3: samplear un valor aleatorio de impaciencia\n",
    "    impa_samp = st.norm.rvs(mean_joint[0]+corr*(conf_samp-mean_joint[1]),1-corr**2,1)\n",
    "\n",
    "    joint.loc[i,'impaciencia'] = impa_samp\n",
    "    joint.loc[i,'confianza'] = conf_samp\n",
    "\n",
    "    \n",
    "imp = samples[:,0]\n",
    "conf = samples[:,1]\n",
    "xmin = imp.min()\n",
    "xmax = imp.max()\n",
    "ymin = conf.min()\n",
    "ymax = conf.max()\n",
    "X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "positions = np.vstack([X.ravel(), Y.ravel()])\n",
    "values = np.vstack([imp, conf])\n",
    "kde = st.gaussian_kde(values)\n",
    "Z = np.reshape(kde(positions).T, X.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = [5,5])\n",
    "x = np.linspace(xmin,xmax,100)\n",
    "y = np.linspace(ymin,ymax,100)\n",
    "ax.contour(x, y, Z, cmap='gray')\n",
    "ax.set_xlim([2,10])\n",
    "ax.set_ylim([21,29])\n",
    "ax.set_xlabel('Impaciencia', fontsize = 18)\n",
    "ax.set_ylabel('Confianza', fontsize = 18)\n",
    "ax.grid(True, color='gray', alpha = 0.25)\n",
    "ax.set_facecolor([1,1,1])\n",
    "ax.plot(-100, -100, 'ro', alpha = 0.4, label='Muestra Gibbs')\n",
    "ax.legend(facecolor = 'white')\n",
    "def animate(i):\n",
    "    idx = int(i)\n",
    "    ax.plot(joint.loc[idx,'impaciencia'],\n",
    "            joint.loc[idx,'confianza'],\n",
    "            'ro', alpha = 0.4)\n",
    "\n",
    "\n",
    "# call the animator.  blit=True means only re-draw the parts that have changed.\n",
    "anim = animation.FuncAnimation(fig, animate, interval = 100, \n",
    "                               frames = len(joint.index), blit=False)\n",
    "writer = animation.PillowWriter(fps=15) \n",
    "#anim.save('img/3_CB/Gibbs_animation.gif', writer = writer); #Compress online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class scaled_inv_chi2:\n",
    "    # An scaled invese chi2 with v degrees of freedom and scale s\n",
    "    # is the same as an inverse gamma with alpha = v/2, beta = (v/2)*(s^2) \n",
    "    def pdf(x, v, s):\n",
    "        alpha = v/2 #shape\n",
    "        beta = v/2 * s**2 #scale\n",
    "        return st.invgamma.pdf(x, a=alpha, scale=beta)\n",
    "    \n",
    "    def cdf(x, v, s):\n",
    "        alpha = v/2 #shape\n",
    "        beta = v/2 * s**2 #scale\n",
    "        return st.invgamma.pdf(x, a=alpha, scale=beta)\n",
    "    \n",
    "    def rvs(v, s, size):\n",
    "        #random samle\n",
    "        alpha = v/2 #shape\n",
    "        beta = v/2 * s**2 #scale\n",
    "        return st.invgamma.rvs(a=alpha, scale=beta, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "dot_text = 'digraph G {rankdir=LR;\\\n",
    "           /* general properties*/\\\n",
    "           node [margin=0, fixedsize=true,\\\n",
    "                 height=0.5, width=0.5];\\\n",
    "           a1 -> a2 [label=\"1\", lblstyle=\"font=\\normalsize\"];\\\n",
    "           a1 -> a1 [label = \"0\", lblstyle=\"font=\\normalsize\"];\\\n",
    "           a2 -> a1 [label=\" \", label=\"$a$\", lblstyle=\"font=\\\\normalsize\"];\\\n",
    "           a2 -> a2 [label=\" \", label=\"$1-a$\", lblstyle=\"font=\\\\normalsize\"];\\\n",
    "           a1[shape=circle, label=\"$\\\\theta$\"]\\\n",
    "           a2[shape=circle, label=\"$1-\\\\theta$\"]\\\n",
    "        }' #warning: use single quote at start and end; double quotes for labels\n",
    "s = Source(dot_text, filename=\"img/3_CB/mcmc_diagram.gv\", format=\"svg\") #THIS IS NOT THE FINAL ONE\n",
    "s.view()\n",
    "#a = t/{1-t}\n",
    "\n",
    "\n",
    "#To typeset latex stuff on the image: \n",
    "#1) open svg in inkscape and write latex formulas. Export as pdf (click the one that says latex)\n",
    "#   to change fontsize of latex in inkscape write before the expression: t/{1-t}\n",
    "#        \\fontsize{34pt}{1em} $latex expression$ ... change #pt for size\n",
    "#2) go to overleaf or latex editor of choice and do this (https://castel.dev/post/lecture-notes-2/):\n",
    "#   2.1) In the preamble:\n",
    "#  \\usepackage{import}\n",
    "#  \\usepackage{xifthen}\n",
    "#  \\usepackage{pdfpages}\n",
    "#  \\usepackage{transparent}\n",
    "#  \\usepackage{graphics} \n",
    "\n",
    "#  \\newcommand{\\incfig}[1]{%\n",
    "#      \\def\\svgwidth{\\columnwidth}\n",
    "#      \\import{./figures/}{#1.pdf_tex} %PUT the inkscape .pdf_tex AND .pdf in a local folder called figures\n",
    "#  }\n",
    "#   2.2)In the body:\n",
    "#  \\begin{figure}[ht]\n",
    "#      \\centering\n",
    "#      \\scalebox{.65}{\\incfig{your_inkscape.pdf_tex}} #change scalebox proportion to rescale\n",
    "#      \\caption{Riemmans theorem}\n",
    "#      \\label{fig:riemmans-theorem}\n",
    "#  \\end{figure}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Read data from a csv\n",
    "z_data = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/api_docs/mt_bruno_elevation.csv')\n",
    "\n",
    "fig = go.Figure(data=[go.Surface(z=z_data.values, showscale=False)])\n",
    "\n",
    "fig.update_layout(title='Panorama de una función', \n",
    "                  scene = dict(\n",
    "                    xaxis_title='X',\n",
    "                    yaxis_title='Y',\n",
    "                    zaxis_title='Z'),\n",
    "                  autosize=True,                  \n",
    "                  width=500, height=500,\n",
    "                  margin=dict(l=10, r=5, b=20, t=40));\n",
    "\n",
    "#fig.show()\n",
    "#fig.write_image(\"img/3_CB/3D_panorama.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#KL divergence\n",
    "xp = np.linspace(5, 16, 100)\n",
    "p = st.norm.pdf(xp, 10, 2)\n",
    "xq = np.linspace(6.5, 30, 100)\n",
    "q = st.norm.pdf(xq, 18, 4) \n",
    "xq2 = np.linspace(-5, 10, 100)\n",
    "q2 = st.norm.pdf(xq2, 2, 2) \n",
    "xq3 = np.linspace(-10, 25, 100)\n",
    "q3 = st.norm.pdf(xq3, 7, 6) \n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=[7,4])\n",
    "ax.plot(xp, p, c='red', label = 'Posterior')\n",
    "ax.plot(xq, q, c='green', linestyle='-', label = 'Candidata 1');\n",
    "ax.plot(xq2, q2, c='green', linestyle='--', label = 'Candidata 2');\n",
    "ax.plot(xq3, q3, c='green', linestyle=':', label = 'Candidata 3');\n",
    "ax.set_xlabel('Parámetros', fontsize = 15)\n",
    "ax.set_ylabel('Densidad', fontsize = 15)\n",
    "ax.set_title('¿Cuál es más parecida al posterior?', fontsize = 20)\n",
    "ax.legend()\n",
    "fig.savefig('img/3_CB/KL_1.svg')\n",
    "#plt.close()\n",
    "\n",
    "def KL(x, p, q):\n",
    "    return p(x) * np.log( p(x) / q(x) ) \n",
    "p_pdf = st.norm(10,2).pdf\n",
    "q_pdf = st.norm(18,4).pdf\n",
    "q2_pdf = st.norm(2,2).pdf\n",
    "q3_pdf = st.norm(7,6).pdf\n",
    "KLq_int, err = integrate.quad(KL, -30, 30, args=(q_pdf,p_pdf)) \n",
    "KLq2_int, err = integrate.quad(KL, -30, 30, args=(q2_pdf,p_pdf)) \n",
    "KLq3_int, err = integrate.quad(KL, -30, 30, args=(q3_pdf,p_pdf)) \n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=[7,4])\n",
    "ax.plot(xp, p, c='red', label = 'Posterior')\n",
    "ax.plot(xq, q, c='green', linestyle='-', label = 'Candidata 1. KL: '+str(round(KLq_int,2)));\n",
    "ax.plot(xq2, q2, c='green', linestyle='--', label = 'Candidata 2. KL: '+str(round(KLq2_int,2)));\n",
    "ax.plot(xq3, q3, c='green', linestyle=':', label = 'Candidata 3. KL: '+str(round(KLq3_int,2)));\n",
    "ax.set_xlabel('Parámetros', fontsize = 15)\n",
    "ax.set_ylabel('Densidad', fontsize = 15)\n",
    "ax.set_title('¿Cuál es más parecida al posterior?', fontsize = 20)\n",
    "ax.legend()\n",
    "fig.savefig('img/3_CB/KL_2.svg')\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "rise": {
   "chalkboard": {
    "color": [
     "rgb(250, 0, 0)",
     "rgb(0, 250, 250)"
    ]
   },
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "simple",
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
