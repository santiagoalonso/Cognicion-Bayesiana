{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on PyMC3 v3.9.3\n",
      "hddm version:  0.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santiago/anaconda3/lib/python3.8/site-packages/IPython/parallel.py:12: ShimWarning: The `IPython.parallel` package has been deprecated since IPython 4.0. You should import from ipyparallel instead.\n",
      "  warn(\"The `IPython.parallel` package has been deprecated since IPython 4.0. \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "from __future__ import print_function\n",
    "\n",
    "#Manejo de matrices y tablas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Estadistica y funciones matemáticas\n",
    "import scipy.stats as st\n",
    "from scipy.optimize import fmin\n",
    "from scipy import integrate\n",
    "from scipy.stats.mstats import mquantiles\n",
    "import statistics \n",
    "import pyreadr\n",
    "import scipy.io as sio\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Probabilistic programs\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt #NOTA: theano va a cambiar a tensorflow en PyMC4\n",
    "import theano\n",
    "from theano.compile.ops import as_op\n",
    "print('Running on PyMC3 v{}'.format(pm.__version__))\n",
    "import hddm\n",
    "print (\"hddm version: \", hddm.__version__)\n",
    "\n",
    "#Graficas\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import altair as alt\n",
    "from altair_saver import save #ademas instalar en terminal: brew cask install chromedriver\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, HBox, VBox, Layout\n",
    "from graphviz import Source, Digraph\n",
    "import dot2tex as d2t\n",
    "from latex import build_pdf\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import arviz as az\n",
    "import colorsys\n",
    "\n",
    "# Image processing stuff\n",
    "#!pip install opencv-python\n",
    "import cv2\n",
    "\n",
    "#Funciones propias (tienen que estar en el mismo directorio)\n",
    "import my_fun as mf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision criterion\n",
    "\n",
    "Santiago Alonso-Díaz, PhD <br>\n",
    "Universidad Javeriana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Drift diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Enigma & Turing [Video recomendado](https://www.youtube.com/watch?v=G2_Q9FoD-oQ) <br><br>\n",
    "\n",
    "\n",
    "<center><img src=\"img/9_CB/DDM1.png\" width = \"801\" height = '800'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El código era probabilístico (pseudo-uniforme, los alemanes nunca usaban la misma letra en el código y el mensaje) SI no se conocia la posición de los rotores\n",
    "\n",
    "<center><img src=\"img/9_CB/DDM2.png\" width = \"601\" height = '600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Las maquinas eran similares en como producian códigos aleatorios (sistema mecánico con rotores en posiciones iniciales; similar a random seeds hoy en día) <br> <br>\n",
    "\n",
    "<center><img src=\"img/9_CB/DDM5.png\" width = \"701\" height = '700'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Si dos códigos interceptados compartían letra en la misma posición era probable que fuera la misma maquina/posición de rotores. <br><br>\n",
    "\n",
    "<center><img src=\"img/9_CB/DDM3.png\" width = \"501\" height = '500'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Podemos acumular información ruidosa hasta un umbral para decidir si en efecto son la misma maquina (umbral superior) o diferente (umbral inferior) y empezar a decodificar <br><br>\n",
    "\n",
    "<center><img src=\"img/9_CB/DDM4.png\" width = \"501\" height = '500'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Qué tiene que ver esto con cognición bayesiana y decisiones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La actividad cerebral es ruidosa. Antes de decidir, LIP acumula actividad de MT hasta un umbral para confirmar. <br><br>\n",
    "\n",
    "<center><img src=\"img/9_CB/DDM6.svg\" width = \"701\" height = '700'></center>\n",
    "Roitman & Shadlen (2002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Los parametros usuales son drift rate, bound levels, starting point, and non-decision times. <br>\n",
    "En muchas versiones del DDM (no todas), se acumula el likelihood ratio (asume prior uniforme): $\\frac{p(evidencia|decisión \\ 1)}{p(evidencia|decisión \\ 2)}$ <br>\n",
    "En otras versiones, se acumula una variable de decision que se puede inferir dadas las acciones y tiempos de respuesta <br><br>\n",
    "\n",
    "\n",
    "<center><img src=\"img/9_CB/DDM7.svg\" width = \"801\" height = '800'></center>\n",
    "Shadlen & Kiani (2002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Cuál es el rojo más brillante? <br><br><br>\n",
    "<center><img src=\"img/9_CB/DDM8.svg\" width = \"501\" height = '500'></center>\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Cuál es el rojo más brillante? <br><br><br>\n",
    "<center><img src=\"img/9_CB/DDM9.svg\" width = \"501\" height = '500'></center>\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Cuál es el rojo más brillante? <br><br><br>\n",
    "<center><img src=\"img/9_CB/DDM10.svg\" width = \"501\" height = '500'></center>\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿En cuál turno fueron más lentos? En ese turno la evidencia se acumuló más lento (los otros parametros constantes) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El framework de acumulación de evidencia se ha aplicado en (reviews en Ratcliff, et al, 2016; Shadlen & Kiani, 2013):\n",
    "\n",
    "* Numerosidad\n",
    "* Memoria\n",
    "* Percepción de movimiento\n",
    "* Lenguaje\n",
    "* Categorización\n",
    "* Marketing\n",
    "* Altruismo\n",
    "* Descuento intertemporal\n",
    "* Otros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hay varios algoritmos para estimar los parámetros del modelo (Tabla de Shinn, et al, 2020). Acá nos centramos en HDDM que es bayesiano (Wiecki, et al, 2013). <br><br>\n",
    "\n",
    "<center><img src=\"img/9_CB/DDM11.svg\" width = \"601\" height = '600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "v: drift rate <br>\n",
    "a: decision threshold <br>\n",
    "z: starting point of accumulation <br>\n",
    "ndt: non-decision times (e.g. encoding) <br>\n",
    "$s_v, \\ s_z, \\ s_{ndt}$: inter-trial variability of parameters <br>\n",
    "x: response time and choice <br>\n",
    "\n",
    "\n",
    "<center><img src=\"img/9_CB/model_HDDM.svg\" width = \"601\" height = '600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Vamos a usar el paquete HDDM (Wiecki, Sofer, and Frank, 2013) y el demo en http://ski.clps.brown.edu/hddm_docs/tutorial_python.html\n",
    "\n",
    "El paquete está basado en PyMC. Sin embargo, requiere definiciones de densidades especiales para $x_{ji}$ por lo que mejor usamos el paquete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Usaremos el experimento de Cavanagh, et al, (2011). Subthalamic nucleus stimulation reverses mediofrontal influence over decision threshold.\n",
    "\n",
    "<center><img src=\"img/9_CB/Cav1.png\" width = \"601\" height = '600'></center>\n",
    "\n",
    "4 items. A: gana 100%; B: 0%; C: 75%; D: 25%. Probabilidades de ganar se aprenden via ensayo y error <br>\n",
    "4 tipos de turno:\n",
    "* High conflict WW win-win (A vs C); \n",
    "* High conflict LL lose-lose (B vs D); \n",
    "* Low-conflict WL (A vs D; C vs B). \n",
    "\n",
    "Hipótesis: High conflict trials aumentan el threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data = hddm.load_csv('data/9_CB/HDDM_data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Veamos los datos de los sujetos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data = hddm.utils.flip_errors(data) #pone los tiempos de respuesta de errores en negativo.\n",
    "\n",
    "fig = plt.figure(figsize = (10,6))\n",
    "ax = fig.add_subplot(111, xlabel='RT (s)', ylabel='count', title='RT distributions')\n",
    "for i, subj_data in data.groupby('subj_idx'):\n",
    "    subj_data.rt.hist(bins=20, histtype='step', ax=ax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ahora ajustar el modelo más simple donde los parametros no varían por condición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate model object passing it our data (no need to call flip_errors() before passing it).\n",
    "# This will tailor an individual hierarchical DDM around your dataset.\n",
    "#m = hddm.HDDM(data, include=('z', 'sv', 'st', 'sz')) #If you want to include inter-trial variability. SLOW\n",
    "m = hddm.HDDM(data)\n",
    "# find a good starting point which helps with the convergence.\n",
    "m.find_starting_values()\n",
    "# start drawing 7000 samples and discarding 5000 as burn-in\n",
    "m.sample(2000, burn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ahora veamos el parametro grupal umbral ($\\mu_a$) y su variabilidad ($\\sigma_a$), y su valor para un sujeto ($a_1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "stats = m.gen_stats()\n",
    "stats[stats.index.isin(['a', 'a_std', 'a_subj.0'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Ahora el plot de los traces, autocorrelaciones (si van a cero indica buen sampling), e histogramas de algunos parámetros\n",
    "m.plot_posteriors(['a', 't', 'v', 'a_std']) #group level parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Para correr varias cadenas hay que correr varias veces el comando. Con varias cadenas se puede calcular rhat (<1.1 convergio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "nchains = 4\n",
    "for i in range(nchains): \n",
    "    print(\"Cadena: \", i)\n",
    "    m = hddm.HDDM(data)\n",
    "    m.find_starting_values()\n",
    "    m.sample(5000, burn=20)\n",
    "    models.append(m)\n",
    "\n",
    "hddm.analyze.gelman_rubin(models) #rhat<1.1 convergio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Plot posterior predictive (red data, blue model) (HDDM plot_posterior_predictive function was not working)\n",
    "def _parents_to_random_posterior_sample(bottom_node, pos=None):\n",
    "    \"\"\"Walks through parents and sets them to pos sample.\"\"\"\n",
    "    for i, parent in enumerate(bottom_node.extended_parents):\n",
    "        #if not isinstance(parent, pm.Node): # Skip non-stochastic nodes\n",
    "        #    continue\n",
    "\n",
    "        if pos is None:\n",
    "            # Set to random posterior position\n",
    "            pos = np.random.randint(0, len(parent.trace()))\n",
    "\n",
    "        assert len(parent.trace()) >= pos, \"pos larger than posterior sample size\"\n",
    "        parent.value = parent.trace()[pos]\n",
    "        \n",
    "def _plot_posterior_pdf_node(bottom_node, axis, value_range=None, samples=10, bins=100):\n",
    "    \"\"\"Calculate posterior predictive for a certain bottom node.\n",
    "    :Arguments:\n",
    "        bottom_node : pymc.stochastic\n",
    "            Bottom node to compute posterior over. Bottom refers to individual level parameters.\n",
    "        axis : matplotlib.axis\n",
    "            Axis to plot into.\n",
    "        value_range : numpy.ndarray\n",
    "            Range over which to evaluate the likelihood.\n",
    "    :Optional:\n",
    "        samples : int (default=10)\n",
    "            Number of posterior samples to use.\n",
    "        bins : int (default=100)\n",
    "            Number of bins to compute histogram over.\n",
    "    \"\"\"\n",
    "\n",
    "    if value_range is None:\n",
    "        # Infer from data by finding the min and max from the nodes\n",
    "        #raise NotImplementedError(\"value_range keyword argument must be supplied.\")\n",
    "        value_range = np.linspace(-5, 5, 100) #in seconds\n",
    "\n",
    "    like = np.empty((samples, len(value_range)), dtype=np.float32)\n",
    "    for sample in range(samples):\n",
    "        _parents_to_random_posterior_sample(bottom_node) #this obtains random parameters (a, v, ndt)\n",
    "        # IMPORTANT ONE: Generate likelihood \n",
    "        like[sample,:] = bottom_node.pdf(value_range) #Basically, this is Navarro & Fuss (2009) likelihood\n",
    "\n",
    "    y = like.mean(axis=0) #The mean of the posterior predictive\n",
    "    try:\n",
    "        y_std = like.std(axis=0)\n",
    "    except FloatingPointError:\n",
    "        print(\"WARNING! %s threw FloatingPointError over std computation. Setting to 0 and continuing.\" % bottom_node.__name__)\n",
    "        y_std = np.zeros_like(y)\n",
    "\n",
    "    # Plot pp\n",
    "    axis.plot(value_range, y, label='post pred', color='b')\n",
    "    axis.fill_between(value_range, y-y_std, y+y_std, color='b', alpha=.8)\n",
    "\n",
    "    # Plot data\n",
    "    if len(bottom_node.value) != 0:\n",
    "        axis.hist(bottom_node.value.values, density=True, color='r',\n",
    "                  range=(value_range[0], value_range[-1]), label='data',\n",
    "                  bins=bins, histtype='step', lw=2.)\n",
    "\n",
    "    axis.set_ylim(bottom=0) # Likelihood and histogram can only be positive\n",
    "\n",
    "\n",
    "observeds = m.get_observeds()\n",
    "max_items = max([len(i[1]) for i in observeds.groupby('tag').groups.items()])\n",
    "columns = min(3, max_items)\n",
    "num_subjs = observeds.shape[0]\n",
    "for tag, nodes in observeds.groupby('tag'): #nodes are parameters e.g. bottom-node is a non-group level parameter\n",
    "    fig = plt.figure(figsize=(10,30))\n",
    "    #fig.suptitle(tag, fontsize=12)\n",
    "    fig.subplots_adjust(top=0.9, hspace=.4, wspace=.3)\n",
    "\n",
    "    nrows = num_subjs or len(nodes)/columns\n",
    "\n",
    "    if len(nodes) - int(nrows * columns) > 0:\n",
    "        nrows += 1\n",
    "\n",
    "    # Plot individual subjects (if present)\n",
    "    i = 0\n",
    "    for subj_i, (node_name, bottom_node) in enumerate(nodes.iterrows()):\n",
    "        i += 1\n",
    "        \n",
    "        ax = fig.add_subplot(int(np.ceil(nrows)), int(columns), int(subj_i+1))\n",
    "        if 'subj_idx' in bottom_node:\n",
    "            ax.set_title('Subj. ' + str(bottom_node['subj_idx']))\n",
    "\n",
    "        _plot_posterior_pdf_node(bottom_node['node'], ax)\n",
    "\n",
    "        if i >= np.ceil(nrows) * columns:\n",
    "            warnings.warn('Too many nodes. Consider increasing number of columns.')\n",
    "            break\n",
    "\n",
    "        if num_subjs is not None and i >= num_subjs:\n",
    "            break\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Con HDDM podemos calcular parametros por condicion (WW, LL, WL) usando la opción depends_on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "m_stim = hddm.HDDM(data, depends_on={'v': 'stim', 'a': 'stim'}) # en depends_on, el value del diccionario es la columna en la data con la condición experimental\n",
    "m_stim.find_starting_values()\n",
    "m_stim.sample(4500, burn=1000)\n",
    "\n",
    "#the sampled parameters are in .nodes_db.node. It is a pandas dataframe\n",
    "v_WW, v_LL, v_WL = m_stim.nodes_db.node[['v(WW)', 'v(LL)', 'v(WL)']]\n",
    "a_WW, a_LL, a_WL = m_stim.nodes_db.node[['a(WW)', 'a(LL)', 'a(WL)']]\n",
    "ndt = m_stim.nodes_db.node['t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hddm.analyze.plot_posterior_nodes([v_WW, v_LL, v_WL]);\n",
    "plt.xlabel('drift-rate');\n",
    "plt.ylabel('Posterior probability');\n",
    "plt.title('Posterior of drift-rate group-level means');\n",
    "\n",
    "plt.figure();\n",
    "hddm.analyze.plot_posterior_nodes([a_WW, a_LL, a_WL]);\n",
    "plt.xlabel('bounds');\n",
    "plt.ylabel('Posterior probability');\n",
    "plt.title('Posterior of bounds group-level means');\n",
    "\n",
    "\n",
    "plt.figure();\n",
    "hddm.analyze.plot_posterior_nodes([ndt]);\n",
    "plt.xlabel('non-decision times');\n",
    "plt.ylabel('Posterior probability');\n",
    "plt.title('Posterior of ndt group-level means');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Model comparison via DIC (lower is better)\n",
    "print(\"Lumped model DIC: %f\" % m.dic)\n",
    "print(\"Stimulus model DIC: %f\" % m_stim.dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El paquete HDDM también puede calcular los parámetros con información externa via regresiones (e.g. que el drift dependa de la edad o actividad cerebral). Detalles en https://github.com/hddm-devs/hddm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El paquete HDDM no simula, usa formulas, sin embargo veamos aproximádamente una simulación para obtener la variable de decisión (la traza). Esto puede ser útil, por ejemplo, para calcular confianza en la decisión.\n",
    "<br><br>\n",
    "<center><img src=\"img/9_CB/Wiecki1.svg\" width = \"601\" height = '600'></center>\n",
    "Imagen: Wiecki, et al, 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulated DV (decision variable)\n",
    "a = 20*a_WL.trace().mean() # bound\n",
    "v = v_WL.trace().mean() # drift\n",
    "t = ndt.trace().mean() # non-decision time\n",
    "n_ttrrials = 200\n",
    "sampling_rate = 1e-2 #arbitrary units \n",
    "\n",
    "#simulated_data, params = hddm.generate.gen_rand_data(params={'a': a, 'v': v, 't': t}, size=n_ttrrials) #No se estimo st, sv, sz\n",
    "#print([simulated_data['rt'].mean() - t, simulated_data['response'].mean(), v,a,t])\n",
    "\n",
    "z = np.random.uniform(-a/4,a/4)#Starting point\n",
    "DVdt = np.append(np.array([z]), np.random.normal(v*sampling_rate, 1, 3000))\n",
    "print(a, v, t, z, DV)\n",
    "DV = np.cumsum(DVdt)\n",
    "idx_below = np.argmax(DV<-a) #bottom threshold\n",
    "idx_above = np.argmax(DV>a) #upper threshold\n",
    "if min(idx_below, idx_above) == 0:\n",
    "    idx_choice_time = max(idx_below, idx_above)\n",
    "else:\n",
    "    idx_choice_time = min(idx_below, idx_above)\n",
    "choicee = 'correct'\n",
    "if idx_choice_time==0: #never passed a threshold\n",
    "    choicee = 'No threshold hit'\n",
    "    idx_choice_time = len(DV)-1\n",
    "elif idx_choice_time == idx_below: \n",
    "    choicee = 'wrong'\n",
    "    \n",
    "    \n",
    "DV = DV[0:idx_choice_time]\n",
    "#DV = DV[(DV<(a*sampling_rate)) & (DV>(-a*sampling_rate))]\n",
    "plt.plot(DV);\n",
    "print(v*sampling_rate, DV.shape[0], idx_choice_time, choicee)\n",
    "#DV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Definamos confianza como p(choice=correct|DV). Implementemos esta idea en un ejercicio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Ejercicio \n",
    "\n",
    "* Simule el DV de un turno correcto e infiera la probabilidad de que DV es mayor a cero. Es decir, asuma que su data es el vector DV y quiere calcular si el posterior del promedio de DV es mayor a cero. Use un likelihood normal y un prior uniforme para el promedio y desviación estándar de DV (recuerde que la desviación estándar debe ser mayor a cero i.e. el prior va de [0, K]). \n",
    "    * Bono: hacerlo de dos formas, a) con pymc3 y b) con la formula para la densidad de una posterior con likelihood normal y priors uniformes.  \n",
    "\n",
    "* Haga lo anterior (i.e. simule el DV) para 5 turnos WW, 5 turnos LL, y 5 turnos WL. ¿Cuál condición genera mayor confianza?\n",
    "\n",
    "* ¿Qué pasa si baja el umbral? Hagalo solo con turnos WL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Signal detection theory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Le pedimos a un par de personas que detecte varias veces si hay un tono/señal/voz del más alla en un ambiente lleno de ruido. <br><br>\n",
    "En la figura, Laurie dice \"sí\" más veces. ¿Es Laurie más sensible al tono/señal/voz del más alla que Chris?\n",
    "\n",
    "<center><img src=\"img/9_CB/Goldstein1.png\" width = \"501\" height = '500'></center>\n",
    "Fuente: Goldestein, 1996, Sensation & Perception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Laurie dice sí más y puede que genere más hits pero también más false alarms.<br><br>\n",
    "\n",
    "|              | Signal + Noise trial |    Noise Trial    |\n",
    "|--------------|:--------------------:|:-----------------:|\n",
    "| Yes response |          Hit         |    False alarm    |\n",
    "| No response  |         Miss         | Correct rejection |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "eje x: fuerza de actividad interna (supuesto: ruido produce actividad más débil)<br>\n",
    "$d$: distancia entre ruido y señal <br>\n",
    "$d/2$: criterio de decisión optimo si turnos de ruido y señal ocurren con igual probabilidad <br>\n",
    "$k$: criterio de decisión usado por la persona (e.g. si fuerza actividad>$k$, decir sí) <br>\n",
    "$c$: sesgo i.e. criterio usado ($k$) menos criterio óptimo ($d/2$)<br>\n",
    "$\\theta^h$: probabilidad de hit <br>\n",
    "$\\theta^f$: probabilidad de false alarm <br>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center><img src=\"img/9_CB/Lee_Wagenmakers1.svg\" width = \"501\" height = '500'></center>\n",
    "Fuente: Lee & Wagenmakers (2013)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Ejercicio\n",
    "\n",
    "* En la anterior figura, sombree el área que correspondería a \"Miss\" y cuál a \"Correct Rejection\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Volviendo a Laurie y Chris, hay por lo menos dos hipótesis para estos datos:\n",
    "* Laurie y Chris difieren en d (sensibilidad)\n",
    "* Laurie y Chris difieren en k (criterio)\n",
    "<br><br>\n",
    "<center><img src=\"img/9_CB/Goldstein1.png\" width = \"501\" height = '500'></center>\n",
    "Fuente: Goldestein, 1996, Sensation & Perception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "No solo aplica para fenómenos perceptuales. Cualquier toma de decisión dependerá de la discriminibilidad y umbrales que se pongan.\n",
    "\n",
    "<center><img src=\"img/9_CB/Krajbich1.svg\" width = \"451\" height = '450'></center>\n",
    "Fuente: Krajbich, et al, 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Krajbich, et al, 2015, de hecho encuentran que una vez controlando por discriminabilidad, la idea de thinking fast and slow (barras) desaparece (scatter plots).\n",
    "\n",
    "| Dictator Game | Intertemporal Discounting |\n",
    "|:-------------:|:-------------------------:|\n",
    "|      <center><img src=\"img/9_CB/Krajbich2.svg\" width = \"451\" height = '450'></center>      |        <center><img src=\"img/9_CB/Krajbich3.svg\" width = \"451\" height = '450'></center>       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "En un marco que integre valor esperado y signal detection theory, los criterios óptimos podrían considerar valor esperado (Lynn, Wormwood, Barret, & Quigley, 2015). Acá, la decisión de cicla o carro depende tanto del valor esperado del método de transporte y la probabilidad de lluvia.\n",
    "<br><br>\n",
    "\n",
    "<center><img src=\"img/9_CB/Lynn1.jpg\" width = \"551\" height = '550'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Modelos basados en https://github.com/junpenglao/Bayesian-Cognitive-Modeling-in-Pymc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intrinsic whole number bias\n",
    "dot_text = 'digraph G {rankdir=TB; compound=true; newrank=true; labelloc=\"t\";\\\n",
    "           label=\"Hierarchical Drift Diffusion Model (HDDM)\";\\\n",
    "           /* general properties*/\\\n",
    "           node [margin=0, fixedsize=true, shape=plaintext,\\\n",
    "                 height=0.3, width=0.3, lblstyle=\"font=\\\\small\"];\\\n",
    "           /* links */\\\n",
    "           mu_v -> v;\\\n",
    "           sigma_v -> v;\\\n",
    "           mu_a -> a;\\\n",
    "           sigma_a -> a;\\\n",
    "           mu_z -> z;\\\n",
    "           sigma_z -> z;\\\n",
    "           mu_ndt -> ndt;\\\n",
    "           sigma_ndt -> ndt;\\\n",
    "           v -> x;\\\n",
    "           a -> x;\\\n",
    "           z -> x;\\\n",
    "           ndt -> x;\\\n",
    "           sv -> x;\\\n",
    "           sz -> x;\\\n",
    "           sndt -> x;\\\n",
    "           subgraph cluster0 {\\\n",
    "               margin = 5; labeljust=l; lblstyle=\"font=\\\\small\";\\\n",
    "               style = rounded;\\\n",
    "               label = \"$j subject$\";\\\n",
    "               v;\\\n",
    "               a;\\\n",
    "               z;\\\n",
    "               ndt;\\\n",
    "               subgraph cluster1 {\\\n",
    "                   margin = 5; labeljust=l; lblstyle=\"font=\\\\small\";\\\n",
    "                   style = rounded;\\\n",
    "                   label = \"$i trial$\";\\\n",
    "                   x;\\\n",
    "               }\\\n",
    "           }\\\n",
    "           /* nodes */\\\n",
    "           v [label = \"$v_j$\", shape = circle];\\\n",
    "           mu_v [label = \"$mu_v$\", shape = circle];\\\n",
    "           sigma_v [label = \"$sig_v$\", shape = circle];\\\n",
    "           a [label = \"$a_j$\", shape = circle];\\\n",
    "           mu_a [label = \"$mu_a$\", shape = circle];\\\n",
    "           sigma_a [label = \"$sig_a$\", shape = circle];\\\n",
    "           z [label = \"$z_j$\", shape = circle];\\\n",
    "           mu_z [label = \"$mu_z$\", shape = circle];\\\n",
    "           sigma_z [label = \"$sig_z$\", shape = circle];\\\n",
    "           ndt [label = \"$ndt_j$\", shape = circle];\\\n",
    "           mu_ndt [label = \"$mu_{ndt}$\", shape = circle];\\\n",
    "           sigma_ndt [label = \"$sig_{ndt}$\", shape = circle];\\\n",
    "           sv [label = \"$s_v$\", shape = circle];\\\n",
    "           sz [label = \"$s_z$\", shape = circle];\\\n",
    "           sndt [label = \"$s_{ndt}$\", shape = circle];\\\n",
    "           x [label = \"$x_{ji}$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           }' #warning: use single quote at start and end; double quotes for labels\n",
    "s = Source(dot_text, filename=\"img/9_CB/model_HDDM.gv\", format=\"svg\") #THIS IS NOT THE FINAL ONE\n",
    "s.view()\n",
    "\n",
    "#distributions:\n",
    "# \\mu_v \\sim Normal(2,3)\n",
    "# \\sigma_v \\sim Half \\ Normal(0,2)\n",
    "# \\mu_a \\sim Normal(1.5,0.75)\n",
    "# \\sigma_a \\sim Half \\ Normal(0,0.1)\n",
    "# \\mu_z \\sim Normal(0.5,0.5)\n",
    "# \\sigma_z \\sim Half \\ Normal(0,0.05)\n",
    "# \\mu_{ndt} \\sim Normal(0.4,0.2)\n",
    "# \\sigma_{ndt} \\sim Half \\ Normal(0,1)\n",
    "# s_v \\sim Half \\ Normal(0,2)\n",
    "# s_z \\sim Beta(1,3)\n",
    "# s_{ndt} \\sim Half \\ Normal(0,0.3)\n",
    "# a_j \\sim \\Gamma(\\mu_a, \\sigma_a^2)\n",
    "# v_j \\sim Normal(\\mu_v, \\sigma_v^2)\n",
    "# z_j \\sim invlogit(Normal(\\mu_z, \\sigma_z^2))\n",
    "# ndt_j \\sim Normal(\\mu_{ndt}, \\sigma_{ndt}^2)\n",
    "# x_{ji} \\sim \\frac{\\pi}{a_j^2} e^{\\left( -v_j a_j z_j - \\frac{v_j^2x}{2} \\right)} \\times \\sum_{k=1}^{\\infty} k e^{\\left(-\\frac{k^2\\pi^2x}{2a_j^2}\\right)} sin(k \\pi z_j)\n",
    "\n",
    "\n",
    "#To typeset latex stuff on the image: \n",
    "#1) open svg in inkscape and write latex formulas. Export as pdf (click the one that says latex)\n",
    "#   to change fontsize of latex in inkscape write before the expression: \n",
    "#        \\fontsize{34pt}{1em} $latex expression$ ... change #pt for size\n",
    "#2) go to overleaf or latex editor of choice and do this (https://castel.dev/post/lecture-notes-2/):\n",
    "#   2.1) In the preamble:\n",
    "#  \\usepackage{import}\n",
    "#  \\usepackage{xifthen}\n",
    "#  \\usepackage{pdfpages}\n",
    "#  \\usepackage{transparent}\n",
    "#  \\usepackage{graphics} \n",
    "\n",
    "#  \\newcommand{\\incfig}[1]{%\n",
    "#      \\def\\svgwidth{\\columnwidth}\n",
    "#      \\import{./figures/}{#1.pdf_tex} %PUT the inkscape .pdf_tex AND .pdf in a local folder called figures\n",
    "#  }\n",
    "#   2.2)In the body:\n",
    "#  \\begin{figure}[ht]\n",
    "#      \\centering\n",
    "#      \\scalebox{.65}{\\incfig{your_inkscape.pdf_tex}} #change scalebox proportion to rescale\n",
    "#      \\caption{Riemmans theorem}\n",
    "#      \\label{fig:riemmans-theorem}\n",
    "#  \\end{figure}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROAD MAP:\n",
    "Present the idea of decision criterions from two cognitive science traditions:\n",
    "\n",
    "* Signal detection theory (Wagenmakers & Jungpeng chapter)\n",
    "* Drift-Difussion (Frank & Wickli paper in frontiers)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "rise": {
   "chalkboard": {
    "color": [
     "rgb(250, 0, 0)",
     "rgb(0, 250, 250)"
    ]
   },
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "simple",
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
