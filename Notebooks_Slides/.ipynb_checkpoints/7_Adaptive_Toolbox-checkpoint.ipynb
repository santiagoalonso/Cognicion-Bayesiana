{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on PyMC3 v3.11.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "\n",
    "#Manejo de matrices y tablas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Estadistica y funciones matemáticas\n",
    "import scipy.stats as st\n",
    "from scipy.optimize import fmin\n",
    "from scipy import integrate\n",
    "from scipy.stats.mstats import mquantiles\n",
    "import statistics \n",
    "import pyreadr\n",
    "import scipy.io as sio\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Probabilistic programs\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt #NOTA: theano va a cambiar a tensorflow en PyMC4\n",
    "import theano\n",
    "from theano.compile.ops import as_op\n",
    "print('Running on PyMC3 v{}'.format(pm.__version__))\n",
    "\n",
    "#Graficas\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import altair as alt\n",
    "from altair_saver import save #ademas instalar en terminal: brew cask install chromedriver\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, HBox, VBox, Layout\n",
    "from graphviz import Source, Digraph\n",
    "import dot2tex as d2t\n",
    "from latex import build_pdf\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import arviz as az\n",
    "\n",
    "#Funciones propias (tienen que estar en el mismo directorio)\n",
    "import my_fun as mf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Adaptive Toolbox\n",
    "\n",
    "Santiago Alonso-Díaz, PhD \\\n",
    "Universidad Javeriana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"img/7_CB/Gigerenzer1.png\" width = \"501\" height = '500'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Los humanos y animales siempre han dependido de heurísticas para solucionar problemas. \n",
    "\n",
    "Ejemplo 1: medir áreas de hendiduras en el piso. Dar una vuelta irregular a la hendidura y dejar feromonas. Dar otra vuelta irregular y estimar el área por la frecuencia que se cruzan los dos caminos.  \n",
    "\n",
    "<center><img src=\"img/7_CB/faris-mohammed-unsplash.jpg\" width = \"301\" height = '300'></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Ejemplo 2: agarrar objetos a alta velocidad. Mantener un ángulo óptico constante entre uno mismo y el objeto.  \n",
    "\n",
    "<center><img src=\"img/7_CB/c-perret-unsplash.jpg\" width = \"301\" height = '300'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Por qué aparecen las heurísticas? \n",
    "* No omnisciencia (saber todas las variables)\n",
    "* No omnipotencia (poder computacional infinito)\n",
    "* Intractabilidad (no hay solución analítica o computable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Las heurísticas usualmente se relacionan (erroneamente) con el accuracy-effort tradeoff:\n",
    "* La segunda (o 3era, 4ta, ...) mejor alternativa \n",
    "    * no del todo, pueden ser de hecho la mejor. \n",
    "* Las usamos por nuestros limítes computacionales \n",
    "    * no siempre, incluso en problemas fáciles pueden ser útiles\n",
    "* Son una alternativa menor por que no usan toda la información, tiempo, y computos \n",
    "    * no siempre, e.g. bias-variance trade-off\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Las heurísticas pueden ser eficientes\n",
    "\n",
    "Less-is-more effect:\n",
    "> \"More information or computation can decrease accuracy; therefore, minds rely on simple heuristics in order to be more accurate than strategies that use more information and time.\" Gigerenzer & Brighton, 2009, pp 110\n",
    "\n",
    "Veamos algunos casos de less-is-more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><div> Tallying</div></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Problema: \n",
    "\n",
    "Predecir outcome Y (e.g. oprimir o no oprimir) con atributos X ($X_1, X_2, ... , X_n$)\n",
    "\n",
    "### Solución: \n",
    "\n",
    "Regresión logística (p>0.5 categoria 1, p<0.5 categoria 1):\n",
    "\n",
    "$$ Y = \\beta_0 + \\beta X $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$ Y = \\beta_0 + \\beta X $$\n",
    "\n",
    "¿Cómo obtener $\\beta_0$ y $\\beta$?\n",
    "\n",
    "* Maximum likelihood (MLE)\n",
    "* Aleatorios\n",
    "* Rankeados por validez (conocimiento del área)\n",
    "* Todos igual (tallying)\n",
    "\n",
    "¿Cuál es una solución heurística? Todas menos MLE.\n",
    "\n",
    "Centremonos en tallying. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Por qué se llama tallying/conteo? Un ejemplo: Y es comprar o no comprar torta de chocolate\n",
    "\n",
    "$Y_i = 1 + 1*animo + 1*precio_{torta} + 1*colesterol_{torta} + 1*azucar_{torta} + 1*hora_{día}$\n",
    "\n",
    "Cuando todo tiene el mismo peso (1), la probabilidad crece a medida que aumenta la suma. Es decir, es como contar (tally) las características disponibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hay una versión simple de tallying: take-the-best de diferentes atributos (cues)\n",
    "\n",
    "A la pregunta cuál tiene más colesterol la persona responde con el mejor cue, es decir, el que considera más relevante y conoce (e.g. calorias en la información nutricional) <br><br>\n",
    "\n",
    "\n",
    "<center><img src=\"img/7_CB/Gigerenzer2.svg\" width = \"501\" height = '500'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Czerlinski, Gigerenzer, & Goldstein (1999) pusieron a prueba tallying, take-the-best, y regresión múltiple en veinte dominios. Trataron de predecir los siguientes outcomes:\n",
    "\n",
    "* Deserción escolar\n",
    "* Habitante de la calle\n",
    "* Mortalidad\n",
    "* Tamaño de ciudad\n",
    "* Atractivo (hombres)\n",
    "* Atractivo (mujeres)\n",
    "* Precio de vivienda\n",
    "* Renta de la tierra\n",
    "* Salarios de profesores\n",
    "* Accidentes de carros\n",
    "* Consumo de gasolina\n",
    "* Obesidad a los 18\n",
    "* Grasa corporal\n",
    "* Fertilidad de peces\n",
    "* Tiempo de sueño mamifero\n",
    "* Calidad de abono de vaca\n",
    "* Biodiversidad\n",
    "* Cantidad de lluvia\n",
    "* Cantidad de oxidantes en Los Angeles\n",
    "* Cantidad de ozono en San Francisco\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Regresión ajusta mejor los datos. Tallying y take-the-best son mejores prediciendo. <br><br><br>\n",
    "\n",
    "<center><img src=\"img/7_CB/Gigerenzer3.svg\" width = \"501\" height = '500'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En otro estudio se probaron algunos algoritmos de machine learning en una tarea de adivinar la población de una ciudad. De nuevo, take-the-best le iba bien con muestras pequeñas. <br><br> <br>\n",
    "\n",
    "<center><img src=\"img/7_CB/Gigerenzer4.svg\" width = \"700\" height = '700'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Tallying o take-the-best no optimizan nada, son heurísticas. Take-the-best forma parte de un grupo de heurísticas donde less-is-more (less information, more accuracy). Cuestionan la noción que siempre hay un accuracy-effort tradeoff (effort = usar toda la info).\n",
    "\n",
    "> \"Why should a mind waste time and effort in estimating the optimal weights of cues if they do not matter or\n",
    "even detract from performance?\" Gigerenzer & Brighton, 2009, pp. 112"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Tres elementos de take-the-best\n",
    "1. Regla de busqueda: Buscar cues (e.g. grasas saturadas, calorias, proteina, sodio) en orden de validez relativo al outcome (e.g. colesterol).\n",
    "2. Regla para parar: Cuando se encuentre el primer cue que discrimine entre objetos (e.g. calorias cake vs calorias pie)\n",
    "3. Regla para decidir: El que tenga el mejor valor en el cue donde se paró."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>Ecological rationality</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>¿Cuando sirven las heurísticas? Para adaptarse. Para enfrentarse al tradeoff entre sesgo-varianza </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Qué es el tradeoff sesgo-varianza? Es un resultado de inferencia estadística <br> <br>\n",
    "\n",
    "<center><img src=\"img/7_CB/bias-and-variance2.jpg\" width = \"400\" height = '400'></center>\n",
    "\n",
    "$$ Prediction \\ error = bias^2 + variance + noise$$\n",
    "\n",
    "En esta formula (y gráfica), a un nivel de error de predicción deseado, si bajo el sesgo (distancia del modelo a datos recolectados) necesito subir la varianza (distancia del modelo a un nuevo dato) para mantener el error de predicción deseado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ejemplo: predecir el clima por el día del año. Un modelo complejo (grado 12) ajusta más puntos (bajo sesgo; gráfica izq.). Sin embargo, unir puntos de un año no asegura que se pueda predecir que pasa un día de otro año (alta varianza; gráfica der.) <br><br>\n",
    "\n",
    "<center><img src=\"img/7_CB/Gigerenzer5.svg\" width = \"900\" height = '900'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Y qué tiene que ver eso con heurísticas?\n",
    "\n",
    "En general, tener alta varianza y bajo sesgo (overfit) puede ser poco adaptativo. Para Gigerenzer & Brighton, un organismo debería apuntar a bajar varianza i.e. mejorar predicción con nuevos datos. Las heurísticas parecen ser buenas para eso. <br> <br>\n",
    "\n",
    "<center><img src=\"img/7_CB/Gigerenzer3.svg\" width = \"401\" height = '400'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La presencia de heurísticas va a depender de la estructura del ambiente\n",
    "\n",
    "* No compensatorio (e.g. los cues no se correlacionan y su relación con el outcome difiere)\n",
    "* Compensatorio (e.g. los cues están muy correlacionados con el outcome pera hay algunos mejores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> \"Why should experts and laypeople rely on heuristics? To summarize, the answer is not simply in the accuracy effort dilemma but in the bias–variance dilemma, as higher accuracy can be achieved by more or less effort.\" Gigerenzer & Brighton, 2009, pp 125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Algunas heurísticas\n",
    "| Heuristic             |                                                                     Description                                                                     |\n",
    "|-----------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------:|\n",
    "| Recognition           |                          If one of two alternatives is recognized, infer that it has the higher <br>value on the criterion.                         |\n",
    "| Fluency               |              If both alternatives are recognized but one is recognized faster, <br>infer that it has the higher value on the criterion.             |\n",
    "| Take-the-best         | (a) search through cues in order of validity, (b) stop search<br>as soon as a cue discriminates, and (c) choose the alternative<br>this cue favors. |\n",
    "| Tallying              |                                        Do not estimate weights but simply count the number of positive cues.                                        |\n",
    "| Satisficing           |                             Search through alternatives and choose the first one that exceeds your <br>aspiration level.                            |\n",
    "| 1/N                   |                                                Allocate resources equally to each of N alternatives.                                                |\n",
    "| Default               |                                                          If there is a default, do nothing.                                                         |\n",
    "| Tit-for-tat           |                                            Cooperate first and then imitate your partner’s last behavior.                                           |\n",
    "| Imitate the majority  |                                                                Imitate the majority.                                                                |\n",
    "| Imitate the succesful |                                                        Imitate the most succesful individual.                                                       |\n",
    "\n",
    "\n",
    "Gigerenzer & Brighton, 2009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modelos bayesianos para heurísticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Take-the-best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Cuál ciudad tiene más habitantes? \n",
    "\n",
    "<center> Lima vs Buenos Aires </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Proceso take-the-best:\n",
    "1) Ordenar los cues por importancia \\\n",
    "2) Parar en el cue que discrimine \\\n",
    "3) Escoger el objeto con el mejor valor en el cue de 2 \n",
    "\n",
    "\n",
    "|   Memory of  | Country Population | Capital |     GDP    | World cup wins |\n",
    "|:------------:|:------------------:|:-------:|:----------:|:--------------:|\n",
    "|     Lima     |     ¯\\\\\\_(ツ)_/¯     |   Yes   | ¯\\\\\\_(ツ)_/¯ |       No       |\n",
    "| Buenos Aires |     ¯\\\\\\_(ツ)_/¯     |   Yes   | ¯\\\\\\_(ツ)_/¯ |       Yes      |\n",
    "\n",
    "Take-the-best: Buenos Aires por World cup wins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$ t_q = \\text{TTB}_{s}(\\mathbf a_q,\\mathbf b_q)$$\n",
    "$$ \\gamma \\sim \\text{Uniform}(0.5,1)$$  \n",
    "$$ y_{iq} \\sim\n",
    "\\begin{cases}\n",
    "\\text{Bernoulli}(\\gamma) & \\text{if $t_q = a$} \\\\\n",
    "\\text{Bernoulli}(1- \\gamma) & \\text{if $t_q = b$} \\\\\n",
    "\\text{Bernoulli}(0.5) & \\text{otherwise}\n",
    "\\end{cases}  $$\n",
    "\n",
    "$t_q$: decisión take-the-best (a o b) (deterministica) \\\n",
    "$a_q, \\ b_q$: vector de cues para opción a y b (observable) \\\n",
    "$s$: orden de los cues por validez (observable) \\\n",
    "$\\gamma$: probabilidad de tomar la decisión take-the-best (i.e. $a_q$) (inferido, $\\ge$ 0.5 pues no se decide lo peor)\\\n",
    "$y_{iq}$: decisión del individuo i a la pregunta q (observable) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejercicio en clase\n",
    "\n",
    "Haga el diagrama que represente las formulas. Está abajo, no lo vea. Luego de hacer el suyo comparelos. ¿Son iguales o qué falto/mejoró?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/7_CB/model_TTB.svg\" width = \"51\" height = '50'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El experimento y data a modelar es de Ben Newell (la versión de Wagenmakers):\n",
    "* 20 sujetos\n",
    "* Cada sujeto contesto las mismas 30 preguntas\n",
    "* Las preguntas eran de varios temas y se escogía entre dos opciones.\n",
    "* Por ejemplo ¿qué ciudad de estas dos tiene más habitantes?\n",
    "* En cada pregunta, el sujeto podía buscar entre 9 cues por orden de validez para ver si las opciones los tenían.\n",
    "* Algo así:\n",
    "\n",
    "<center><img src=\"img/7_CB/TTB_experiment.svg\" width = \"550\" height = '550'></center>\n",
    "\n",
    "* Otro ejemplo:\n",
    "\n",
    "<center><img src=\"img/7_CB/TTB_experiment2.svg\" width = \"551\" height = '550'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Load data\n",
    "m = np.array(pd.read_csv('data/7_CB/m.csv').iloc[:,1:]) #design matrix; rows: stimuli (e.g. Berlin); columns: cues (1 in the stimuli, 0 not)\n",
    "p = np.array(pd.read_csv('data/7_CB/p.csv').iloc[:,1:]) #pairs of stimulus index; rows: questions. columns: stimuli index as numbered in the rows of matrix m \n",
    "y = np.array(pd.read_csv('data/7_CB/y.csv').iloc[:,1:]) #choices: rows subjects, columns questions; 0, b_q is chosen, 1 a_q is chosen\n",
    "n = np.shape(m)[0]  # number of stimuli\n",
    "nc = pd.read_csv('data/7_CB/nc.csv')['nc'][0] #number of cues\n",
    "nq = pd.read_csv('data/7_CB/nq.csv')['nq'][0] #number of questions\n",
    "ns = pd.read_csv('data/7_CB/ns.csv')['ns'][0] #number of subjects\n",
    "v = np.array( pd.read_csv('data/7_CB/v.csv')['v']) #cue validity\n",
    "x = np.array(pd.read_csv('data/7_CB/x.csv')['x']) #for WADD evidence provided by each cue, defined as the log-odds of their validity i.e x = log (v/(1-v)). The first one is 100 to avoid Inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#More data/observables\n",
    "s = np.argsort(v)  # cue validity (rank-order; high rank high validity)\n",
    "t = [] #take the best decision\n",
    "# TTB Model For Each Question\n",
    "for q in range(nq): #questions\n",
    "    # Add Cue Contributions To Mimic TTB Decision\n",
    "    tmp1 = np.zeros(nc)\n",
    "    for j in range(nc): #cues\n",
    "        # the -1 because it was data from r (i.e. index in python start at 0)\n",
    "        # 2 to the validity to enhance cue differences. In words of Lee & Wagenmakers:\n",
    "        # \"applying non- compensatory weights (the pow(2,s[j])) so that \n",
    "        # the first discriminating cue determines the decision\". \n",
    "        # The power 2 is the math trick that implements take the best (it could be other base > 2).\n",
    "        tmp1[j] = (m[p[q, 0] - 1, j] - m[p[q, 1] - 1, j]) * np.power(2, s[j]) \n",
    "        \n",
    "    # Find if Cues Favor First (sum>0), Second (sum<0), or Neither Stimulus (sum = 0)\n",
    "    tmp2 = np.sum(tmp1)\n",
    "    tmp3 = -1 * np.float32(-tmp2 > 0) + np.float32(tmp2 > 0) #-1 second stimulus (b_q) is better, 0 neither, 1 first stimulus (a_q) is better \n",
    "    t.append(tmp3 + 1) #it can be 0 (b_q), 1 (neither), or 2 (a_q) ... index for the yiq distribution (see pymc model)\n",
    "\n",
    "t = np.asarray(t, dtype=int) \n",
    "#all are 2 i.e. first stimulus (a_q) is always take-the-best \n",
    "#i.e. gamma is the prob. of take the best (see gammat in pymc model1)\n",
    "#in Lee & Wagenmakers words: \"In the presentation of results, but not the presentation to \n",
    "#subjects in the experiment, the stimulus pairs are coded so that choice “a” always \n",
    "#corresponds to the TTB choice\"\n",
    "tmat = np.tile(t[np.newaxis, :], (ns, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejercicio en clase\n",
    "\n",
    "Traduzca el modelo gráfico a PyMC3. Está abajo, no lo vea. Luego de hacer el suyo comparelos. ¿Son iguales o qué falto/mejoró?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/7_CB/model_TTB.svg\" width = \"55\" height = '55'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Modelos basados en https://github.com/junpenglao/Bayesian-Cognitive-Modeling-in-Pymc3\n",
    "with pm.Model() as model1:\n",
    "    gamma = pm.Uniform(\"gamma\", lower=0.5, upper=1)\n",
    "    gammat = tt.stack([1 - gamma, 0.5, gamma])\n",
    "\n",
    "    yiq = pm.Bernoulli(\"yiq\", p=gammat[tmat], observed=y)\n",
    "    trace1 = pm.sample()\n",
    "    ppc = pm.sample_posterior_predictive(trace1, samples=5000)\n",
    "    data = az.from_pymc3(trace=trace1)\n",
    "\n",
    "az.plot_trace(data, var_names=[\"gamma\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "gamma = trace1[\"gamma\"]\n",
    "fig, axes = plt.subplots(1, 1, figsize=(15, 4))\n",
    "my_pdf1 = st.kde.gaussian_kde(gamma)\n",
    "x1 = np.linspace(0.68, 0.82, 200)\n",
    "axes.plot(x1, my_pdf1(x1), \"k\", lw=2.5, alpha=0.6)  # distribution function\n",
    "axes.set_xlim((0.66, 0.84))\n",
    "axes.set_xlabel(r\"$\\gamma$ (probability of choosing TTB)\", fontsize=15)\n",
    "axes.set_ylabel(\"Posterior Density\", fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\gamma$ no es 1. Es decir, las personas no siempre escogen la take-the-best pero si con alta probabilidad (el MAP es alrededor de 0.75)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "az.style.use(\"arviz-darkgrid\")\n",
    "yiqpred = np.asarray(ppc[\"yiq\"])\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "x1 = np.repeat(np.arange(ns) + 1, nq).reshape(ns, -1).flatten()\n",
    "y1 = np.repeat(np.arange(nq) + 1, ns).reshape(nq, -1).T.flatten()\n",
    "\n",
    "plt.scatter(y1, x1, s=np.mean(yiqpred, axis=0) * 200, c=\"w\") #size s of the dot is the probability of picking the best\n",
    "plt.scatter(y1[y.flatten() == 1], x1[y.flatten() == 1], marker=\"x\", c=\"r\") #x where subjs picked the best\n",
    "plt.plot(np.ones(100) * 24.5, np.linspace(0, 21, 100), \"--\", lw=1.5, c=\"k\")\n",
    "plt.axis([0, 31, 0, 21]);\n",
    "plt.ylabel('Subjects')\n",
    "plt.xlabel('Question');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "La grilla esta completa con x rojas en un 75% aproximadamente (el sujeto escogió la take-the-best). El modelo predice en cada pregunta esa probabilidad: el tamaño de los circulos es el promedio del posterior predictive check y son casi constantes (también ver la posterior gamma).\n",
    "\n",
    "La línea punteada es para resaltar que algunos sujetos (x rojas) no se fueron por TTB en la preguntas finales.\n",
    "\n",
    "Depronto usaron TTB y otra estrategia como sumar la evidencia de todos los cues, no solo parar en el mejor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Latent-mixture model of take-the-best and WADD\n",
    "\n",
    "Modelo aditivo ponderado (Weighted ADDitive (WADD) model). El WADD, suma la evidencia en todos los cues para ambas alternativas, y escoge la de mayor evidencia.\n",
    "\n",
    "$$ \\phi \\sim \\text{Uniform}(0,1)$$\n",
    "$$ z_i \\sim \\text{Bernoulli}(\\phi)$$\n",
    "$$ \\gamma \\sim \\text{Uniform}(0.5,1)$$  \n",
    "$$ t_{iq} = \n",
    "\\begin{cases}\n",
    "\\text{TTB}_s\\,(\\mathbf a_q,\\mathbf b_q) & \\text{if $z_i = 1$} \\\\\n",
    "\\text{WADD}\\,(\\mathbf a_q,\\mathbf b_q) & \\text{if $z_i = 0$} \\\\\n",
    "\\end{cases}  $$  \n",
    "\n",
    "$$ y_{iq} \\sim\n",
    "\\begin{cases}\n",
    "\\text{Bernoulli}(\\gamma) & \\text{if $t_{iq} = a$} \\\\\n",
    "\\text{Bernoulli}(1- \\gamma) & \\text{if $t_{iq} = b$} \\\\\n",
    "\\text{Bernoulli}(0.5) & \\text{otherwise}\n",
    "\\end{cases}  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejercicio en clase\n",
    "\n",
    "Haga el diagrama que represente las formulas. Está abajo, no lo vea. Luego de hacer el suyo comparelos. ¿Son iguales o qué falto/mejoró?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/7_CB/model_TTB_WADD.svg\" width = \"60\" height = '60'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Question cue contributions template\n",
    "qcc = np.zeros((nq, nc))\n",
    "for q in range(nq):\n",
    "    # Add Cue Contributions To Mimic TTB Decision\n",
    "    for j in range(nc):\n",
    "        qcc[q, j] = m[p[q, 0] - 1, j] - m[p[q, 1] - 1, j]\n",
    "\n",
    "qccmat = np.tile(qcc[np.newaxis, :, :], (ns, 1, 1))\n",
    "# TTB Model For Each Question\n",
    "s = np.argsort(v)  # s[1:nc] <- rank(v[1:nc])\n",
    "smat = np.tile(s[np.newaxis, :], (ns, nq, 1)) #mat is for matrix\n",
    "ttmp = np.sum(qccmat * np.power(2, smat), axis=2) \n",
    "tmat = -1 * (-ttmp > 0) + (ttmp > 0) + 1 #0 (b_q), 1 (neither), or 2 (a_q) ... index for the yiq distribution (see pymc model)\n",
    "t = tmat[0]\n",
    "\n",
    "# WADD Model For Each Question \n",
    "# We no longer use non-compensatory weights (np.power(2, smat))\n",
    "xmat = np.tile(x[np.newaxis, :], (ns, nq, 1)) #mat is for matrix\n",
    "wtmp = np.sum(qccmat * xmat, axis=2)\n",
    "wmat = -1 * (-wtmp > 0) + (wtmp > 0) + 1 #0 (b_q), 1 (neither), or 2 (a_q) ... index for the yiq distribution (see pymc model)\n",
    "w = wmat[0]\n",
    "\n",
    "#\"In the presentation of results, but not the presentation to \n",
    "#subjects in the experiment, the stimulus pairs are coded so that choice “a” always \n",
    "#corresponds to the TTB choice\"\n",
    "t, w #Note that WADD manages to pick in the last six questions the worst option (b_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejercicio en clase\n",
    "\n",
    "Traduzca el modelo gráfico a PyMC3. Está abajo, no lo vea. Luego de hacer el suyo comparelos. ¿Son iguales o qué falto/mejoró?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model2:\n",
    "    phi = pm.Beta(\"phi\", alpha=1, beta=1, testval=0.01)\n",
    "\n",
    "    zi = pm.Bernoulli(\n",
    "        \"zi\",\n",
    "        p=phi,\n",
    "        shape=ns,\n",
    "        testval=np.asarray(\n",
    "            [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        ),\n",
    "    )\n",
    "    zi_ = tt.reshape(tt.repeat(zi, nq), (ns, nq))\n",
    "\n",
    "    gamma = pm.Uniform(\"gamma\", lower=0.5, upper=1)\n",
    "    gammat = tt.stack([1 - gamma, 0.5, gamma])\n",
    "\n",
    "    t2 = tt.switch(tt.eq(zi_, 1), tmat, wmat)\n",
    "    yiq = pm.Bernoulli(\"yiq\", p=gammat[t2], observed=y)\n",
    "\n",
    "    trace2 = pm.sample()\n",
    "    ppc2 = pm.sample_posterior_predictive(trace2, samples=5000)\n",
    "    data2 = az.from_pymc3(trace=trace2)\n",
    "\n",
    "az.plot_trace(data2, compact=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 4))\n",
    "zitrc = trace2[\"zi\"]\n",
    "plt.bar(np.arange(ns) + 1, 1 - np.mean(zitrc, axis=0))\n",
    "plt.yticks([0, 1], (\"TTB\", \"WADD\"))\n",
    "plt.xlabel(\"Subject\")\n",
    "plt.ylabel(\"Group\")\n",
    "plt.axis([0, 21, 0, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.style.use(\"arviz-darkgrid\")\n",
    "yiqpred = np.asarray(ppc2[\"yiq\"])\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "x1 = np.repeat(np.arange(ns) + 1, nq).reshape(ns, -1).flatten()\n",
    "y1 = np.repeat(np.arange(nq) + 1, ns).reshape(nq, -1).T.flatten()\n",
    "\n",
    "plt.scatter(y1, x1, s=np.mean(yiqpred, axis=0) * 200, c=\"w\") #size s of the dot is the probability of picking the best\n",
    "plt.scatter(y1[y.flatten() == 1], x1[y.flatten() == 1], marker=\"x\", c=\"r\") #x where subjs picked the best\n",
    "plt.plot(np.ones(100) * 24.5, np.linspace(0, 21, 100), \"--\", lw=1.5, c=\"k\")\n",
    "plt.axis([0, 31, 0, 21]);\n",
    "plt.ylabel('Subjects')\n",
    "plt.xlabel('Question');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejercicio\n",
    "\n",
    "1) Comparé la grilla TTB con la que acabamos de hacer TTB + WADD. Comente cuál es la principal diferencia para las seis preguntas finales. Tip: salve ambas figuras y pongalas juntas.\n",
    "\n",
    "2) Comparé el modelo que acabamos de correr con uno donde $\\phi = 0$. Es decir, un modelo nulo con otro donde incluimos $\\phi$ ¿Cuál es mejor? Use criterios de información o factores de Bayes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Ordenes de cues diferentes\n",
    "\n",
    "¿Es valido el supuesto que todos usan el mismo orden de los 9 cues? ¿O diferentes individuos usan diferentes ordenes? Pongamos un prior a validez de los cues por individuo $v_{ic}$\n",
    "\n",
    "$$ v_{ic} \\sim Normal(0,0.001)$$\n",
    "$$ s_i = rank \\ order(v_{iq})$$\n",
    "$$ t_{iq} = \\text{TTB}_{si}(\\mathbf a_q,\\mathbf b_q)$$\n",
    "$$ \\gamma \\sim \\text{Uniform}(0.5,1)$$  \n",
    "$$ y_{iq} \\sim\n",
    "\\begin{cases}\n",
    "\\text{Bernoulli}(\\gamma) & \\text{if $t_{iq} = a$} \\\\\n",
    "\\text{Bernoulli}(1- \\gamma) & \\text{if $t_{iq} = b$} \\\\\n",
    "\\text{Bernoulli}(0.5) & \\text{otherwise}\n",
    "\\end{cases}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/7_CB/model_TTB_order.svg\" width = \"600\" height = '600'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "nq = int(nq)\n",
    "with pm.Model() as model3:\n",
    "    gamma = pm.Uniform(\"gamma\", lower=0.5, upper=1)\n",
    "    gammat = tt.stack([1 - gamma, 0.5, gamma])\n",
    "\n",
    "    v1 = pm.HalfNormal(\"vic\", sd=1, shape=ns * nc)\n",
    "    s1 = pm.Deterministic(\"si\", tt.argsort(v1.reshape((ns, 1, nc)), axis=2))\n",
    "    smat2 = tt.tile(s1, (1, nq, 1))  # s[1:nc] <- rank(v[1:nc])\n",
    "\n",
    "    # TTB Model For Each Question\n",
    "    ttmp = tt.sum(qccmat * tt.power(2, smat2), axis=2)\n",
    "    tmat = -1 * (-ttmp > 0) + (ttmp > 0) + 1\n",
    "\n",
    "    yiq = pm.Bernoulli(\"yiq\", p=gammat[tmat], observed=y)\n",
    "    \n",
    "    #Junpeng Lao uses metropolis because the model geometry seems to be unsmooth ()\n",
    "    trace3 = pm.sample(100000, step=pm.Metropolis(), compute_convergence_checks=False)\n",
    "    ppc3 = pm.sample_posterior_predictive(trace3, samples=5000)\n",
    "    data3 = az.from_pymc3(trace=trace3)\n",
    "\n",
    "az.plot_trace(data3, var_names=[\"gamma\", \"vic\"], compact=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "burnin = 50_000\n",
    "# v1trace = np.squeeze(trace3['v1'][burnin:])\n",
    "# s1trace = np.argsort(v1trace, axis=2)\n",
    "s1trace = np.squeeze(trace3[burnin:][\"si\"]) #sampled orders\n",
    "\n",
    "for subj_id in [12, 13]:\n",
    "    subj_s = np.squeeze(s1trace[:, subj_id - 1, :]) #sampled orders for subject\n",
    "    unique_ord = np.vstack(list({tuple(row) for row in subj_s})) #detects unique orders (a set does not repeat values)\n",
    "    num_display = 10\n",
    "    print(\"Subject %s\" % (subj_id))\n",
    "    print(\n",
    "        \"There are %s search orders sampled in the posterior.\" % (unique_ord.shape[0])\n",
    "    )\n",
    "\n",
    "    mass_ = []\n",
    "    for s_ in unique_ord:\n",
    "        mass_.append(np.mean(np.sum(subj_s == s_, axis=1) == len(s_)))\n",
    "    mass_ = np.asarray(mass_)\n",
    "    sortmass = np.argsort(mass_)[::-1]\n",
    "\n",
    "    for i in sortmass[:num_display]:\n",
    "        s_ = unique_ord[i]\n",
    "        print(\"Order=(\" + str(s_ + 1) + \"), Estimated Mass=\" + str(mass_[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[trace3[burnin:][\"si\"].shape, s1trace.shape]\n",
    "subj_id = 12\n",
    "[s1trace[:, subj_id - 1, :].shape, np.squeeze(s1trace[:, subj_id - 1, :]).shape]\n",
    "unique_ord.shape\n",
    "\n",
    "\n",
    "\n",
    "a = [[1,2], [1,3], [1,2]]\n",
    "d = list({tuple(row) for row in a})\n",
    "d = {tuple(row) for row in a}\n",
    "np.vstack(d)\n",
    "#np.array(d)\n",
    "len(s_)\n",
    "s_ = unique_ord[0]\n",
    "print(s_.shape)\n",
    "np.sum(subj_s == s_, axis=1).shape\n",
    "subj_s.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The return order is not at all similar to the result in JAGS (as shown in the book on p.233)\n",
    "\n",
    "In general, order seems to be hard to obtain. There may be an specification problem (e.g. different orders work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejercicio en clase\n",
    "\n",
    "* Haga el diagrama de un modelo que combine TTB + WADD y diferentes ordenes. Esta en el libro de Lee & Wagenmakers luego de hacer el suyo comparelos. ¿Son iguales o qué falto/mejoró? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Is less information truly better? Not so fast: a bayesian argument\n",
    "<center><img src=\"img/7_CB/Parpart1.png\" width = \"700\" height = '700'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Regression: uses all cues \\\n",
    "Tallying/TTB: searches cues based on their criterion validity (obtained by experience); disregards covariance between cues (e.g. correlations between goals and league position)\n",
    "\n",
    "<center><img src=\"img/7_CB/Parpart2.png\" width = \"700\" height = '700'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Un argumento clave en la teoría de heurísticas es *less-is-more*: \n",
    "\n",
    "> “There is a point where too much information and too much information processing can hurt” Gigerenzer and Todd (1999) (p. 21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Para confirmar, Paula Parpart y colegas proponen hacer una comparación entre regresión y heurísticas tipo tallying en un marco probabílisitico\n",
    "\n",
    "$$ Respuesta = \\beta_1  Cue_1 + \\beta_2 Cue_2 + ... + \\beta_k Cue_k  $$\n",
    "\n",
    "La respuesta a la pregunta (e.g. ¿cuál casa es más cara?) es una suma ponderada de los valores de los cues (>0 resp. A; <0 resp. B). \n",
    "\n",
    "Una regresión tradicional busca los mejores $\\beta$ mientras que tallying tradicional pondría todos en 1. \n",
    "\n",
    "¿Cuál es mejor modelo cognitivo? Segun Gigerenzer, et al, tallying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La propuesta bayesiana es poner un prior sobre los $\\beta_k$ (y un likelihood que más adelante definimos cuando construyamos los modelos)\n",
    "\n",
    "$$p(\\beta_k) \\sim Normal(0,\\eta) $$\n",
    "\n",
    "Si $\\eta$, la desviación estandard, es grande la inferencia es equivalente a una regresión tradicional e.g. MCMC puede buscar $\\beta_k$ sin guiarse por un prior fuerte. \n",
    "\n",
    "Si $\\eta$ es pequeño Parpart et al, 2018 muestra que para cualquier par $\\beta_i, \\ \\beta_j$, condicionado a los cues X y respuestas Y, su ratio, cuando $\\eta$ se acerca a cero, es la unidad (i.e. tallying), \n",
    "\n",
    "$$\\lim_{\\eta \\to 0} \\frac{\\mathbb{E}[\\beta_i | X, y]}{\\mathbb{E}[\\beta_j | X, y]} = 1 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    ">\" ... we find that best performance comes from intermediate models on the continuum, which do not entirely ignore cue weights or cue covariance but that nonetheless down-weight this information via the influence of their priors \" Parpart, et al, 2018, pp(128).\n",
    "<center><img src=\"img/7_CB/Parpart3.png\" width = \"700\" height = '700'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* La versión fuerte de less-is-more, donde se descarta toda la información de la importancia de los cues (e.g. tallying) es falsa.\n",
    "\n",
    "* Por otro lado, no se hace optimización pura (i.e. prior centrado en cero). Los sesgos inductivos basados en heurísticas afectan procesos cognitivos.\n",
    "\n",
    "> \" ... we provide a formal understanding of why heuristics can outperform full-information models by placing all models in a common probabilistic inference framework, where heuristics correspond to extreme priors that will usually be outperformed by intermediate models that use all available information\" Parpart, et al, 2018, pp(134)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ahora implementemos el paper de Paula Parpart y colegas usando programación probabilística en python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Data setup\n",
    "#IMPORTANT NOTE: \n",
    "#choice data (i.e. dependent below) are ideal responses based on the actual criterion\n",
    "def data_setup_parpart(idx_data, training_samples):\n",
    "    #idx_data: scalar, data index in ALL_DATA\n",
    "    #percent_training: scalar between 0-1, percentage of all data used for training\n",
    "    \n",
    "    \n",
    "    #ALL_DATA: ordered as Fig. 3 of Parpart et al 2018.\n",
    "    ALL_DATA = [\"house.world\",\"mortality\",\"cit.world\",\"prf.world\",\"bodyfat.world\", \"car.world\",\"cloud\",\n",
    "            \"dropout\",\"fat.world\", \"fuel.world\", \"glps\",\n",
    "            \"homeless.world\", \"landrent.world\", \"mammal.world\", \"oxidants\",\n",
    "            \"attractiveness.men\", \"attractiveness.women\", \"fish.fertility\",\"oxygen\", \"ozone\"]\n",
    "\n",
    "    y_pos = 2 #column position of dependent variable (criterion) i.e. correct answer to question is based on this \n",
    "    #in dataset, 0 below median, 1 above median. \n",
    "    #Gigerenzer et al, converted continuous variables to median splits \n",
    "    dataset = pd.read_table(\"data/7_CB/Parpart 2018/Data/\" + ALL_DATA[idx_data] + \".txt\")\n",
    "    idx = list(range(4,dataset.shape[1])) #make sure all cues start at column 4\n",
    "    col_cues = np.array(idx) #idx of columns with cues\n",
    "    labels_cues = dataset.columns[idx]\n",
    "    Predictors = len(labels_cues) #number of cues\n",
    "    N = dataset.shape[0] # number of objects e.g cities\n",
    "    #k = 100 #number of partitions for cross validations\n",
    "\n",
    "    #Create Paired Data (ALL binary comparisons of objects e.g. cities)\n",
    "    comb = np.array(list(combinations(list(range(N)), 2)))\n",
    "    idx = np.stack([np.random.choice([0,1], 2, replace = False) for rep in range(comb.shape[0])]) #[0,1] shuffled many times\n",
    "    comb = np.transpose(np.stack([comb[i,ele] for i,ele in enumerate(idx)])) #columns shuffled\n",
    "    y = np.repeat(np.nan, comb.shape[1]) # correct classification; A(+1) or B(-1)  \n",
    "    difference = np.repeat(np.nan, comb.shape[1]) \n",
    "    bdata_diff = pd.DataFrame(np.nan, index=np.arange(comb.shape[1]), columns=labels_cues)\n",
    "    for i in range(comb.shape[1]):\n",
    "        # takes out only the 2 rows from dataset that are compared at step i \n",
    "        binary = dataset.loc[comb[:,i],:].reset_index(drop=True) #2 random rows\n",
    "        if i == 0:\n",
    "            comparisons = binary\n",
    "        else:\n",
    "            comparisons = pd.concat([comparisons, binary])\n",
    "\n",
    "        ## always compare row 1 with row 2 (no matter which ones has the higher criterion value) upper row - lower row\n",
    "        if binary.iloc[0,y_pos] > binary.iloc[1,y_pos]:\n",
    "            y[i] = 1 #(A)\n",
    "        else:\n",
    "            y[i] = - 1 #(B)\n",
    "\n",
    "        ## cue values (row 1) - cue values (row 2) \n",
    "        bdata_diff.loc[i,:] = binary.loc[0,labels_cues] - binary.loc[1,labels_cues] # \n",
    "\n",
    "    bdata_diff['dependent'] = y\n",
    "    paired_data = copy.deepcopy(bdata_diff)\n",
    "    dataset = copy.deepcopy(paired_data)\n",
    "    \n",
    "    # Assess paired_data cue validities and order as v= R/R+W  ------R:right, W:wrong\n",
    "    cue_validities_raw = np.repeat(np.nan, Predictors)\n",
    "    cue_validities = np.repeat(np.nan, Predictors) #between 0 (does not predict which is better) and 1 (always predicts which is better)\n",
    "    for c in range(Predictors):\n",
    "        condition = (paired_data.iloc[:,c]==paired_data.loc[:,'dependent']).sum() == 0\n",
    "        if condition: # stays 0 now if it was 0 \n",
    "            cue_validities[c] = 0\n",
    "        else:\n",
    "            cue_validities_raw[c] = (paired_data.iloc[:,c]==paired_data.loc[:, 'dependent']).sum()/((paired_data.iloc[:,c]==1).sum()+(paired_data.iloc[:,c]==-1).sum()) \n",
    "            cue_validities[c] = cue_validities_raw[c] - 0.5 #the 0.5 is to make a 0.5 validity 0. Parpart's code says that this brings back to same scale as regression weights as otherwise order can be different!\n",
    "    cue_order = np.argsort(-abs(cue_validities)) \n",
    "    \n",
    "\n",
    "    # number of objects (e.g. paired cities comparisons) after evening out\n",
    "    N = dataset.shape[0]\n",
    "\n",
    "    #Partitions for cross-validation\n",
    "    percent_training = training_samples/N\n",
    "    # Generate the cross-validation partitions: \n",
    "    percent =(1 - percent_training)  #### Hold the testset (distinct from random training set) \n",
    "    training_sample_size = percent_training*N\n",
    "    #re = np.repeat(np.nan, k) # resampling\n",
    "    #i = 0 #partition number (see k above) (looping var)\n",
    "    trainset, testset = train_test_split(dataset, test_size=percent)\n",
    "    trainset = trainset.reset_index(drop=True)\n",
    "    testset = testset.reset_index(drop=True)\n",
    "    #print([trainset.shape, testset.shape, dataset.shape, training_sample_size])\n",
    "    Predictors = trainset.shape[1]-1\n",
    "    #Re-shuffling zero variance cases (incompatible with COR model) \n",
    "    cov_mat = trainset[labels_cues].corr()\n",
    "    # NA cases = zero variance cases, get resampled now until one is found without any zero variance cases\n",
    "    max_while = 1000000\n",
    "    mm = 0\n",
    "    while cov_mat.isna().any(axis = None) and mm<=max_while:\n",
    "        trainset, testset = train_test_split(dataset, test_size=percent)\n",
    "        trainset = trainset.reset_index(drop=True) \n",
    "        testset = testset.reset_index(drop=True)\n",
    "        cov_mat = trainset[labels_cues].corr()\n",
    "        if mm == max_while:\n",
    "            raise NameError('Reshuffling zero variance cases took too long')\n",
    "    \n",
    "    return trainset, testset, Predictors, labels_cues, ALL_DATA[idx_data], cue_validities, cue_order, pd.read_table(\"data/7_CB/Parpart 2018/Data/\" + ALL_DATA[idx_data] + \".txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "idx_data = 13 #There are 20 data sets i.e. between 0 and 19 (13 shows the effect clearly; Fig 3 Parpart)\n",
    "#Problematic sets (e.g in sampling; or in partitions that take too long): 1, 6, 8,10,14,19 ... predictions for 7 are off\n",
    "training_samples = 30 #for some datasets more than 50 wont work \n",
    "penalties = np.array([0.001, 0.01, 0.1, 1, 10, 200])\n",
    "npart = 10 #number of partitions (Parpart et al use 1000 because they use analytic results; with sampling it would take too long)\n",
    "predictions = np.zeros((penalties.shape[0], npart))\n",
    "for k in range(npart):\n",
    "    trainset, testset, Predictors, labels_cues, dataused, cue_validities, cue_order, dataset_original = data_setup_parpart(idx_data, training_samples)\n",
    "    cue_order2 = np.argsort(-cue_validities)\n",
    "    x = trainset.loc[:,labels_cues].iloc[:,cue_order2] # assumption that cue directionalities are known in advance (Dawes, 1979)  \n",
    "    y = trainset['dependent']\n",
    "    criterion = dataset_original.columns[2] #theme of the question e.g. higher price house for data set \"house.world\"\n",
    "    y[y<0] = 0 #changes dummy coding (option B:0; option A:1)\n",
    "    dirweights = mf.sign(cue_validities[cue_order2]) # assumption that cue directionalities are known in advance (Dawes, 1979)  \n",
    "    col_pos = (dirweights > 0)*1\n",
    "    eta = 0.001 #Place holder, below I put this in a loop. Parpart calls them penalties\n",
    "    mixed_cues = any(col_pos==0) and any(col_pos==1)\n",
    "    with pm.Model() as Half_Ridge:\n",
    "        eta_Ridge =  pm.Data(\"eta\", eta)\n",
    "        #Priors\n",
    "        #Weights\n",
    "        posNormal = pm.Bound(pm.Normal, lower=0.0)\n",
    "        negNormal = pm.Bound(pm.Normal, upper=0.0)\n",
    "        if mixed_cues:#some cues are positively and other negatively related to the criterion\n",
    "            weight_pos = posNormal('weights_pos', mu=0, sigma=eta_Ridge, shape = (col_pos==1).sum())\n",
    "            weight_neg = negNormal('weights_neg', mu=0, sigma=eta_Ridge, shape = (col_pos==0).sum())\n",
    "            weights = pm.Deterministic('weights', tt.concatenate([weight_pos, weight_neg]))\n",
    "        elif any(col_pos==1): #all cues are positively related to the criterion\n",
    "            weight_pos = posNormal('weights_pos', mu=0, sigma=eta_Ridge, shape = (col_pos==1).sum())\n",
    "            weights = pm.Deterministic('weights', weight_pos)\n",
    "        elif any(col_pos==0): #all cues are negatively related to the criterion\n",
    "            weight_neg = negNormal('weights_neg', mu=0, sigma=eta_Ridge, shape = (col_pos==0).sum())\n",
    "            weights = pm.Deterministic('weights', weight_pos)\n",
    "        print(weights.tag.test_value.shape)   \n",
    "\n",
    "\n",
    "        #Likelihood\n",
    "        mu = weights*x #rows stimulus, columns: cues, cells: cue*weight\n",
    "        print(mu.tag.test_value.shape)\n",
    "        theta = pm.Deterministic('theta', pm.math.sigmoid(tt.sum(mu, axis=1)))\n",
    "        print(theta.tag.test_value.shape) \n",
    "        y_1 = pm.Bernoulli('y_1', p=theta, observed=y) \n",
    "\n",
    "        #Sampling \n",
    "        #trace = pm.sample(1000, init = 'adapt_diag', tune=1500, target_accept = 0.95)\n",
    "        #ppc = pm.sample_posterior_predictive(trace, samples=5000)\n",
    "        #data = az.from_pymc3(trace=trace)\n",
    "\n",
    "\n",
    "    preds = []\n",
    "    observed_choices = testset['dependent']\n",
    "    observed_cues = testset.loc[:,labels_cues].iloc[:,cue_order2]\n",
    "    for eta in penalties:\n",
    "        with Half_Ridge:\n",
    "            pm.set_data({\"eta\": eta})\n",
    "            trace = pm.sample(1000, init = 'adapt_diag', tune=1500, target_accept = 0.95)\n",
    "            #ppc = pm.sample_posterior_predictive(trace, samples=5000)\n",
    "            #data = az.from_pymc3(trace=trace)\n",
    "        mean_weights = trace['weights'].mean(axis=0)\n",
    "        y_hat = mean_weights*observed_cues  \n",
    "        z_i = y_hat.sum(axis=1)\n",
    "        p_i = 1/(1+np.exp(-z_i))\n",
    "        choice_i = np.where(p_i>0.5, 1, np.where(p_i<0.5,-1,0)) #if likelihood is bernoulli\n",
    "        #choice_i = np.where(z_i>0, 1, np.where(z_i<0,-1,0)) #if likelihood is normal\n",
    "        prediction_accuracy = (observed_choices==choice_i).mean()\n",
    "        preds.append(prediction_accuracy)\n",
    "        #print(prediction_accuracy)\n",
    "        print(\"Partition: \", k, \"Data set used: \", dataused, \"Accuracy: \", prediction_accuracy, \"eta: \", eta)\n",
    "    predictions[:,k] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "xp = list(range(len(penalties)))\n",
    "yp = predictions[-1:None:-1,:]\n",
    "plt.errorbar(xp, yp.mean(axis=1), \n",
    "             yerr=yp.std(axis=1)/np.sqrt(npart))\n",
    "plt.scatter(xp, yp.mean(axis=1))\n",
    "plt.title(dataused + \"\\nTraining size: \" + str(training_samples) + '; n tests: ' + str(npart));\n",
    "plt.xlabel('Prior\\'s precision (1/$\\eta^2$) \\n small-regression; large-tallying')\n",
    "plt.ylabel('Test Performance(%)')\n",
    "ttic = []\n",
    "for i, xt in enumerate(np.round(1/(penalties**2),4)[-1:None:-1]): ttic.append(np.format_float_scientific(xt))\n",
    "plt.xticks(ticks = xp, labels = ttic);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ahora tratemos de inferir $\\eta$.\n",
    "\n",
    "¿Que hacemos? ¿Cómo extendemos el modelo half-ridge?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_data = 13 #There are 20 data sets i.e. between 0 and 19\n",
    "#Problematic sets (e.g in sampling; or in partitions that take too long): 1, 6, 8,10,14,19 ... predictions for 7 are off\n",
    "training_samples = 50 #for some datasets more than 20 wont work \n",
    "trainset, testset, Predictors, labels_cues, dataused, cue_validities, cue_order, dataset_original = data_setup_parpart(idx_data, training_samples)\n",
    "cue_order2 = np.argsort(-cue_validities)\n",
    "x = trainset.loc[:,labels_cues].iloc[:,cue_order2] # assumption that cue directionalities are known in advance (Dawes, 1979)  \n",
    "y = trainset['dependent']\n",
    "criterion = dataset_original.columns[2] #theme of the question e.g. higher price house for data set \"house.world\"\n",
    "y[y<0] = 0 #changes dummy coding (option B:0; option A:1)\n",
    "dirweights = mf.sign(cue_validities[cue_order2]) # assumption that cue directionalities are known in advance (Dawes, 1979)  \n",
    "col_pos = (dirweights > 0)*1\n",
    "mixed_cues = any(col_pos==0) and any(col_pos==1)\n",
    "with pm.Model() as Half_Ridge2:\n",
    "\n",
    "    #Priors\n",
    "    #eta\n",
    "    eta_Ridge = pm.Uniform('eta', lower = 0.001, upper = 10)\n",
    "    #Weights\n",
    "    posNormal = pm.Bound(pm.Normal, lower=0.0)\n",
    "    negNormal = pm.Bound(pm.Normal, upper=0.0)\n",
    "    if mixed_cues:#some cues are positively and other negatively related to the criterion\n",
    "        weight_pos = posNormal('weights_pos', mu=0, sigma=eta_Ridge, shape = (col_pos==1).sum())\n",
    "        weight_neg = negNormal('weights_neg', mu=0, sigma=eta_Ridge, shape = (col_pos==0).sum())\n",
    "        weights = pm.Deterministic('weights', tt.concatenate([weight_pos, weight_neg]))\n",
    "    elif any(col_pos==1): #all cues are positively related to the criterion\n",
    "        weight_pos = posNormal('weights_pos', mu=0, sigma=eta_Ridge, shape = (col_pos==1).sum())\n",
    "        weights = pm.Deterministic('weights', weight_pos)\n",
    "    elif any(col_pos==0): #all cues are negatively related to the criterion\n",
    "        weight_neg = negNormal('weights_neg', mu=0, sigma=eta_Ridge, shape = (col_pos==0).sum())\n",
    "        weights = pm.Deterministic('weights', weight_pos)\n",
    "    print(weights.tag.test_value.shape)   \n",
    "    \n",
    "\n",
    "    #Likelihood\n",
    "    mu = weights*x #rows stimulus, columns: cues, cells: cue*weight\n",
    "    print(mu.tag.test_value.shape)\n",
    "    theta = pm.Deterministic('theta', pm.math.sigmoid(tt.sum(mu, axis=1)))\n",
    "    print(theta.tag.test_value.shape) \n",
    "    y_1 = pm.Bernoulli('y_1', p=theta, observed=y) \n",
    "\n",
    "    #Sampling \n",
    "    trace = pm.sample(1000, init = 'adapt_diag', tune=1500, target_accept = 0.95)\n",
    "    #ppc = pm.sample_posterior_predictive(trace, samples=5000)\n",
    "    data = az.from_pymc3(trace=trace)\n",
    "\n",
    "observed_choices = testset['dependent']\n",
    "observed_cues = testset.loc[:,labels_cues].iloc[:,cue_order2]\n",
    "mean_weights = trace['weights'].mean(axis=0)\n",
    "y_hat = mean_weights*observed_cues  \n",
    "z_i = y_hat.sum(axis=1)\n",
    "p_i = 1/(1+np.exp(-z_i))\n",
    "choice_i = np.where(p_i>0.5, 1, np.where(p_i<0.5,-1,0)) #if likelihood is bernoulli\n",
    "#choice_i = np.where(z_i>0, 1, np.where(z_i<0,-1,0)) #if likelihood is normal\n",
    "prediction_accuracy = (observed_choices==choice_i).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Data set used: \", dataused)\n",
    "az.plot_trace(data, var_names=['eta', 'weights'], compact=True);\n",
    "print('Predictions correct: ', np.round(prediction_accuracy,4))\n",
    "print('Average eta: ', np.round(trace['eta'].mean(axis=0),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El parametro $\\eta$ NO está alrededor de cero (tallying puro) o muy alejado (regresión pura). \n",
    "\n",
    "El efecto less-is-more existe $\\left(\\frac{1}{\\eta^2}>0\\right)$ pero no en su versión extrema $\\left(\\frac{1}{\\eta^2}<\\infty\\right)$. Ver los pesos de la regresión, no están centrados en la misma posición como predice tallying puro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "az.plot_density(data, var_names=['eta']);\n",
    "plt.savefig('img/7_CB/eta_parpart.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ejercicios\n",
    "\n",
    "* Haga el diagrama del modelo half-ridge2\n",
    "\n",
    "* En el modelo half-ridge2: \n",
    "    * Ponga el promedio de los priors en +1 para los positivos, y -1 para los negativos para emular unit weights para $\\eta$ pequeños (i.e. un prior de tallying). Comente que observa diferente a cuando el prior de los weights está centrado en cero. \n",
    "        * Compare ambos (prior weights centrado en 0 vs +/-1) con criterios estadísticos (e.g. WAICC, o LOO). ¿Cuál es mejor según ese criterio?\n",
    "    * Cambie el likelihood del modelo por uno normal en vez de Bernoulli ¿Cambia algo? ¿Si, no, por qué?\n",
    "        * Compare el modelo Normal y Bernoulli con criterios estadísticos (e.g. WAICC, o LOO). ¿Cuál es mejor según ese criterio?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Take-the-best\n",
    "dot_text = 'digraph G {rankdir=TB; compound=true; newrank=true; labelloc=\"t\";\\\n",
    "           label=\"TTB\";\\\n",
    "           /* general properties*/\\\n",
    "           node [margin=0, fixedsize=true, shape=plaintext,\\\n",
    "                 height=0.5, width=0.5, lblstyle=\"font=\\\\small\"];\\\n",
    "           /* links */\\\n",
    "           s -> tq;\\\n",
    "           aq -> tq;\\\n",
    "           bq -> tq;\\\n",
    "           tq -> yiq;\\\n",
    "           gamma -> yiq;\\\n",
    "           subgraph cluster0 {\\\n",
    "               margin = 10; labeljust=l; lblstyle=\"font=\\\\small\";\\\n",
    "               style = rounded;\\\n",
    "               label = \"$q questions$\";\\\n",
    "               tq;\\\n",
    "               aq;\\\n",
    "               bq;\\\n",
    "               subgraph cluster1 {\\\n",
    "                   margin = 10; labeljust=l; lblstyle=\"font=\\\\small\";\\\n",
    "                   style = rounded;\\\n",
    "                   label = \"$i subjects$\";\\\n",
    "                   yiq;\\\n",
    "               }\\\n",
    "           }\\\n",
    "           /* nodes */\\\n",
    "           s [label = \"$s$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           aq [label = \"$a_q$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           bq [label = \"$b_q$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           tq [label = \"$t_{q}$\", shape = square, peripheries = 2];\\\n",
    "           yiq [label = \"$y_{iq}$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           gamma [label = \"$gamma$\", shape = circle];\\\n",
    "           }' #warning: use single quote at start and end; double quotes for labels\n",
    "s = Source(dot_text, filename=\"img/7_CB/model_TTB.gv\", format=\"svg\") #THIS IS NOT THE FINAL ONE\n",
    "s.view()\n",
    "\n",
    "#distributions:\n",
    "# t_q = TTB_s(a_q, b_q)\n",
    "# \\gamma \\sim Uniform(0.5,1)\n",
    "# $$ y_{iq} \\sim\n",
    "#\\begin{cases}\n",
    "#\\text{Bernoulli}(\\gamma) & \\text{if $t_q = a$} \\\\\n",
    "#\\text{Bernoulli}(1- \\gamma) & \\text{if $t_q = b$} \\\\\n",
    "#\\text{Bernoulli}(0.5) & \\text{otherwise}\n",
    "#\\end{cases}  $$\n",
    "\n",
    "#To typeset latex stuff on the image: \n",
    "#1) open svg in inkscape and write latex formulas. Export as pdf (click the one that says latex)\n",
    "#   to change fontsize of latex in inkscape write before the expression: \n",
    "#        \\fontsize{34pt}{1em} $latex expression$ ... change #pt for size\n",
    "#2) go to overleaf or latex editor of choice and do this (https://castel.dev/post/lecture-notes-2/):\n",
    "#   2.1) In the preamble:\n",
    "#  \\usepackage{import}\n",
    "#  \\usepackage{xifthen}\n",
    "#  \\usepackage{pdfpages}\n",
    "#  \\usepackage{transparent}\n",
    "#  \\usepackage{graphics} \n",
    "\n",
    "#  \\newcommand{\\incfig}[1]{%\n",
    "#      \\def\\svgwidth{\\columnwidth}\n",
    "#      \\import{./figures/}{#1.pdf_tex} %PUT the inkscape .pdf_tex AND .pdf in a local folder called figures\n",
    "#  }\n",
    "#   2.2)In the body:\n",
    "#  \\begin{figure}[ht]\n",
    "#      \\centering\n",
    "#      \\scalebox{.65}{\\incfig{your_inkscape.pdf_tex}} #change scalebox proportion to rescale\n",
    "#      \\caption{Riemmans theorem}\n",
    "#      \\label{fig:riemmans-theorem}\n",
    "#  \\end{figure}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Latent-mixture model: Take-the-best + WADD\n",
    "dot_text = 'digraph G {rankdir=TB; compound=true; newrank=true; labelloc=\"t\";\\\n",
    "           label=\"TTB+WADD\";\\\n",
    "           /* general properties*/\\\n",
    "           node [margin=0, fixedsize=true, shape=plaintext,\\\n",
    "                 height=0.5, width=0.5, lblstyle=\"font=\\\\small\"];\\\n",
    "           /* links */\\\n",
    "           phi -> z;\\\n",
    "           z -> tq;\\\n",
    "           s -> tq;\\\n",
    "           aq -> tq;\\\n",
    "           bq -> tq;\\\n",
    "           tq -> yiq;\\\n",
    "           gamma -> yiq;\\\n",
    "           subgraph cluster0 {\\\n",
    "               margin = 10; labeljust=l; lblstyle=\"font=\\\\small\";\\\n",
    "               style = rounded;\\\n",
    "               label = \"$q questions$\";\\\n",
    "               tq;\\\n",
    "               aq;\\\n",
    "               bq;\\\n",
    "               subgraph cluster1 {\\\n",
    "                   margin = 10; labeljust=l; lblstyle=\"font=\\\\small\";\\\n",
    "                   style = rounded;\\\n",
    "                   label = \"$i subjects$\";\\\n",
    "                   yiq;\\\n",
    "                   tq;\\\n",
    "                   z;\\\n",
    "               }\\\n",
    "           }\\\n",
    "           /* nodes */\\\n",
    "           s [label = \"$s$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           aq [label = \"$a_q$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           bq [label = \"$b_q$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           tq [label = \"$t_{iq}$\", shape = square, peripheries = 2];\\\n",
    "           yiq [label = \"$y_{iq}$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           gamma [label = \"$gamma$\", shape = circle];\\\n",
    "           phi [label = \"$phi$\", shape = circle];\\\n",
    "           z [label = \"$z_i$\", shape = square];\\\n",
    "           }' #warning: use single quote at start and end; double quotes for labels\n",
    "s = Source(dot_text, filename=\"img/7_CB/model_TTB_WADD.gv\", format=\"svg\") #THIS IS NOT THE FINAL ONE\n",
    "s.view()\n",
    "\n",
    "#distributions:\n",
    "# \\phi \\sim Uniform(0,1)\n",
    "# \\z_i \\sim Bernoulli(\\phi)\n",
    "# \\gamma \\sim Uniform(0.5,1)\n",
    "#$$ t_{iq} = \n",
    "#\\begin{cases}\n",
    "#\\text{TTB}\\,(\\mathbf a_q,\\mathbf b_q) & \\text{if $z_i = 1$} \\\\\n",
    "#\\text{WADD}\\,(\\mathbf a_q,\\mathbf b_q) & \\text{if $z_i = 0$} \\\\\n",
    "#\\end{cases}  $$  \n",
    "# $$ y_{iq} \\sim\n",
    "#\\begin{cases}\n",
    "#\\text{Bernoulli}(\\gamma) & \\text{if $t_{iq} = a$} \\\\\n",
    "#\\text{Bernoulli}(1- \\gamma) & \\text{if $t_{iq} = b$} \\\\\n",
    "#\\text{Bernoulli}(0.5) & \\text{otherwise}\n",
    "#\\end{cases}  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Take-the-best different orders\n",
    "dot_text = 'digraph G {rankdir=TB; compound=true; newrank=true; labelloc=\"t\";\\\n",
    "           label=\"TTB+Order\";\\\n",
    "           /* general properties*/\\\n",
    "           node [margin=0, fixedsize=true, shape=plaintext,\\\n",
    "                 height=0.5, width=0.5, lblstyle=\"font=\\\\small\"];\\\n",
    "           /* links */\\\n",
    "           v -> s;\\\n",
    "           s -> tq;\\\n",
    "           aq -> tq;\\\n",
    "           bq -> tq;\\\n",
    "           tq -> yiq;\\\n",
    "           gamma -> yiq;\\\n",
    "           subgraph cluster0 {\\\n",
    "               margin = 10; labeljust=l; lblstyle=\"font=\\\\small\";\\\n",
    "               style = rounded;\\\n",
    "               label = \"$q \\\\\\ questions$\";\\\n",
    "               tq;\\\n",
    "               aq;\\\n",
    "               bq;\\\n",
    "               subgraph cluster1 {\\\n",
    "                   margin = 10; labeljust=l; lblstyle=\"font=\\\\small\";\\\n",
    "                   style = rounded;\\\n",
    "                   label = \"$i \\\\\\ subj$\";\\\n",
    "                   yiq;\\\n",
    "                   tq;\\\n",
    "                   s;\\\n",
    "                   subgraph cluster2{\\\n",
    "                       margin = 10; labeljust=l; lblstyle=\"font=\\\\small\";\\\n",
    "                       style = rounded;\\\n",
    "                       label = \"$c \\\\\\ cue$\";\\\n",
    "                       v;\\\n",
    "                   }\\\n",
    "               }\\\n",
    "           }\\\n",
    "           /* nodes */\\\n",
    "           v [label = \"$v_{ic}$\", shape = circle];\\\n",
    "           s [label = \"$s_i$\", peripheries = 2, shape = square];\\\n",
    "           aq [label = \"$a_q$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           bq [label = \"$b_q$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           tq [label = \"$t_{iq}$\", shape = square, peripheries = 2];\\\n",
    "           yiq [label = \"$y_{iq}$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           gamma [label = \"$\\\\\\gamma$\", shape = circle];\\\n",
    "           }' #warning: use single quote at start and end; double quotes for labels\n",
    "s = Source(dot_text, filename=\"img/7_CB/model_TTB_order.gv\", format=\"svg\") #THIS IS NOT THE FINAL ONE\n",
    "s.view()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Parpart et al, 2018 COR model\n",
    "\n",
    "def data_setup_parpart(idx_data, training_samples):\n",
    "    #idx_data: scalar, data index in ALL_DATA\n",
    "    #percent_training: scalar between 0-1, percentage of all data used for training\n",
    "    \n",
    "    \n",
    "    #idx_data = 0 #data index in ALL_DATA\n",
    "    ALL_DATA = [\"house.world\",\"mortality\",\"cit.world\",\"prf.world\",\"bodyfat.world\", \"car.world\",\"cloud\",\n",
    "            \"dropout\",\"fat.world\", \"fuel.world\", \"glps\",\n",
    "            \"homeless.world\", \"landrent.world\", \"mammal.world\", \"oxidants\",\n",
    "            \"attractiveness.men\", \"attractiveness.women\", \"fish.fertility\",\"oxygen\", \"ozone\"]\n",
    "\n",
    "    y_pos = 2 #column position of dependent variable (criterion) i.e. correct answer to question is based on this \n",
    "    #in dataset, 0 below median, 1 above median. \n",
    "    #Gigerenzer et al, converted continuous variables to median splits \n",
    "    dataset = pd.read_table(\"data/7_CB/Parpart 2018/Data/\" + ALL_DATA[idx_data] + \".txt\")\n",
    "    idx = list(range(4,dataset.shape[1])) #make sure all cues start at column 4\n",
    "    col_cues = np.array(idx) #idx of columns with cues\n",
    "    labels_cues = dataset.columns[idx]\n",
    "    Predictors = len(labels_cues) #number of cues\n",
    "    N = dataset.shape[0] # number of objects e.g cities\n",
    "    #k = 100 #number of partitions for cross validations\n",
    "\n",
    "    #Create Paired Data (ALL binary comparisons of objects e.g. cities)\n",
    "    comb = np.array(list(combinations(list(range(N)), 2)))\n",
    "    idx = np.stack([np.random.choice([0,1], 2, replace = False) for rep in range(comb.shape[0])]) #[0,1] shuffled many times\n",
    "    comb = np.transpose(np.stack([comb[i,ele] for i,ele in enumerate(idx)])) #columns shuffled\n",
    "    y = np.repeat(np.nan, comb.shape[1]) # correct classification; A(+1) or B(-1)  \n",
    "    difference = np.repeat(np.nan, comb.shape[1]) \n",
    "    bdata_diff = pd.DataFrame(np.nan, index=np.arange(comb.shape[1]), columns=labels_cues)\n",
    "    for i in range(comb.shape[1]):\n",
    "        # takes out only the 2 rows from dataset that are compared at step i \n",
    "        binary = dataset.loc[comb[:,i],:].reset_index(drop=True) #2 random rows\n",
    "        if i == 0:\n",
    "            comparisons = binary\n",
    "        else:\n",
    "            comparisons = pd.concat([comparisons, binary])\n",
    "\n",
    "        ## always compare row 1 with row 2 (no matter which ones has the higher criterion value) upper row - lower row\n",
    "        if binary.iloc[0,y_pos] > binary.iloc[1,y_pos]:\n",
    "            y[i] = 1 #(A)\n",
    "        else:\n",
    "            y[i] = - 1 #(B)\n",
    "\n",
    "        ## cue values (row 1) - cue values (row 2) \n",
    "        bdata_diff.loc[i,:] = binary.loc[0,labels_cues] - binary.loc[1,labels_cues] # \n",
    "\n",
    "    bdata_diff['dependent'] = y\n",
    "    paired_data = copy.deepcopy(bdata_diff)\n",
    "    dataset = copy.deepcopy(paired_data)\n",
    "    \n",
    "    # Assess paired_data cue validities and order as v= R/R+W  ------R:right, W:wrong\n",
    "    cue_validities_raw = np.repeat(np.nan, Predictors)\n",
    "    cue_validities = np.repeat(np.nan, Predictors) #between 0 (does not predict which is better) and 1 (always predicts which is better)\n",
    "    for c in range(Predictors):\n",
    "        condition = (paired_data.iloc[:,c]==paired_data.loc[:,'dependent']).sum() == 0\n",
    "        if condition: # stays 0 now if it was 0 \n",
    "            cue_validities[c] = 0\n",
    "        else:\n",
    "            cue_validities_raw[c] = (paired_data.iloc[:,c]==paired_data.loc[:, 'dependent']).sum()/((paired_data.iloc[:,c]==1).sum()+(paired_data.iloc[:,c]==-1).sum()) \n",
    "            cue_validities[c] = cue_validities_raw[c] - 0.5 #the 0.5 is to make a 0.5 validity 0. Parpart's code says that this brings back to same scale as regression weights as otherwise order can be different!\n",
    "    cue_order = np.argsort(-abs(cue_validities)) \n",
    "    \n",
    "\n",
    "    # number of objects (e.g. paired cities comparisons) after evening out\n",
    "    N = dataset.shape[0]\n",
    "\n",
    "    #Partitions for cross-validation\n",
    "    percent_training = training_samples/N\n",
    "    # Generate the cross-validation partitions: \n",
    "    percent =(1 - percent_training)  #### Hold the testset (distinct from random training set) \n",
    "    training_sample_size = percent_training*N\n",
    "    #re = np.repeat(np.nan, k) # resampling\n",
    "    #i = 0 #partition number (see k above) (looping var)\n",
    "    trainset, testset = train_test_split(dataset, test_size=percent)\n",
    "    trainset = trainset.reset_index(drop=True)\n",
    "    testset = testset.reset_index(drop=True)\n",
    "    #print([trainset.shape, testset.shape, dataset.shape, training_sample_size])\n",
    "    Predictors = trainset.shape[1]-1\n",
    "    #Re-shuffling zero variance cases (incompatible with COR model) \n",
    "    cov_mat = trainset[labels_cues].corr()\n",
    "    # NA cases = zero variance cases, get resampled now until one is found without any zero variance cases\n",
    "    max_while = 1000000\n",
    "    mm = 0\n",
    "    while cov_mat.isna().any(axis = None) and mm<=max_while:\n",
    "        trainset, testset = train_test_split(dataset, test_size=percent)\n",
    "        trainset = trainset.reset_index(drop=True) \n",
    "        testset = testset.reset_index(drop=True)\n",
    "        cov_mat = trainset[labels_cues].corr()\n",
    "        if mm == max_while:\n",
    "            raise NameError('Reshuffling zero variance cases took too long')\n",
    "\n",
    "    #Throwing out redundant predictors from x for both OLS and COR model fitting:      \n",
    "    lower_triangle = pd.DataFrame(np.tril(cov_mat,-1), index = labels_cues, columns= labels_cues)    \n",
    "    # == 1 has problems, so > .9999 grabs all the ones\n",
    "    if (lower_triangle>0.99999999).any(axis=None): # if there is at least 1 complete redundancy (TRUE) in the lower triangle, \n",
    "        eliminate = np.transpose(np.where(np.array(cov_mat>0.99999999))) #1st col: row position, 2nd col: column position\n",
    "        var_delete = np.repeat(np.nan, eliminate.shape[0])\n",
    "        for f in range(eliminate.shape[0]):\n",
    "            if eliminate[f,0] != eliminate[f,1]: # only take those that are not the matrix diagonal\n",
    "                var_delete[f] = eliminate[f,0] # store the row number of that first variable  \n",
    "        redundant = np.sort(pd.DataFrame(var_delete[var_delete > 0])[0].unique()).astype(int) # only take each variable once, in order, and only as many as necessary to get rid of redundancy\n",
    "        m = redundant.shape[0] # how many redundancies there are overall\n",
    "        # deleting m-1 of the redundant predictors still gets rid of all redundancies\n",
    "        trainset = trainset.drop(list(labels_cues[redundant[0:(m-1)]]), axis = 1)\n",
    "        testset = testset.drop(list(labels_cues[redundant[0:(m-1)]]), axis = 1)\n",
    "        #paired_data = copy.deepcopy(dataset.drop(list(labels_cues[redundant[0:(m-1)]]), axis = 1))\n",
    "        labels_cues = trainset.columns[0:-1]\n",
    "        Predictors = trainset.shape[1] - 1 # - dependent\n",
    "    #test = testset.drop(['dependent'],axis=1) \n",
    "    \n",
    "    \n",
    "    \n",
    "    return trainset, testset, Predictors, labels_cues, ALL_DATA[idx_data], cue_validities, cue_order, pd.read_table(\"data/7_CB/Parpart 2018/Data/\" + ALL_DATA[idx_data] + \".txt\")\n",
    "\n",
    "\n",
    "idx_data = 0 #There are 20 data sets i.e. between 0 and 19\n",
    "training_samples = 10 #more than 10 to 15 will not run, the model is misspecified as explained by Parpart\n",
    "trainset, testset, Predictors, labels_cues, dataused, cue_validities, cue_order, dataset_original = data_setup_parpart(idx_data, training_samples)\n",
    "x = trainset.loc[:,labels_cues]  \n",
    "y = trainset['dependent']\n",
    "criterion = dataset_original.columns[2]\n",
    "y[y<0] = 0 #changes dummy coding (option B:0; option A:1)\n",
    "eta = 50.001 #Place holder, below I put this in a loop. Parpart calls them penalties in her R code\n",
    "with pm.Model() as COR:\n",
    "    #IMPORTANT NOTE: more than 10 to 15 samples in training will not run\n",
    "    #The model is misspecified.\n",
    "    \n",
    "    #COR: Covariance Orthogonalizing Regularization (Paula Parpart's et al, 2018):\n",
    "    # \"the model architecture implements m regression problems at once, \n",
    "    # meaning the criterion variable y is regressed onto all cues m times (Fig. A1) ...\n",
    "    # ...the COR model is misspecified ... is artificially multivariate despite \n",
    "    # the original prediction problem being univariate. Nevertheless, the COR model \n",
    "    # opens up new insights into the role of cue covariance in establishing a continuum \n",
    "    # between heuristics that rely on cue validity and full-information models\"\n",
    "    \n",
    "    #Priors\n",
    "    #Weights matrix: \n",
    "    # \"an improper uniform prior on all W ii ( 1 ⩽ i ⩽ m ) and \n",
    "    # a prior of N (0, eta^2 ) for all W ij ( i ≠ j ). The joint \n",
    "    # distribution on W treats all weights as independent.\" (pp 143)\n",
    "    # As eta goes to zero, off diagonal weights squeeze towards zero. \n",
    "    # This is a penalty for multiple cues because single cues (diagonal weights) \n",
    "    # become more important and the use of multiple cues becomes irrelevant\n",
    "    \n",
    "    eta_COR =  pm.Data(\"eta\", eta)\n",
    "    w_diag = pm.Uniform('weights_diag', shape = Predictors, lower=-100, upper=100) #predictors are the cues (1D shape: (Predictors,))\n",
    "    #w_diag = pm.Uniform('weights_diag', shape = Predictors, lower=0, upper=100) #predictors are the cues (1D shape: (Predictors,))\n",
    "    temp = tt.eye(Predictors)\n",
    "    w_diag_reshape = w_diag*temp #(shape: Predictors X Predictors; diagonal: w_diag)\n",
    "    w_offdiag = pm.Normal('weights_offdiag', mu = 0, sigma = eta_COR, \n",
    "                          shape = (Predictors,Predictors-1))\n",
    "    left_zeros = tt.zeros((Predictors,1)) #to build a diagonal with zeros\n",
    "    bottom_zeros = tt.zeros((1,Predictors))\n",
    "    w_offdiag_reshape =  tt.concatenate([left_zeros, w_offdiag], axis = 1) \n",
    "    w_offdiag_reshape =  tt.concatenate([w_offdiag_reshape, bottom_zeros], axis = 0)\n",
    "    temp = tt.reshape(w_offdiag_reshape, (1, (Predictors+1)*Predictors))\n",
    "    w_offdiag_reshape = tt.reshape(temp[0, 0:-Predictors], (Predictors,Predictors)) #(shape: Predictors X Predictors; diagonal: zeros)\n",
    "    w_matrix = pm.Deterministic(\"weights\", w_diag_reshape + w_offdiag_reshape)\n",
    "    #print(w_matrix.tag.test_value.shape) #prints shape of theano variable\n",
    "    \n",
    "\n",
    "    #Likelihood\n",
    "    mu = pm.math.dot(x, w_matrix) #rows: observations; #columns: y_j; cell: weighted sum for observation i \n",
    "    theta = pm.Deterministic('theta', pm.math.sigmoid(mu))\n",
    "    print(mu.tag.test_value.shape, theta.tag.test_value.shape)\n",
    "    theta_mean = tt.mean(theta, axis = 0)\n",
    "    print(theta_mean.tag.test_value.shape) \n",
    "    y_multiplexed = np.transpose(np.tile(y,(Predictors,1)))\n",
    "    print(y_multiplexed.shape) \n",
    "    y_1 = pm.Bernoulli('y_1', p=theta, observed=y_multiplexed) \n",
    "\n",
    "    \n",
    "    #Sampling\n",
    "    #step = pm.Metropolis()\n",
    "    #trace = pm.sample(1000, init = 'adapt_diag', tune = 1500, step = step)\n",
    "    trace = pm.sample(1000, init = 'adapt_diag', tune=1500, target_accept = 0.95)\n",
    "    #ppc = pm.sample_posterior_predictive(trace, samples=5000)\n",
    "    data = az.from_pymc3(trace=trace)\n",
    "\n",
    "print(\"Data set used: \", dataused)\n",
    "print(\"Off-diagonal penalty used: \", eta)\n",
    "az.plot_trace(data, var_names=['weights_diag', \"weights_offdiag\"], compact=True);\n",
    "\n",
    "#TTB choice (Parpart et al, 2018, pp 132)\n",
    "#eta -> 0 pure TTB i.e. only diagonal weights matter\n",
    "#eta -> inf (or really large) linear regression\n",
    "mean_weights = pd.DataFrame(trace['weights'].mean(axis=0), \n",
    "                            columns = labels_cues, index = labels_cues)\n",
    "observed_choices = np.reshape(np.array(testset['dependent']), (1,testset.shape[0]))\n",
    "observed_cues = testset.loc[:,labels_cues]\n",
    "y_hat = np.dot(observed_cues, mean_weights) # rows choice, columns: summatory for set of weights\n",
    "j_i_star = np.argmax(np.abs(y_hat),axis=1) #index with largest absolute y_hat\n",
    "z_i = y_hat[range(y_hat.shape[0]),j_i_star]\n",
    "TTB_choice_i = np.where(z_i>0, 1, np.where(z_i<0,-1,0))\n",
    "prediction_accuracy = (observed_choices==TTB_choice_i).mean()\n",
    "prediction_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "rise": {
   "chalkboard": {
    "color": [
     "rgb(250, 0, 0)",
     "rgb(0, 250, 250)"
    ]
   },
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "simple",
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
