{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on PyMC3 v3.9.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "\n",
    "#Manejo de matrices y tablas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Estadistica y funciones matemáticas\n",
    "import scipy.stats as st\n",
    "from scipy.optimize import fmin\n",
    "from scipy import integrate\n",
    "from scipy.stats.mstats import mquantiles\n",
    "import statistics \n",
    "import pyreadr\n",
    "import scipy.io as sio\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Probabilistic programs\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt #NOTA: theano va a cambiar a tensorflow en PyMC4\n",
    "import theano\n",
    "from theano.compile.ops import as_op\n",
    "print('Running on PyMC3 v{}'.format(pm.__version__))\n",
    "\n",
    "#Graficas\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import altair as alt\n",
    "from altair_saver import save #ademas instalar en terminal: brew cask install chromedriver\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, HBox, VBox, Layout\n",
    "from graphviz import Source, Digraph\n",
    "import dot2tex as d2t\n",
    "from latex import build_pdf\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import arviz as az\n",
    "\n",
    "#Funciones propias (tienen que estar en el mismo directorio)\n",
    "import my_fun as mf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Adaptive Toolbox\n",
    "\n",
    "Santiago Alonso-Díaz, PhD \\\n",
    "Universidad Javeriana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"img/7_CB/Gigerenzer1.png\" width = \"501\" height = '500'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Los humanos y animales siempre han dependido de heurísticas para solucionar problemas. \n",
    "\n",
    "Ejemplo 1: medir áreas de hendiduras en el piso. Dar una vuelta irregular a la hendidura y dejar feromonas. Dar otra vuelta irregular y estimar el área por la frecuencia que se cruzan los dos caminos.  \n",
    "\n",
    "<center><img src=\"img/7_CB/faris-mohammed-unsplash.jpg\" width = \"301\" height = '300'></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Ejemplo 2: agarrar objetos a alta velocidad. Mantener un ángulo óptico constante entre uno mismo y el objeto.  \n",
    "\n",
    "<center><img src=\"img/7_CB/c-perret-unsplash.jpg\" width = \"301\" height = '300'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Por qué aparecen las heurísticas? \n",
    "* No omnisciencia (saber todas las variables)\n",
    "* No omnipotencia (poder computacional infinito)\n",
    "* Intractabilidad (no hay solución analítica o computable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Las heurísticas usualmente se relacionan (erroneamente) con el accuracy-effort tradeoff:\n",
    "* La segunda (o 3era, 4ta, ...) mejor alternativa \n",
    "    * no del todo, pueden ser de hecho la mejor. \n",
    "* Las usamos por nuestros limítes computacionales \n",
    "    * no siempre, incluso en problemas fáciles pueden ser útiles\n",
    "* Son una alternativa menor por que no usan toda la información, tiempo, y computos \n",
    "    * no siempre, e.g. bias-variance trade-off\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Las heurísticas pueden ser eficientes\n",
    "\n",
    "Less-is-more effect:\n",
    "> \"More information or computation can decrease accuracy; therefore, minds rely on simple heuristics in order to be more accurate than strategies that use more information and time.\" Gigerenzer & Brighton, 2009, pp 110\n",
    "\n",
    "Veamos algunos casos de less-is-more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><div> Tallying</div></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Problema: \n",
    "\n",
    "Predecir outcome Y (e.g. oprimir o no oprimir) con atributos X ($X_1, X_2, ... , X_n$)\n",
    "\n",
    "### Solución: \n",
    "\n",
    "Regresión logística (p>0.5 categoria 1, p<0.5 categoria 1):\n",
    "\n",
    "$$ Y = \\beta_0 + \\beta X $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$ Y = \\beta_0 + \\beta X $$\n",
    "\n",
    "¿Cómo obtener $\\beta_0$ y $\\beta$?\n",
    "\n",
    "* Maximum likelihood (MLE)\n",
    "* Aleatorios\n",
    "* Rankeados por validez (conocimiento del área)\n",
    "* Todos igual (tallying)\n",
    "\n",
    "¿Cuál es una solución heurística? Todas menos MLE.\n",
    "\n",
    "Centremonos en tallying. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Por qué se llama tallying/conteo? Un ejemplo: Y es comprar o no comprar torta de chocolate\n",
    "\n",
    "$Y_i = 1 + 1*animo + 1*precio_{torta} + 1*colesterol_{torta} + 1*azucar_{torta} + 1*hora_{día}$\n",
    "\n",
    "Cuando todo tiene el mismo peso (1), la probabilidad crece a medida que aumenta la suma. Es decir, es como contar (tally) las características disponibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hay una versión simple de tallying: take-the-best de diferentes atributos (cues)\n",
    "\n",
    "A la pregunta cuál tiene más colesterol la persona responde con el mejor cue, es decir, el que considera más relevante y conoce (e.g. calorias en la información nutricional) <br><br>\n",
    "\n",
    "\n",
    "<center><img src=\"img/7_CB/Gigerenzer2.svg\" width = \"501\" height = '500'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Czerlinski, Gigerenzer, & Goldstein (1999) pusieron a prueba tallying, take-the-best, y regresión múltiple en veinte dominios. Trataron de predecir los siguientes outcomes:\n",
    "\n",
    "* Deserción escolar\n",
    "* Habitante de la calle\n",
    "* Mortalidad\n",
    "* Tamaño de ciudad\n",
    "* Atractivo (hombres)\n",
    "* Atractivo (mujeres)\n",
    "* Precio de vivienda\n",
    "* Renta de la tierra\n",
    "* Salarios de profesores\n",
    "* Accidentes de carros\n",
    "* Consumo de gasolina\n",
    "* Obesidad a los 18\n",
    "* Grasa corporal\n",
    "* Fertilidad de peces\n",
    "* Tiempo de sueño mamifero\n",
    "* Calidad de abono de vaca\n",
    "* Biodiversidad\n",
    "* Cantidad de lluvia\n",
    "* Cantidad de oxidantes en Los Angeles\n",
    "* Cantidad de ozono en San Francisco\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Regresión ajusta mejor los datos. Tallying y take-the-best son mejores prediciendo. <br><br><br>\n",
    "\n",
    "<center><img src=\"img/7_CB/Gigerenzer3.svg\" width = \"501\" height = '500'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En otro estudio se probaron algunos algoritmos de machine learning en una tarea de adivinar la población de una ciudad. De nuevo, take-the-best le iba bien con muestras pequeñas. <br><br> <br>\n",
    "\n",
    "<center><img src=\"img/7_CB/Gigerenzer4.svg\" width = \"700\" height = '700'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Tallying o take-the-best no optimizan nada, son heurísticas. Take-the-best forma parte de un grupo de heurísticas donde less-is-more (less information, more accuracy). Cuestionan la noción que siempre hay un accuracy-effort tradeoff (effort = usar toda la info).\n",
    "\n",
    "> \"Why should a mind waste time and effort in estimating the optimal weights of cues if they do not matter or\n",
    "even detract from performance?\" Gigerenzer & Brighton, 2009, pp. 112"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Tres elementos de take-the-best\n",
    "1. Regla de busqueda: Buscar cues (e.g. grasas saturadas, calorias, proteina, sodio) en orden de validez relativo al outcome (e.g. colesterol).\n",
    "2. Regla para parar: Cuando se encuentre el primer cue que discrimine entre objetos (e.g. calorias cake vs calorias pie)\n",
    "3. Regla para decidir: El que tenga el mejor valor en el cue donde se paró."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>Ecological rationality</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>¿Cuando sirven las heurísticas? Para adaptarse. Para enfrentarse al tradeoff entre sesgo-varianza </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Qué es el tradeoff sesgo-varianza? Es un resultado de inferencia estadística <br> <br>\n",
    "\n",
    "<center><img src=\"img/7_CB/bias-and-variance2.jpg\" width = \"400\" height = '400'></center>\n",
    "\n",
    "$$ Prediction \\ error = bias^2 + variance + noise$$\n",
    "\n",
    "En esta formula (y gráfica), a un nivel de error de predicción deseado, si bajo el sesgo (distancia del modelo a datos recolectados) necesito subir la varianza (distancia del modelo a un nuevo dato) para mantener el error de predicción deseado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ejemplo: predecir el clima por el día del año. Un modelo complejo (grado 12) ajusta más puntos (bajo sesgo; gráfica izq.). Sin embargo, unir puntos de un año no asegura que se pueda predecir que pasa un día de otro año (alta varianza; gráfica der.) <br><br>\n",
    "\n",
    "<center><img src=\"img/7_CB/Gigerenzer5.svg\" width = \"900\" height = '900'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Y qué tiene que ver eso con heurísticas?\n",
    "\n",
    "En general, tener alta varianza y bajo sesgo (overfit) puede ser poco adaptativo. Para Gigerenzer & Brighton, un organismo debería apuntar a bajar varianza i.e. mejorar predicción con nuevos datos. Las heurísticas parecen ser buenas para eso. <br> <br>\n",
    "\n",
    "<center><img src=\"img/7_CB/Gigerenzer3.svg\" width = \"401\" height = '400'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La presencia de heurísticas va a depender de la estructura del ambiente\n",
    "\n",
    "* No compensatorio (e.g. los cues no se correlacionan y su relación con el outcome difiere)\n",
    "* Compensatorio (e.g. los cues están muy correlacionados con el outcome pera hay algunos mejores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> \"Why should experts and laypeople rely on heuristics? To summarize, the answer is not simply in the accuracy effort dilemma but in the bias–variance dilemma, as higher accuracy can be achieved by more or less effort.\" Gigerenzer & Brighton, 2009, pp 125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Algunas heurísticas\n",
    "| Heuristic             |                                                                     Description                                                                     |\n",
    "|-----------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------:|\n",
    "| Recognition           |                          If one of two alternatives is recognized, infer that it has the higher <br>value on the criterion.                         |\n",
    "| Fluency               |              If both alternatives are recognized but one is recognized faster, <br>infer that it has the higher value on the criterion.             |\n",
    "| Take-the-best         | (a) search through cues in order of validity, (b) stop search<br>as soon as a cue discriminates, and (c) choose the alternative<br>this cue favors. |\n",
    "| Tallying              |                                        Do not estimate weights but simply count the number of positive cues.                                        |\n",
    "| Satisficing           |                             Search through alternatives and choose the first one that exceeds your <br>aspiration level.                            |\n",
    "| 1/N                   |                                                Allocate resources equally to each of N alternatives.                                                |\n",
    "| Default               |                                                          If there is a default, do nothing.                                                         |\n",
    "| Tit-for-tat           |                                            Cooperate first and then imitate your partner’s last behavior.                                           |\n",
    "| Imitate the majority  |                                                                Imitate the majority.                                                                |\n",
    "| Imitate the succesful |                                                        Imitate the most succesful individual.                                                       |\n",
    "\n",
    "\n",
    "Gigerenzer & Brighton, 2009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modelos bayesianos para heurísticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Take-the-best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Cuál ciudad tiene más habitantes? \n",
    "\n",
    "<center> Lima vs Buenos Aires </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Proceso take-the-best:\n",
    "1) Ordenar los cues por importancia \\\n",
    "2) Parar en el cue que discrimine \\\n",
    "3) Escoger el objeto con el mejor valor en el cue de 2 \n",
    "\n",
    "\n",
    "|   Memory of  | Country Population | Capital |     GDP    | World cup wins |\n",
    "|:------------:|:------------------:|:-------:|:----------:|:--------------:|\n",
    "|     Lima     |     ¯\\\\\\_(ツ)_/¯     |   Yes   | ¯\\\\\\_(ツ)_/¯ |       No       |\n",
    "| Buenos Aires |     ¯\\\\\\_(ツ)_/¯     |   Yes   | ¯\\\\\\_(ツ)_/¯ |       Yes      |\n",
    "\n",
    "Take-the-best: Buenos Aires por World cup wins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$ t_q = \\text{TTB}_{s}(\\mathbf a_q,\\mathbf b_q)$$\n",
    "$$ \\gamma \\sim \\text{Uniform}(0.5,1)$$  \n",
    "$$ y_{iq} \\sim\n",
    "\\begin{cases}\n",
    "\\text{Bernoulli}(\\gamma) & \\text{if $t_q = a$} \\\\\n",
    "\\text{Bernoulli}(1- \\gamma) & \\text{if $t_q = b$} \\\\\n",
    "\\text{Bernoulli}(0.5) & \\text{otherwise}\n",
    "\\end{cases}  $$\n",
    "\n",
    "$t_q$: decisión take-the-best (a o b) (deterministica) \\\n",
    "$a_q, \\ b_q$: vector de cues para opción a y b (observable) \\\n",
    "$s$: orden de los cues por validez (observable) \\\n",
    "$\\gamma$: probabilidad de tomar la decisión take-the-best (i.e. $a_q$) (inferido, $\\ge$ 0.5 pues no se decide lo peor)\\\n",
    "$y_{iq}$: decisión del individuo i a la pregunta q (observable) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejercicio en clase\n",
    "\n",
    "Haga el diagrama que represente las formulas. Está abajo, no lo vea. Luego de hacer el suyo comparelos. ¿Son iguales o qué falto/mejoró?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/7_CB/model_TTB.svg\" width = \"51\" height = '50'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El experimento y data a modelar es de Ben Newell (la versión de Wagenmakers):\n",
    "* 20 sujetos\n",
    "* Cada sujeto contesto las mismas 30 preguntas\n",
    "* Las preguntas eran de varios temas y se escogía entre dos opciones.\n",
    "* Por ejemplo ¿qué ciudad de estas dos tiene más habitantes?\n",
    "* En cada pregunta, el sujeto podía buscar entre 9 cues por orden de validez para ver si las opciones los tenían.\n",
    "* Algo así:\n",
    "\n",
    "<center><img src=\"img/7_CB/TTB_experiment.svg\" width = \"550\" height = '550'></center>\n",
    "\n",
    "* Otro ejemplo:\n",
    "\n",
    "<center><img src=\"img/7_CB/TTB_experiment2.svg\" width = \"551\" height = '550'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Load data\n",
    "m = np.array(pd.read_csv('data/7_CB/m.csv').iloc[:,1:]) #design matrix; rows: stimuli (e.g. Berlin); columns: cues (1 in the stimuli, 0 not)\n",
    "p = np.array(pd.read_csv('data/7_CB/p.csv').iloc[:,1:]) #pairs of stimulus index; rows: questions. columns: stimuli index as numbered in the rows of matrix m \n",
    "y = np.array(pd.read_csv('data/7_CB/y.csv').iloc[:,1:]) #choices: rows subjects, columns questions; 0, b_q is chosen, 1 a_q is chosen\n",
    "n = np.shape(m)[0]  # number of stimuli\n",
    "nc = pd.read_csv('data/7_CB/nc.csv')['nc'][0] #number of cues\n",
    "nq = pd.read_csv('data/7_CB/nq.csv')['nq'][0] #number of questions\n",
    "ns = pd.read_csv('data/7_CB/ns.csv')['ns'][0] #number of subjects\n",
    "v = np.array( pd.read_csv('data/7_CB/v.csv')['v']) #cue validity\n",
    "x = np.array(pd.read_csv('data/7_CB/x.csv')['x']) #for WADD evidence provided by each cue, defined as the log-odds of their validity i.e x = log (v/(1-v)). The first one is 100 to avoid Inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#More data/observables\n",
    "s = np.argsort(v)  # cue validity (rank-order; high rank high validity)\n",
    "t = [] #take the best decision\n",
    "# TTB Model For Each Question\n",
    "for q in range(nq): #questions\n",
    "    # Add Cue Contributions To Mimic TTB Decision\n",
    "    tmp1 = np.zeros(nc)\n",
    "    for j in range(nc): #cues\n",
    "        # the -1 because it was data from r (i.e. index in python start at 0)\n",
    "        # 2 to the validity to enhance cue differences. In words of Lee & Wagenmakers:\n",
    "        # \"applying non- compensatory weights (the pow(2,s[j])) so that \n",
    "        # the first discriminating cue determines the decision\". \n",
    "        # The power 2 is the math trick that implements take the best (it could be other base > 2).\n",
    "        tmp1[j] = (m[p[q, 0] - 1, j] - m[p[q, 1] - 1, j]) * np.power(2, s[j]) \n",
    "        \n",
    "    # Find if Cues Favor First (sum>0), Second (sum<0), or Neither Stimulus (sum = 0)\n",
    "    tmp2 = np.sum(tmp1)\n",
    "    tmp3 = -1 * np.float32(-tmp2 > 0) + np.float32(tmp2 > 0) #-1 second stimulus (b_q) is better, 0 neither, 1 first stimulus (a_q) is better \n",
    "    t.append(tmp3 + 1) #it can be 0 (b_q), 1 (neither), or 2 (a_q) ... index for the yiq distribution (see pymc model)\n",
    "\n",
    "t = np.asarray(t, dtype=int) \n",
    "#all are 2 i.e. first stimulus (a_q) is always take-the-best \n",
    "#i.e. gamma is the prob. of take the best (see gammat in pymc model1)\n",
    "#in Lee & Wagenmakers words: \"In the presentation of results, but not the presentation to \n",
    "#subjects in the experiment, the stimulus pairs are coded so that choice “a” always \n",
    "#corresponds to the TTB choice\"\n",
    "tmat = np.tile(t[np.newaxis, :], (ns, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejercicio en clase\n",
    "\n",
    "Traduzca el modelo gráfico a PyMC3. Está abajo, no lo vea. Luego de hacer el suyo comparelos. ¿Son iguales o qué falto/mejoró?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/7_CB/model_TTB.svg\" width = \"55\" height = '55'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Modelos basados en https://github.com/junpenglao/Bayesian-Cognitive-Modeling-in-Pymc3\n",
    "with pm.Model() as model1:\n",
    "    gamma = pm.Uniform(\"gamma\", lower=0.5, upper=1)\n",
    "    gammat = tt.stack([1 - gamma, 0.5, gamma])\n",
    "\n",
    "    yiq = pm.Bernoulli(\"yiq\", p=gammat[tmat], observed=y)\n",
    "    trace1 = pm.sample()\n",
    "    ppc = pm.sample_posterior_predictive(trace1, samples=5000)\n",
    "    data = az.from_pymc3(trace=trace1)\n",
    "\n",
    "az.plot_trace(data, var_names=[\"gamma\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "gamma = trace1[\"gamma\"]\n",
    "fig, axes = plt.subplots(1, 1, figsize=(15, 4))\n",
    "my_pdf1 = st.kde.gaussian_kde(gamma)\n",
    "x1 = np.linspace(0.68, 0.82, 200)\n",
    "axes.plot(x1, my_pdf1(x1), \"k\", lw=2.5, alpha=0.6)  # distribution function\n",
    "axes.set_xlim((0.66, 0.84))\n",
    "axes.set_xlabel(r\"$\\gamma$ (probability of choosing TTB)\", fontsize=15)\n",
    "axes.set_ylabel(\"Posterior Density\", fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\gamma$ no es 1. Es decir, las personas no siempre escogen la take-the-best pero si con alta probabilidad (el MAP es alrededor de 0.75)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "az.style.use(\"arviz-darkgrid\")\n",
    "yiqpred = np.asarray(ppc[\"yiq\"])\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "x1 = np.repeat(np.arange(ns) + 1, nq).reshape(ns, -1).flatten()\n",
    "y1 = np.repeat(np.arange(nq) + 1, ns).reshape(nq, -1).T.flatten()\n",
    "\n",
    "plt.scatter(y1, x1, s=np.mean(yiqpred, axis=0) * 200, c=\"w\") #size s of the dot is the probability of picking the best\n",
    "plt.scatter(y1[y.flatten() == 1], x1[y.flatten() == 1], marker=\"x\", c=\"r\") #x where subjs picked the best\n",
    "plt.plot(np.ones(100) * 24.5, np.linspace(0, 21, 100), \"--\", lw=1.5, c=\"k\")\n",
    "plt.axis([0, 31, 0, 21]);\n",
    "plt.ylabel('Subjects')\n",
    "plt.xlabel('Question');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "La grilla esta completa con x rojas en un 75% aproximadamente (el sujeto escogió la take-the-best). El modelo predice en cada pregunta esa probabilidad: el tamaño de los circulos es el promedio del posterior predictive check y son casi constantes (también ver la posterior gamma).\n",
    "\n",
    "La línea punteada es para resaltar que algunos sujetos (x rojas) no se fueron por TTB en la preguntas finales.\n",
    "\n",
    "Depronto usaron TTB y otra estrategia como sumar la evidencia de todos los cues, no solo parar en el mejor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Latent-mixture model of take-the-best and WADD\n",
    "\n",
    "Modelo aditivo ponderado (Weighted ADDitive (WADD) model). El WADD, suma la evidencia en todos los cues para ambas alternativas, y escoge la de mayor evidencia.\n",
    "\n",
    "$$ \\phi \\sim \\text{Uniform}(0,1)$$\n",
    "$$ z_i \\sim \\text{Bernoulli}(\\phi)$$\n",
    "$$ \\gamma \\sim \\text{Uniform}(0.5,1)$$  \n",
    "$$ t_{iq} = \n",
    "\\begin{cases}\n",
    "\\text{TTB}_s\\,(\\mathbf a_q,\\mathbf b_q) & \\text{if $z_i = 1$} \\\\\n",
    "\\text{WADD}\\,(\\mathbf a_q,\\mathbf b_q) & \\text{if $z_i = 0$} \\\\\n",
    "\\end{cases}  $$  \n",
    "\n",
    "$$ y_{iq} \\sim\n",
    "\\begin{cases}\n",
    "\\text{Bernoulli}(\\gamma) & \\text{if $t_{iq} = a$} \\\\\n",
    "\\text{Bernoulli}(1- \\gamma) & \\text{if $t_{iq} = b$} \\\\\n",
    "\\text{Bernoulli}(0.5) & \\text{otherwise}\n",
    "\\end{cases}  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejercicio en clase\n",
    "\n",
    "Haga el diagrama que represente las formulas. Está abajo, no lo vea. Luego de hacer el suyo comparelos. ¿Son iguales o qué falto/mejoró?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/7_CB/model_TTB_WADD.svg\" width = \"60\" height = '60'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Question cue contributions template\n",
    "qcc = np.zeros((nq, nc))\n",
    "for q in range(nq):\n",
    "    # Add Cue Contributions To Mimic TTB Decision\n",
    "    for j in range(nc):\n",
    "        qcc[q, j] = m[p[q, 0] - 1, j] - m[p[q, 1] - 1, j]\n",
    "\n",
    "qccmat = np.tile(qcc[np.newaxis, :, :], (ns, 1, 1))\n",
    "# TTB Model For Each Question\n",
    "s = np.argsort(v)  # s[1:nc] <- rank(v[1:nc])\n",
    "smat = np.tile(s[np.newaxis, :], (ns, nq, 1)) #mat is for matrix\n",
    "ttmp = np.sum(qccmat * np.power(2, smat), axis=2) \n",
    "tmat = -1 * (-ttmp > 0) + (ttmp > 0) + 1 #0 (b_q), 1 (neither), or 2 (a_q) ... index for the yiq distribution (see pymc model)\n",
    "t = tmat[0]\n",
    "\n",
    "# WADD Model For Each Question \n",
    "# We no longer use non-compensatory weights (np.power(2, smat))\n",
    "xmat = np.tile(x[np.newaxis, :], (ns, nq, 1)) #mat is for matrix\n",
    "wtmp = np.sum(qccmat * xmat, axis=2)\n",
    "wmat = -1 * (-wtmp > 0) + (wtmp > 0) + 1 #0 (b_q), 1 (neither), or 2 (a_q) ... index for the yiq distribution (see pymc model)\n",
    "w = wmat[0]\n",
    "\n",
    "#\"In the presentation of results, but not the presentation to \n",
    "#subjects in the experiment, the stimulus pairs are coded so that choice “a” always \n",
    "#corresponds to the TTB choice\"\n",
    "t, w #Note that WADD manages to pick in the last six questions the worst option (b_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejercicio en clase\n",
    "\n",
    "Traduzca el modelo gráfico a PyMC3. Está abajo, no lo vea. Luego de hacer el suyo comparelos. ¿Son iguales o qué falto/mejoró?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model2:\n",
    "    phi = pm.Beta(\"phi\", alpha=1, beta=1, testval=0.01)\n",
    "\n",
    "    zi = pm.Bernoulli(\n",
    "        \"zi\",\n",
    "        p=phi,\n",
    "        shape=ns,\n",
    "        testval=np.asarray(\n",
    "            [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        ),\n",
    "    )\n",
    "    zi_ = tt.reshape(tt.repeat(zi, nq), (ns, nq))\n",
    "\n",
    "    gamma = pm.Uniform(\"gamma\", lower=0.5, upper=1)\n",
    "    gammat = tt.stack([1 - gamma, 0.5, gamma])\n",
    "\n",
    "    t2 = tt.switch(tt.eq(zi_, 1), tmat, wmat)\n",
    "    yiq = pm.Bernoulli(\"yiq\", p=gammat[t2], observed=y)\n",
    "\n",
    "    trace2 = pm.sample()\n",
    "    ppc2 = pm.sample_posterior_predictive(trace2, samples=5000)\n",
    "    data2 = az.from_pymc3(trace=trace2)\n",
    "\n",
    "az.plot_trace(data2, compact=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 4))\n",
    "zitrc = trace2[\"zi\"]\n",
    "plt.bar(np.arange(ns) + 1, 1 - np.mean(zitrc, axis=0))\n",
    "plt.yticks([0, 1], (\"TTB\", \"WADD\"))\n",
    "plt.xlabel(\"Subject\")\n",
    "plt.ylabel(\"Group\")\n",
    "plt.axis([0, 21, 0, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.style.use(\"arviz-darkgrid\")\n",
    "yiqpred = np.asarray(ppc2[\"yiq\"])\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "x1 = np.repeat(np.arange(ns) + 1, nq).reshape(ns, -1).flatten()\n",
    "y1 = np.repeat(np.arange(nq) + 1, ns).reshape(nq, -1).T.flatten()\n",
    "\n",
    "plt.scatter(y1, x1, s=np.mean(yiqpred, axis=0) * 200, c=\"w\") #size s of the dot is the probability of picking the best\n",
    "plt.scatter(y1[y.flatten() == 1], x1[y.flatten() == 1], marker=\"x\", c=\"r\") #x where subjs picked the best\n",
    "plt.plot(np.ones(100) * 24.5, np.linspace(0, 21, 100), \"--\", lw=1.5, c=\"k\")\n",
    "plt.axis([0, 31, 0, 21]);\n",
    "plt.ylabel('Subjects')\n",
    "plt.xlabel('Question');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejercicio\n",
    "\n",
    "1) Comparé la grilla TTB con la que acabamos de hacer TTB + WADD. Comente cuál es la principal diferencia para las seis preguntas finales. Tip: salve ambas figuras y pongalas juntas.\n",
    "\n",
    "2) Comparé el modelo que acabamos de correr con uno donde $\\phi = 0$. Es decir, un modelo nulo con otro donde incluimos $\\phi$ ¿Cuál es mejor? Use criterios de información o factores de Bayes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Ordenes de cues diferentes\n",
    "\n",
    "¿Es valido el supuesto que todos usan el mismo orden de los 9 cues? ¿O diferentes individuos usan diferentes ordenes? Pongamos un prior a validez de los cues por individuo $v_{ic}$\n",
    "\n",
    "$$ v_{ic} \\sim Normal(0,0.001)$$\n",
    "$$ s_i = rank \\ order(v_{iq})$$\n",
    "$$ t_{iq} = \\text{TTB}_{si}(\\mathbf a_q,\\mathbf b_q)$$\n",
    "$$ \\gamma \\sim \\text{Uniform}(0.5,1)$$  \n",
    "$$ y_{iq} \\sim\n",
    "\\begin{cases}\n",
    "\\text{Bernoulli}(\\gamma) & \\text{if $t_{iq} = a$} \\\\\n",
    "\\text{Bernoulli}(1- \\gamma) & \\text{if $t_{iq} = b$} \\\\\n",
    "\\text{Bernoulli}(0.5) & \\text{otherwise}\n",
    "\\end{cases}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/7_CB/model_TTB_order.svg\" width = \"600\" height = '600'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "nq = int(nq)\n",
    "with pm.Model() as model3:\n",
    "    gamma = pm.Uniform(\"gamma\", lower=0.5, upper=1)\n",
    "    gammat = tt.stack([1 - gamma, 0.5, gamma])\n",
    "\n",
    "    v1 = pm.HalfNormal(\"vic\", sd=1, shape=ns * nc)\n",
    "    s1 = pm.Deterministic(\"si\", tt.argsort(v1.reshape((ns, 1, nc)), axis=2))\n",
    "    smat2 = tt.tile(s1, (1, nq, 1))  # s[1:nc] <- rank(v[1:nc])\n",
    "\n",
    "    # TTB Model For Each Question\n",
    "    ttmp = tt.sum(qccmat * tt.power(2, smat2), axis=2)\n",
    "    tmat = -1 * (-ttmp > 0) + (ttmp > 0) + 1\n",
    "\n",
    "    yiq = pm.Bernoulli(\"yiq\", p=gammat[tmat], observed=y)\n",
    "    \n",
    "    #Junpeng Lao uses metropolis because the model geometry seems to be unsmooth ()\n",
    "    trace3 = pm.sample(100000, step=pm.Metropolis(), compute_convergence_checks=False)\n",
    "    ppc3 = pm.sample_posterior_predictive(trace3, samples=5000)\n",
    "    data3 = az.from_pymc3(trace=trace3)\n",
    "\n",
    "az.plot_trace(data3, var_names=[\"gamma\", \"vic\"], compact=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "burnin = 50_000\n",
    "# v1trace = np.squeeze(trace3['v1'][burnin:])\n",
    "# s1trace = np.argsort(v1trace, axis=2)\n",
    "s1trace = np.squeeze(trace3[burnin:][\"si\"]) #sampled orders\n",
    "\n",
    "for subj_id in [12, 13]:\n",
    "    subj_s = np.squeeze(s1trace[:, subj_id - 1, :]) #sampled orders for subject\n",
    "    unique_ord = np.vstack(list({tuple(row) for row in subj_s})) #detects unique orders (a set does not repeat values)\n",
    "    num_display = 10\n",
    "    print(\"Subject %s\" % (subj_id))\n",
    "    print(\n",
    "        \"There are %s search orders sampled in the posterior.\" % (unique_ord.shape[0])\n",
    "    )\n",
    "\n",
    "    mass_ = []\n",
    "    for s_ in unique_ord:\n",
    "        mass_.append(np.mean(np.sum(subj_s == s_, axis=1) == len(s_)))\n",
    "    mass_ = np.asarray(mass_)\n",
    "    sortmass = np.argsort(mass_)[::-1]\n",
    "\n",
    "    for i in sortmass[:num_display]:\n",
    "        s_ = unique_ord[i]\n",
    "        print(\"Order=(\" + str(s_ + 1) + \"), Estimated Mass=\" + str(mass_[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[trace3[burnin:][\"si\"].shape, s1trace.shape]\n",
    "subj_id = 12\n",
    "[s1trace[:, subj_id - 1, :].shape, np.squeeze(s1trace[:, subj_id - 1, :]).shape]\n",
    "unique_ord.shape\n",
    "\n",
    "\n",
    "\n",
    "a = [[1,2], [1,3], [1,2]]\n",
    "d = list({tuple(row) for row in a})\n",
    "d = {tuple(row) for row in a}\n",
    "np.vstack(d)\n",
    "#np.array(d)\n",
    "len(s_)\n",
    "s_ = unique_ord[0]\n",
    "print(s_.shape)\n",
    "np.sum(subj_s == s_, axis=1).shape\n",
    "subj_s.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The return order is not at all similar to the result in JAGS (as shown in the book on p.233)\n",
    "\n",
    "In general, order seems to be hard to obtain. There may be an specification problem (e.g. different orders work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejercicio en clase\n",
    "\n",
    "* Haga el diagrama de un modelo que combine TTB + WADD y diferentes ordenes. Esta en el libro de Lee & Wagenmakers luego de hacer el suyo comparelos. ¿Son iguales o qué falto/mejoró? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Is less information truly better? Not so fast: a bayesian argument\n",
    "<center><img src=\"img/7_CB/Parpart1.png\" width = \"700\" height = '700'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Regression: uses all cues, including covariance structure of cues (e.g. corr. between goals and position) \\\n",
    "TTB: searches cues based on their criterion validity (obtained by experience); disregards covariance \n",
    "\n",
    "<center><img src=\"img/7_CB/Parpart2.png\" width = \"700\" height = '700'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary Parpart:\n",
    "* For example, Gigerenzer and Todd (1999) write, “There is a point where too much information and too much information processing can hurt” (p. 21) (pp 128)\n",
    "* every model, including heuristics, has an inductive bias ... pp (129) MY NOTE (source: https://arxiv.org/pdf/1806.01261.pdf): inductive bias is to prioritize one solution over another ... they are assumption about the data-generating process or space of solutions\n",
    "* At one end of the continuum (infinitely diffuse prior), the Bayesian model is equivalent to a variant of linear regression, and at the other end (infinitely strong prior) it is equivalent to a heuristic...we find that best performance comes from intermediate models on the continuum, which do not entirely ignore cue weights or cue covariance but that nonetheless down-weight this information via the influence of their priors pp(128)\n",
    "* The Bayesian framework offers a different perspective on the bias-variance dilemma. Provided a Bayesian model is correctly specified, it always integrates new data optimally, striking the perfect balance between prior and data. Thus using more information can only improve performance. From the Bayesian standpoint, a less-is-more effect can arise only if a model uses the data incorrectly, for example by weighting it too heavily relative to prior knowledge pp(129).\n",
    "* Ridge regression stuff on page 130\n",
    "* intermediate half-ridge models outperform tallying in all 20 datasets, suggests that ignoring information is never the best solution. The best-performing model uses all the information in the training data, combining it with the appropriate prior. pp (131)\n",
    "* we provide a formal understanding of why heuristics can outperform full-information models by placing all models in a common probabilistic inference framework, where heuristics correspond to extreme priors that will usually be outperformed by intermediate models that use all available information.(pg 134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Data setup\n",
    "ALL_DATA = [\"house.world\",\"mortality\",\"cit.world\",\"prf.world\",\"bodyfat.world\", \"car.world\",\"cloud\",\n",
    "            \"dropout\",\"fat.world\", \"fuel.world\", \"glps\",\n",
    "            \"homeless.world\", \"landrent.world\", \"mammal.world\", \"oxidants\",\n",
    "            \"attractiveness.men\", \"attractiveness.women\", \"fish.fertility\",\"oxygen\", \"ozone\"]\n",
    "eta_penalty = np.array([1000000, 100000, 1000, 700.00000000, 330.07641174, \n",
    "                        156.81303712, 74.49889703,  35.39301171, 16.81454797, \n",
    "                        7.98827254, 3.79507663, 1.80296886, 0.85655628, 0.40693363,\n",
    "                        0.19332644, 0.09184572, 0.03, 0.01, 0.001, 0.0001, 0.00001])\n",
    "y_pos = 2 #column position of dependent variable (criterion)\n",
    "#TODO: with ALL_DATA i.e. so far I did it only with one of the datasets\n",
    "e = 0\n",
    "#in dataset, 0 below median, 1 above median. Gigerenzer et al, converted continuous variables to median splits \n",
    "dataset = pd.read_table(\"data/7_CB/Parpart 2018/Data/\" + ALL_DATA[e] + \".txt\")\n",
    "idx = list(range(4,dataset.shape[1]))\n",
    "col_cues = np.array(idx) #idx of columns with cues\n",
    "labels_cues = dataset.columns[idx]\n",
    "Predictors = len(labels_cues) #number of cues\n",
    "N = dataset.shape[0] # number of objects e.g cities\n",
    "k = 100 #number of partitions for cross validations\n",
    "\n",
    "#Create Paired Data (ALL binary comparisons of objects e.g. cities)\n",
    "comb = np.array(list(combinations(list(range(N)), 2)))\n",
    "idx = np.stack([np.random.choice([0,1], 2, replace = False) for rep in range(comb.shape[0])]) #[0,1] shuffled many times\n",
    "comb = np.transpose(np.stack([comb[i,ele] for i,ele in enumerate(idx)])) #columns shuffled\n",
    "y = np.repeat(np.nan, comb.shape[1]) # correct classification; A(+1) or B(-1)  \n",
    "difference = np.repeat(np.nan, comb.shape[1]) \n",
    "bdata_diff = pd.DataFrame(np.nan, index=np.arange(comb.shape[1]), columns=labels_cues)\n",
    "for i in range(comb.shape[1]):\n",
    "    # takes out only the 2 rows from dataset that are compared at step i \n",
    "    binary = dataset.loc[comb[:,i],:].reset_index(drop=True) #2 random rows\n",
    "    if i == 0:\n",
    "        comparisons = binary\n",
    "    else:\n",
    "        comparisons = pd.concat([comparisons, binary])\n",
    "    \n",
    "    ## always compare row 1 with row 2 (no matter which ones has the higher criterion value) upper row - lower row\n",
    "    if binary.iloc[0,y_pos] > binary.iloc[1,y_pos]:\n",
    "        y[i] = 1 #(A)\n",
    "    else:\n",
    "        y[i] = - 1 #(B)\n",
    "    \n",
    "    ## cue values (row 1) - cue values (row 2) \n",
    "    bdata_diff.loc[i,:] = binary.loc[0,labels_cues] - binary.loc[1,labels_cues] # \n",
    "\n",
    "bdata_diff['dependent'] = y\n",
    "paired_data = copy.deepcopy(bdata_diff)\n",
    "dataset = copy.deepcopy(paired_data)\n",
    "\n",
    "# Assess paired_data cue validities and order as v= R/R+W  ------R:right, W:wrong\n",
    "cue_validities_raw = np.repeat(np.nan, Predictors)\n",
    "cue_validities = np.repeat(np.nan, Predictors) #between 0 (does not predict which is better) and 1 (always predicts which is better)\n",
    "for c in range(Predictors):\n",
    "    condition = (paired_data.iloc[:,c]==paired_data.loc[:,'dependent']).sum() == 0\n",
    "    if condition: # stays 0 now if it was 0 \n",
    "        cue_validities[c] = 0\n",
    "    else:\n",
    "        cue_validities_raw[c] = (paired_data.iloc[:,c]==paired_data.loc[:, 'dependent']).sum()/((paired_data.iloc[:,c]==1).sum()+(paired_data.iloc[:,c]==-1).sum()) \n",
    "        cue_validities[c] = cue_validities_raw[c] - 0.5 # back to same scale as regression weights as otherwise order can be different!\n",
    "cue_order = np.argsort(-abs(cue_validities)) \n",
    "\n",
    "# number of objects (e.g. paired cities comparisons) after evening out\n",
    "N = dataset.shape[0]\n",
    "\n",
    "#Partitions for cross-validation\n",
    "training_size = np.array([10, 20, round(0.9*N)])/N #in percent of dataset rows \n",
    "#TODO: loop for each training size and partition; currently I did it only for one training size v and one partition i\n",
    "v = 0 #training size (looping var)\n",
    "percent_training  =  training_size[v]\n",
    "# Generate the cross-validation partitions: \n",
    "percent =(1 - percent_training)  #### Hold the testset (distinct from random training set) \n",
    "training_sample_size = percent_training*N\n",
    "re = np.repeat(np.nan, k) # resampling\n",
    "i = 0 #partition number (see k above) (looping var)\n",
    "trainset, testset = train_test_split(dataset, test_size=percent)\n",
    "trainset = trainset.reset_index(drop=True)\n",
    "testset = testset.reset_index(drop=True)\n",
    "#print([trainset.shape, testset.shape, dataset.shape, training_sample_size])\n",
    "Predictors = trainset.shape[1]-1\n",
    "#Re-shuffling zero variance cases (incompatible with COR model) \n",
    "re[i] = 0 #I think this is useless (brought it from Papart R code and kept it just in case)\n",
    "cov_mat = trainset[labels_cues].corr()\n",
    "# NA cases = zero variance cases, get resampled now until one is found without any zero variance cases\n",
    "while cov_mat.isna().any(axis = None):\n",
    "    trainset, testset = train_test_split(dataset, test_size=percent)\n",
    "    trainset = trainset.reset_index(drop=True) # re[i] = 0 at i=1\n",
    "    testset = testset.reset_index(drop=True)\n",
    "    re[i] = re[i] + 1 #I think this is useless (brought it from Papart R code and kept it just in case)\n",
    "    cov_mat = trainset[labels_cues].corr()\n",
    "    \n",
    "#Throwing out redundant predictors from x for both OLS and COR model fitting:      \n",
    "lower_triangle = pd.DataFrame(np.tril(cov_mat,-1), index = labels_cues, columns= labels_cues)    \n",
    "# == 1 has problems, so > .9999 grabs all the ones\n",
    "if (lower_triangle>0.99999999).any(axis=None): # if there is at least 1 complete redundancy (TRUE) in the lower triangle, \n",
    "    eliminate = np.transpose(np.where(np.array(cov_mat>0.99999999))) #1st col: row position, 2nd col: column position\n",
    "    var_delete = np.repeat(np.nan, eliminate.shape[0])\n",
    "    for f in range(eliminate.shape[0]):\n",
    "        if eliminate[f,0] != eliminate[f,1]: # only take those that are not the matrix diagonal\n",
    "            var_delete[f] = eliminate[f,0] # store the row number of that first variable  \n",
    "    redundant = np.sort(pd.DataFrame(var_delete[var_delete > 0])[0].unique()).astype(int) # only take each variable once, in order, and only as many as necessary to get rid of redundancy\n",
    "    m = redundant.shape[0] # how many redundancies there are overall\n",
    "    # deleting m-1 of the redundant predictors still gets rid of all redundancies\n",
    "    trainset = trainset.drop(list(labels_cues[redundant[0:(m-1)]]), axis = 1)\n",
    "    testset = testset.drop(list(labels_cues[redundant[0:(m-1)]]), axis = 1)\n",
    "    labels_cues = trainset.columns[0:-1]\n",
    "    Predictors = trainset.shape[1] - 1 # - dependent\n",
    "test = testset.drop(['dependent'],axis=1) \n",
    "x = trainset.loc[:,labels_cues]   \n",
    "y = trainset['dependent']\n",
    "y[y<0] = 0 #changes dummy coding (option B:0; option A:1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bayesian heuristic model (Paula Parpart et al, 2018)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.0\n",
      "1    1.0\n",
      "2    0.0\n",
      "3    0.0\n",
      "4    1.0\n",
      "5    0.0\n",
      "6    0.0\n",
      "7    1.0\n",
      "8    1.0\n",
      "9    0.0\n",
      "Name: dependent, dtype: float64 <class 'pandas.core.series.Series'>\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Current_Taxes</th>\n",
       "      <th>Number_of_bathrooms</th>\n",
       "      <th>Lot_size</th>\n",
       "      <th>Living_space</th>\n",
       "      <th>Number_of_garage_spaces</th>\n",
       "      <th>Number_of_rooms</th>\n",
       "      <th>Number_of_bedrooms</th>\n",
       "      <th>Age_of_house</th>\n",
       "      <th>Price</th>\n",
       "      <th>Number_of_fireplaces</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Current_Taxes  Number_of_bathrooms  Lot_size  Living_space  \\\n",
       "0            0.0                  0.0      -1.0           0.0   \n",
       "1            0.0                  0.0       0.0           0.0   \n",
       "2            0.0                  0.0       1.0           1.0   \n",
       "3           -1.0                 -1.0      -1.0          -1.0   \n",
       "4            1.0                  1.0       1.0           0.0   \n",
       "5           -1.0                 -1.0      -1.0          -1.0   \n",
       "6           -1.0                 -1.0       0.0          -1.0   \n",
       "7            1.0                  1.0       0.0           0.0   \n",
       "8            0.0                  0.0       0.0           0.0   \n",
       "9           -1.0                 -1.0       0.0           0.0   \n",
       "\n",
       "   Number_of_garage_spaces  Number_of_rooms  Number_of_bedrooms  Age_of_house  \\\n",
       "0                     -1.0             -1.0                 0.0          -1.0   \n",
       "1                      0.0              0.0                -1.0           0.0   \n",
       "2                      0.0              1.0                 0.0           1.0   \n",
       "3                      0.0             -1.0                 0.0           0.0   \n",
       "4                      0.0              1.0                 0.0          -1.0   \n",
       "5                     -1.0             -1.0                -1.0           0.0   \n",
       "6                      0.0             -1.0                 0.0          -1.0   \n",
       "7                      0.0              0.0                 0.0          -1.0   \n",
       "8                      1.0             -1.0                 0.0           0.0   \n",
       "9                      1.0              0.0                 0.0           0.0   \n",
       "\n",
       "   Price  Number_of_fireplaces  \n",
       "0    0.0                  -1.0  \n",
       "1    0.0                   1.0  \n",
       "2    0.0                   0.0  \n",
       "3    0.0                   0.0  \n",
       "4    0.0                   0.0  \n",
       "5   -1.0                   0.0  \n",
       "6   -1.0                  -1.0  \n",
       "7    0.0                   0.0  \n",
       "8    1.0                   1.0  \n",
       "9   -1.0                   0.0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(x)\n",
    "print(y, type(y))\n",
    "print('\\n')\n",
    "x\n",
    "#Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [intercept, weights_offdiag, weights_diag]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='2678' class='' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      33.48% [2678/8000 00:12<00:24 Sampling 4 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eta = np.array([50.0001]) #Try different ones! Parpart calls them penalties in her R code\n",
    "with pm.Model() as COR:\n",
    "    \n",
    "    #COR: Covariance Orthogonalizing Regularization (Paula Parpart's et al, 2018):\n",
    "    # the prior modulates sensitivity for covariation among cues...\n",
    "    # the model architecture implements m regression problems at once, \n",
    "    # meaning the criterion variable y is regressed onto all cues m times (Fig. A1)\n",
    "    \n",
    "    #Priors\n",
    "    #Weights matrix: \n",
    "    # \"an improper uniform prior on all W ii ( 1 ⩽ i ⩽ m ) and \n",
    "    # a prior of N (0, η^2 ) for all W ij ( i ≠ j )\" (pp 143)\n",
    "    w_diag = pm.Uniform('weights_diag', shape = Predictors) #predictors are the cues (1D shape: (Predictors,))\n",
    "    temp = tt.eye(Predictors)\n",
    "    w_diag_reshape = w_diag*temp #(shape: Predictors X Predictors; diagonal: w_diag)\n",
    "    w_offdiag = pm.Normal('weights_offdiag', mu = 0, sigma = eta[0], \n",
    "                          shape = (Predictors,Predictors-1))\n",
    "    left_zeros = tt.zeros((Predictors,1)) #to build a diagonal with zeros\n",
    "    bottom_zeros = tt.zeros((1,Predictors))\n",
    "    w_offdiag_reshape =  tt.concatenate([left_zeros, w_offdiag], axis = 1) \n",
    "    w_offdiag_reshape =  tt.concatenate([w_offdiag_reshape, bottom_zeros], axis = 0)\n",
    "    temp = tt.reshape(w_offdiag_reshape, (1, (Predictors+1)*Predictors))\n",
    "    w_offdiag_reshape = tt.reshape(temp[0, 0:-Predictors], (Predictors,Predictors)) #(shape: Predictors X Predictors; diagonal: zeros)\n",
    "    w_matrix = pm.Deterministic(\"weights\", w_diag_reshape + w_offdiag_reshape)\n",
    "    #print(w_matrix.tag.test_value.shape) #prints shape of theano variable\n",
    "    \n",
    "    intercept = pm.Normal('intercept', mu=0, sd=10)\n",
    "\n",
    "    #Likelihood\n",
    "    mu = intercept + pm.math.dot(x, w_matrix)  \n",
    "    theta = pm.Deterministic('theta', pm.math.sigmoid(mu))\n",
    "    y_multiplexed = np.transpose(np.tile(y,(Predictors,1)))\n",
    "    y_1 = pm.Bernoulli('y_1', p=theta, observed=y_multiplexed)\n",
    "\n",
    "    trace = pm.sample(1000, init = 'adapt_diag', tune=1000, target_accept = 0.95)\n",
    "    ppc = pm.sample_posterior_predictive(trace, samples=5000)\n",
    "    data = az.from_pymc3(trace=trace)\n",
    "\n",
    "az.plot_trace(data, var_names=[\"intercept\", \"weights_diag\"], compact=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.array([1,2,3,4]).reshape(1,4)\n",
    "matrix_eye = np.eye(4)\n",
    "matrix*matrix_eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_w = 5\n",
    "a = np.array(np.random.rand(n_w))\n",
    "d = np.diag(a)\n",
    "eta = 1\n",
    "b = np.random.normal(loc = 0, scale = eta, size = (n_w, n_w))\n",
    "b2 = -(np.eye(n_w) - 1)\n",
    "b = b*b2\n",
    "[d,b,d+b]\n",
    "\n",
    "matrix = np.array([[1, 2, 3],\n",
    "          [4, 5, 6],\n",
    "          [7, 8, 9],\n",
    "          [10, 11, 12]])\n",
    "\n",
    "matrix_new = np.zeros((4,5))\n",
    "#print(matrix_new)\n",
    "matrix_new[:-1,1:] = matrix.reshape(3,4)\n",
    "#print(matrix_new)\n",
    "#print(matrix_new.reshape(-1)[:-4])\n",
    "matrix_new = matrix_new.reshape(-1)[:-4].reshape(4,4)\n",
    "#print(matrix_new)\n",
    "\n",
    "matrix = np.array([[1, 2, 3],\n",
    "          [4, 5, 6],\n",
    "          [7, 8, 9],\n",
    "          [10, 11, 12]])\n",
    "print(matrix)\n",
    "\n",
    "d = matrix.shape[0]\n",
    "matrix_new = np.zeros((d, d+1))\n",
    "print(matrix_new)\n",
    "matrix_new[:-1,1:] = matrix.reshape((d-1, d))\n",
    "print(matrix_new)\n",
    "matrix_new = matrix_new.reshape(-1)[:-d].reshape(d,d)\n",
    "matrix_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Take-the-best\n",
    "dot_text = 'digraph G {rankdir=TB; compound=true; newrank=true; labelloc=\"t\";\\\n",
    "           label=\"TTB\";\\\n",
    "           /* general properties*/\\\n",
    "           node [margin=0, fixedsize=true, shape=plaintext,\\\n",
    "                 height=0.5, width=0.5, lblstyle=\"font=\\\\small\"];\\\n",
    "           /* links */\\\n",
    "           s -> tq;\\\n",
    "           aq -> tq;\\\n",
    "           bq -> tq;\\\n",
    "           tq -> yiq;\\\n",
    "           gamma -> yiq;\\\n",
    "           subgraph cluster0 {\\\n",
    "               margin = 10; labeljust=l; lblstyle=\"font=\\\\small\";\\\n",
    "               style = rounded;\\\n",
    "               label = \"$q questions$\";\\\n",
    "               tq;\\\n",
    "               aq;\\\n",
    "               bq;\\\n",
    "               subgraph cluster1 {\\\n",
    "                   margin = 10; labeljust=l; lblstyle=\"font=\\\\small\";\\\n",
    "                   style = rounded;\\\n",
    "                   label = \"$i subjects$\";\\\n",
    "                   yiq;\\\n",
    "               }\\\n",
    "           }\\\n",
    "           /* nodes */\\\n",
    "           s [label = \"$s$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           aq [label = \"$a_q$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           bq [label = \"$b_q$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           tq [label = \"$t_{q}$\", shape = square, peripheries = 2];\\\n",
    "           yiq [label = \"$y_{iq}$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           gamma [label = \"$gamma$\", shape = circle];\\\n",
    "           }' #warning: use single quote at start and end; double quotes for labels\n",
    "s = Source(dot_text, filename=\"img/7_CB/model_TTB.gv\", format=\"svg\") #THIS IS NOT THE FINAL ONE\n",
    "s.view()\n",
    "\n",
    "#distributions:\n",
    "# t_q = TTB_s(a_q, b_q)\n",
    "# \\gamma \\sim Uniform(0.5,1)\n",
    "# $$ y_{iq} \\sim\n",
    "#\\begin{cases}\n",
    "#\\text{Bernoulli}(\\gamma) & \\text{if $t_q = a$} \\\\\n",
    "#\\text{Bernoulli}(1- \\gamma) & \\text{if $t_q = b$} \\\\\n",
    "#\\text{Bernoulli}(0.5) & \\text{otherwise}\n",
    "#\\end{cases}  $$\n",
    "\n",
    "#To typeset latex stuff on the image: \n",
    "#1) open svg in inkscape and write latex formulas. Export as pdf (click the one that says latex)\n",
    "#   to change fontsize of latex in inkscape write before the expression: \n",
    "#        \\fontsize{34pt}{1em} $latex expression$ ... change #pt for size\n",
    "#2) go to overleaf or latex editor of choice and do this (https://castel.dev/post/lecture-notes-2/):\n",
    "#   2.1) In the preamble:\n",
    "#  \\usepackage{import}\n",
    "#  \\usepackage{xifthen}\n",
    "#  \\usepackage{pdfpages}\n",
    "#  \\usepackage{transparent}\n",
    "#  \\usepackage{graphics} \n",
    "\n",
    "#  \\newcommand{\\incfig}[1]{%\n",
    "#      \\def\\svgwidth{\\columnwidth}\n",
    "#      \\import{./figures/}{#1.pdf_tex} %PUT the inkscape .pdf_tex AND .pdf in a local folder called figures\n",
    "#  }\n",
    "#   2.2)In the body:\n",
    "#  \\begin{figure}[ht]\n",
    "#      \\centering\n",
    "#      \\scalebox{.65}{\\incfig{your_inkscape.pdf_tex}} #change scalebox proportion to rescale\n",
    "#      \\caption{Riemmans theorem}\n",
    "#      \\label{fig:riemmans-theorem}\n",
    "#  \\end{figure}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Latent-mixture model: Take-the-best + WADD\n",
    "dot_text = 'digraph G {rankdir=TB; compound=true; newrank=true; labelloc=\"t\";\\\n",
    "           label=\"TTB+WADD\";\\\n",
    "           /* general properties*/\\\n",
    "           node [margin=0, fixedsize=true, shape=plaintext,\\\n",
    "                 height=0.5, width=0.5, lblstyle=\"font=\\\\small\"];\\\n",
    "           /* links */\\\n",
    "           phi -> z;\\\n",
    "           z -> tq;\\\n",
    "           s -> tq;\\\n",
    "           aq -> tq;\\\n",
    "           bq -> tq;\\\n",
    "           tq -> yiq;\\\n",
    "           gamma -> yiq;\\\n",
    "           subgraph cluster0 {\\\n",
    "               margin = 10; labeljust=l; lblstyle=\"font=\\\\small\";\\\n",
    "               style = rounded;\\\n",
    "               label = \"$q questions$\";\\\n",
    "               tq;\\\n",
    "               aq;\\\n",
    "               bq;\\\n",
    "               subgraph cluster1 {\\\n",
    "                   margin = 10; labeljust=l; lblstyle=\"font=\\\\small\";\\\n",
    "                   style = rounded;\\\n",
    "                   label = \"$i subjects$\";\\\n",
    "                   yiq;\\\n",
    "                   tq;\\\n",
    "                   z;\\\n",
    "               }\\\n",
    "           }\\\n",
    "           /* nodes */\\\n",
    "           s [label = \"$s$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           aq [label = \"$a_q$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           bq [label = \"$b_q$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           tq [label = \"$t_{iq}$\", shape = square, peripheries = 2];\\\n",
    "           yiq [label = \"$y_{iq}$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           gamma [label = \"$gamma$\", shape = circle];\\\n",
    "           phi [label = \"$phi$\", shape = circle];\\\n",
    "           z [label = \"$z_i$\", shape = square];\\\n",
    "           }' #warning: use single quote at start and end; double quotes for labels\n",
    "s = Source(dot_text, filename=\"img/7_CB/model_TTB_WADD.gv\", format=\"svg\") #THIS IS NOT THE FINAL ONE\n",
    "s.view()\n",
    "\n",
    "#distributions:\n",
    "# \\phi \\sim Uniform(0,1)\n",
    "# \\z_i \\sim Bernoulli(\\phi)\n",
    "# \\gamma \\sim Uniform(0.5,1)\n",
    "#$$ t_{iq} = \n",
    "#\\begin{cases}\n",
    "#\\text{TTB}\\,(\\mathbf a_q,\\mathbf b_q) & \\text{if $z_i = 1$} \\\\\n",
    "#\\text{WADD}\\,(\\mathbf a_q,\\mathbf b_q) & \\text{if $z_i = 0$} \\\\\n",
    "#\\end{cases}  $$  \n",
    "# $$ y_{iq} \\sim\n",
    "#\\begin{cases}\n",
    "#\\text{Bernoulli}(\\gamma) & \\text{if $t_{iq} = a$} \\\\\n",
    "#\\text{Bernoulli}(1- \\gamma) & \\text{if $t_{iq} = b$} \\\\\n",
    "#\\text{Bernoulli}(0.5) & \\text{otherwise}\n",
    "#\\end{cases}  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Take-the-best different orders\n",
    "dot_text = 'digraph G {rankdir=TB; compound=true; newrank=true; labelloc=\"t\";\\\n",
    "           label=\"TTB+Order\";\\\n",
    "           /* general properties*/\\\n",
    "           node [margin=0, fixedsize=true, shape=plaintext,\\\n",
    "                 height=0.5, width=0.5, lblstyle=\"font=\\\\small\"];\\\n",
    "           /* links */\\\n",
    "           v -> s;\\\n",
    "           s -> tq;\\\n",
    "           aq -> tq;\\\n",
    "           bq -> tq;\\\n",
    "           tq -> yiq;\\\n",
    "           gamma -> yiq;\\\n",
    "           subgraph cluster0 {\\\n",
    "               margin = 10; labeljust=l; lblstyle=\"font=\\\\small\";\\\n",
    "               style = rounded;\\\n",
    "               label = \"$q \\\\\\ questions$\";\\\n",
    "               tq;\\\n",
    "               aq;\\\n",
    "               bq;\\\n",
    "               subgraph cluster1 {\\\n",
    "                   margin = 10; labeljust=l; lblstyle=\"font=\\\\small\";\\\n",
    "                   style = rounded;\\\n",
    "                   label = \"$i \\\\\\ subj$\";\\\n",
    "                   yiq;\\\n",
    "                   tq;\\\n",
    "                   s;\\\n",
    "                   subgraph cluster2{\\\n",
    "                       margin = 10; labeljust=l; lblstyle=\"font=\\\\small\";\\\n",
    "                       style = rounded;\\\n",
    "                       label = \"$c \\\\\\ cue$\";\\\n",
    "                       v;\\\n",
    "                   }\\\n",
    "               }\\\n",
    "           }\\\n",
    "           /* nodes */\\\n",
    "           v [label = \"$v_{ic}$\", shape = circle];\\\n",
    "           s [label = \"$s_i$\", peripheries = 2, shape = square];\\\n",
    "           aq [label = \"$a_q$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           bq [label = \"$b_q$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           tq [label = \"$t_{iq}$\", shape = square, peripheries = 2];\\\n",
    "           yiq [label = \"$y_{iq}$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           gamma [label = \"$\\\\\\gamma$\", shape = circle];\\\n",
    "           }' #warning: use single quote at start and end; double quotes for labels\n",
    "s = Source(dot_text, filename=\"img/7_CB/model_TTB_order.gv\", format=\"svg\") #THIS IS NOT THE FINAL ONE\n",
    "s.view()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "TODO: \n",
    "\n",
    "* Seguir con https://www.sciencedirect.com/science/article/pii/S0010028517303286. Acá la idea es ver si puedo poner su modelo en pymc3\n",
    "* Si lo anterior no lo puedo hacer, poner mi modelo de intrinsic whole number bias en Pymc3. Es una heurística. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
