{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on PyMC v3.11.4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#Tables and matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Stats\n",
    "import scipy.stats as st\n",
    "from scipy.optimize import fmin\n",
    "from scipy import integrate\n",
    "from scipy.stats.mstats import mquantiles\n",
    "import statistics \n",
    "\n",
    "#Probabilistic programs\n",
    "#!pip install pymc3\n",
    "#!pip install pymc==4.0.0b1\n",
    "import pymc3 as pm\n",
    "#import pymc as pm\n",
    "import theano.tensor as tt \n",
    "from theano.compile.ops import as_op\n",
    "print('Running on PyMC v{}'.format(pm.__version__))\n",
    "\n",
    "#Graphs\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import altair as alt\n",
    "from altair_saver import save #ademas instalar en terminal: brew cask install chromedriver\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, HBox, VBox, Layout\n",
    "from graphviz import Source, Digraph\n",
    "import dot2tex as d2t\n",
    "from latex import build_pdf\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import arviz as az\n",
    "\n",
    "#User-defined functions (in the same folder as the notebook)\n",
    "import my_fun as mf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision-making (risk)\n",
    "\n",
    "Santiago Alonso-Díaz, PhD <br>\n",
    "Universidad Javeriana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "95 years old men with a tumor \n",
    "- 0.9 probability that is cancerous \n",
    "- If not, he has 34.8 months of life\n",
    "- If bad:\n",
    "    - With radiotherapy,  16.7 months of life\n",
    "    - With surgery, 0.35 prob. of dying, 0.65 of 20.3 months of life\n",
    "    - Without treatment, 5.6 months of life\n",
    "    - Radiotherapy or surgery reduces months of life by an additional 1 month\n",
    "\n",
    "Show that the radiotherapy is the preferred treatment (i.e. that expected value is 17.5 months of life)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In decision theory, the assumption is that we chose based on expected utility\n",
    "\n",
    "$$ U(x) = p(x) v(x)$$\n",
    "\n",
    "Question 1: Is probability the same as risk? No\n",
    "\n",
    "Question 2: Is risk the same as uncertainty? No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The word risk has increased in usage (Li & Hertwig, 2020)\n",
    "<center><img src=\"img/6_CB/Li1.svg\" width = \"400\" height = '400'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Main associations with risk (Li & Hertwig, 2020)\n",
    "<center><img src=\"img/6_CB/Li2.svg\" width = \"400\" height = '400'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Risk preferences in different domains correlate (Dohmen, et al, 2011)\n",
    "<center><img src=\"img/6_CB/Dohmen1.png\" width = \"500\" height = '500'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\" style = \"font-size: 70px\"> What is risk? </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb436eb0bdc48e2a6df296103c968fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Distr.: ', options=(('Normal', 0), ('Uniform', 1), ('Log-Normal', 2)), va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In risk, we know the probability distribution of the event.\n",
    "# We can have different measures of risk from the probability distributions (e.g. variance)\n",
    "wD = widgets.Dropdown(options=[('Normal', 0), \n",
    "                               ('Uniform', 1), \n",
    "                               ('Log-Normal', 2)],\n",
    "                        value=0, description='Distr.: ')\n",
    "out = widgets.interactive_output(mf.dropdown_callback,{'distr': wD})\n",
    "VBox([wD, out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\" style = \"font-size: 70px\"> Risk attitudes in economics </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9364ee65f5d741718705d22d68ba4dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(FloatSlider(value=1.0, description='Attitude', max=2.0, step=0.01), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Risk attitudes as concavity of the utility function\n",
    "# High value, low probability, high risk\n",
    "wD = widgets.FloatSlider(min = 0, max = 2, step = 0.01, value=1,\n",
    "                        description = 'Attitude')\n",
    "out = widgets.interactive_output(mf.slider_econ_risk,{'alpha': wD})\n",
    "VBox([wD, out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\" style = \"font-size: 70px\"> Risk attitudes in cognitive and behavioral sciences </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hay muchas alternativas:\n",
    "* Regret and disappointment theory (Bell, 1982, 1985; Loomes & Sugden, 1982)\n",
    "* Priority heuristic (Brandstätter, Gigerenzer, & Hertwig, 2006)\n",
    "* Transfer-of-attention exchange model (Birnbaum, 2008; Birnbaum & Chavez, 1997)\n",
    "* Decision field theory (Busemeyer & Townsend, 1993; Roe, Busemeyer, & Townsend, 2001) \n",
    "* Weighted utility theory (e.g. Fishburn, 1983)\n",
    "* Proportional difference model (González-Vallejo, 2002) \n",
    "* Decision affect theory (Mellers, 2000) \n",
    "* Dual system model of preference under risk (Mukherjee, 2010) \n",
    "* Rank-dependent expected utility theories (e.g., Quiggin, 1982)\n",
    "* Decisions by sampling (Stewart, Chater, & Brown, 2006)\n",
    "* Prospect theory (Kahneman & Tversky, 1979)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\" style = \"font-size: 70px\"> How can we measure risk? </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Self-reports\n",
    "    * Sensation seeking (Zuckerman, Eysenck, & Eysenck, 1978)\n",
    "    * Adventure seeking (Eysenck, Pearson, Easting, & Allsopp, 1985)\n",
    "    * Impulsivity (Barratt, 1985; Eysenck et al., 1985)\n",
    "    * Others (Frey, et al, 2017)\n",
    "* Behavioral\n",
    "    * The Balloon Analogue Risk Task (Lejuez, et al, 2002)\n",
    "    * Loteries (Kahneman & Tversky, 1979)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Descriptive Bayesian models of risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# BART\n",
    "<center><img src=\"img/6_CB/Lejuez1.png\" width = \"600\" height = '600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/6_CB/Lejuez2.png\" width = \"600\" height = '600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Multiple trials. Different ballon colors signal different risks (the subject learns them by trial and error). Each pump increases the risk of explotion and dollars win.\n",
    "<center><img src=\"img/6_CB/Lejuez3.png\" width = \"500\" height = '500'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What type of risk is measured with BART?\n",
    "\n",
    "Lejuez, et al, 2002 correlated BART with other measures\n",
    "\n",
    "* Sensation seeking, impulsivity, empathy, depression, personality, drug addictions, gambling addiction, use of preservatives, others)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Optimal pumps (center) \n",
    "<center><img src=\"img/6_CB/Lejuez4.png\" width = \"600\" height = '600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Results\n",
    "\n",
    "* Risk aversion ($pumps < pumps_{optimo}$)\n",
    "* Low variability in orange and yellow\n",
    "\n",
    "<center><img src=\"img/6_CB/Lejuez5.png\" width = \"600\" height = '600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "BART correlates with self-reports. Lejuez, et al, 2002 summarised the self-reports with (PCA, principal components analysis).\n",
    "\n",
    "PCA (hypothetical example with two measures)\n",
    "<center><img src=\"img/6_CB/GaussianScatterPCA.svg\" width = \"200\" height = '200'></center>\n",
    "\n",
    "PCA (Lejuez)\n",
    "\n",
    "<center><img src=\"img/6_CB/Lejuez6.png\" width = \"600\" height = '600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "BART correlated with the PCA risk dimensions\n",
    "\n",
    "<center><img src=\"img/6_CB/Lejuez7.png\" width = \"500\" height = '500'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Discusion\n",
    "* BART measures risk preferences, not fully captures with self-reports\n",
    "* In BART, people are risk averse on average (fewer pumps than optimal).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Aplicación del BART: Contrafactuales\n",
    "\n",
    "\n",
    "<center><img src=\"img/6_CB/FitzGibbon1.png\" width = \"600\" height = '600'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Existencialismo ¿Qué tal si hubiera hecho esto?\n",
    "* ¿pagar para saber que hubiera pasado aún si lleva a \"dolor\"? e.g. confirmar infidelidad de su pareja\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Información no siempre es instrumental (i.e. no trae beneficios), pero satisface curiosidad.\n",
    "\n",
    "<center><img src=\"img/6_CB/FitzGibbon2.png\" width = \"500\" height = '520'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Información no instrumental incluso activa areas del cerebro de recompensa (Fitzgibbon, et al, 2020)\n",
    "\n",
    "<center><img src=\"img/6_CB/FitzGibbon3.png\" width = \"600\" height = '600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Predicción: \n",
    "* Curiosidad contrafactual es un motivador así tenga costos como arrepentimiento.\n",
    "* La curiosidad contrafactual satisfecha induce cambios de comportamientos (e.g. preferir más riesgo) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Podemos usar BART para estudiar curiosidad contra-factual\n",
    "\n",
    "<center><img src=\"img/6_CB/FitzGibbon4.png\" width = \"500\" height = '500'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/6_CB/FitzGibbon5.png\" width = \"700\" height = '700'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Los 5 experimentos fueron similares exceptuando costo info. pues esa era la intervención.\n",
    "<center><img src=\"img/6_CB/FitzGibbon6.png\" width = \"600\" height = '600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Los turnos con información aumentaban el arrepentimiento (within-subject). Missed opportunity es el numero de pumps adicionales que aguantaba el globo.\n",
    "\n",
    "<center><img src=\"img/6_CB/FitzGibbon7.png\" width = \"600\" height = '600'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Modelación de riesgo en BART (BART chapter, Lee & Wagenmakers, 2013, textbook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Task description: The probability of the balloon bursting is constant, and the expected gain of every decision to pump is zero.\n",
    "<center><img src=\"img/6_CB/LeeWag1.png\" width = \"600\" height = '600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\gamma^{+}: \\text{risk taking}, \\ \\beta: \\text{consistency}, \\ p: \\text{belief of burst prob.}, \\ d_{jk}: \\text{observed choice}, \\ Choice_k: \\text{k pumps}, \\\\ \\omega: \\text{# of pumps considered optimal}, \\theta_{jk}:\\text{cashing prob.}$\n",
    "<center><img src=\"img/6_CB/model_BART.svg\" width = \"451\" height = '450'></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Understanding/visualizing the parameters\n",
    "p = 0.15 #(belief) prob. of bursting. Assumed by lee and wagenmakers\n",
    "gammap = np.linspace(0,10,100)\n",
    "beta = np.linspace(0,10,100)\n",
    "omega = - (gammap)/(np.log(1-p))\n",
    "\n",
    "#why omega is npumps_optimal conditional on a risk parameter gammap?\n",
    "#let's rename omega to npumps_optim and do some basic algebra\n",
    "#npumps_optim*np.log(1-p) = - gammap\n",
    "#e^(npumps_optim*np.log(1-p)) = (e^-gammap)\n",
    "#e^(np.log((prob_noburst)^npumps_optim)) = (e^-gammap)\n",
    "#prob_noburst^npumps_optim = (e^-gammap) #important one\n",
    "#Conditional on a gammap, the equality means that\n",
    "#above is not tolerable, below is still tolerable. \n",
    "#npumps_optim fullfils the equality \n",
    "\n",
    "k = 9 #observed number of pumps\n",
    "thetajk = 1/(1+np.exp(beta*(k-omega))) #beta is a temperature parameter\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize = [10,4])\n",
    "ax[0].plot(gammap, omega)\n",
    "ax[0].set_xlabel('Risk attitudes ($\\\\gamma^{+}$)')\n",
    "ax[0].set_ylabel('Optimal number of pumps ($\\\\omega$)')\n",
    "\n",
    "ax[1].plot(beta, thetajk)\n",
    "ax[1].set_xlabel('Consistency or temperature ($\\\\beta$)')\n",
    "ax[1].set_ylabel('Prob. of choosing to cash');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#The following implementation in PyMC3 was done by Junpeng Lao: \n",
    "# https://github.com/junpenglao/Bayesian-Cognitive-Modeling-in-Pymc3\n",
    "\n",
    "#First: data setup\n",
    "p = 0.15  # (Belief of) bursting probability\n",
    "ntrials = 90  # Number of trials for the BART\n",
    "extreme_npumps = 30 #some unlikely large number\n",
    "\n",
    "Data = pd.read_csv(\"data/6_CB/GeorgeSober.txt\", sep=\"\\t\") #subject named George (he can be sober, tipsy, or drunk; change the filename)\n",
    "print(Data.groupby(['pres.bl']).mean())\n",
    "#gr.fact: I think it is the visual growth of balloon i.e. a cue for the subject\n",
    "#'prob': (real) of bursting; \n",
    "#'pumps': on that trial; \n",
    "#'cash': won on the trial; 'total': cash so far\n",
    "\n",
    "cash = np.asarray(Data[\"cash\"] != 0, dtype=int)\n",
    "npumps = np.asarray(Data[\"pumps\"], dtype=int)\n",
    "options = cash + npumps #type of choices experienced i.e. receiving some cash and pumping n times\n",
    "d = np.full([ntrials, extreme_npumps], np.nan) \n",
    "k = np.full([ntrials, extreme_npumps], np.nan) \n",
    "# response vector\n",
    "for j, ipumps in enumerate(npumps): \n",
    "    inds = np.arange(options[j], dtype=int) \n",
    "    k[j, inds] = inds + 1 #+1 because arange starts at 0\n",
    "    if ipumps > 0:\n",
    "        d[j, 0:ipumps] = 0\n",
    "    if cash[j] == 1:\n",
    "        d[j, ipumps] = 1\n",
    "\n",
    "indexmask = np.isfinite(d) #to clean nans\n",
    "d = d[indexmask] #vector with cashing decisions (0:no, 1: yes); a 0 along a reset of k below means a burst \n",
    "k = k[indexmask] #vector with number of pumps at each decision point in d\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npumps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model():\n",
    "    #Priors\n",
    "    gammap = pm.Uniform(\"gammap\", lower=0, upper=10, testval=1.2)\n",
    "    beta = pm.Uniform(\"beta\", lower=0, upper=10, testval=0.5)\n",
    "    omega = pm.Deterministic(\"omega\", -gammap / np.log(1 - p))\n",
    "    \n",
    "    #Likelihood\n",
    "    thetajk = 1 - pm.math.invlogit(-beta * (k - omega))\n",
    "\n",
    "    djk = pm.Bernoulli(\"djk\", p=thetajk, observed=d)\n",
    "\n",
    "    trace = pm.sample(tune=2000, cores=4, chains = 4)\n",
    "    data = az.from_pymc3(trace=trace)\n",
    "\n",
    "\n",
    "az.plot_trace(data, var_names=[\"gammap\", \"beta\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "az.plot_trace(data, var_names=[\"omega\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#This plots the fig. 16.3 of lee, wagenmakers, 2013, textbook\n",
    "gammaplus = trace[\"gammap\"]\n",
    "beta = trace[\"beta\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].hist(npumps, bins=range(1, 9), rwidth=0.8, align=\"left\")\n",
    "axes[0].set_xlabel(\"Number of Pumps\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Frequency\", fontsize=12)\n",
    "  \n",
    "my_pdf1 = st.kde.gaussian_kde(gammaplus)\n",
    "x1 = np.linspace(0.5, 1, 200)\n",
    "axes[1].plot(x1, my_pdf1(x1), \"k\", lw=2.5, alpha=0.6)  # distribution function\n",
    "axes[1].set_xlim((0.5, 1))\n",
    "axes[1].set_xlabel(r\"$\\gamma^+$\", fontsize=15)\n",
    "axes[1].set_ylabel(\"Posterior Density\", fontsize=12)\n",
    "\n",
    "my_pdf2 = st.kde.gaussian_kde(beta)\n",
    "x2 = np.linspace(0.3, 1.3, 200)\n",
    "axes[2].plot(\n",
    "    x2, my_pdf2(x2), \"k\", lw=2.5, alpha=0.6,\n",
    ")  # distribution function\n",
    "axes[2].set_xlim((0.3, 1.3))\n",
    "axes[2].set_xlabel(r\"$\\beta$\", fontsize=15)\n",
    "axes[2].set_ylabel(\"Posterior Density\", fontsize=12)\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercises\n",
    "\n",
    "1) Apply the model to data from a different subject, Bill, provided in the file BillSober.txt. Compare the estimated parameters for George and Bill. Who has the greater propensity for risk?\n",
    "\n",
    "2) What happens if two pumps are added to each trial for George’s data? Make this change to the npumps variable (i.e. duplicate npumps). Which of the two parameters changed the most?\n",
    "\n",
    "3) Modify George’s data in a different way to affect the behavioral consistency parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### But what does it mean? \n",
    "\n",
    "We have data from the same subject with different alcohol levels. We can estimate parameters for each level or do a hierarchical model. Let's do the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Alcohol and risk.\n",
    "\n",
    "Example 1 (driving) (Burian, Liguori, & Robinson,2002).\n",
    "\n",
    "Penalty severity decreases risky driving in those without alcohol (top panel).\n",
    "\n",
    "In the most extreme penalty, alcohol increases risky driving (inverted u-shape) (bottom panel) \n",
    "<center><img src=\"img/6_CB/Risk_drive_alcohol.png\" width = \"500\" height = '500'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2 (unsafe sex)\n",
    "\n",
    "Similar to driving, there seems to be an inverse-u shaped effect of alcohol in risky behavior\n",
    "\n",
    "<center><img src=\"img/6_CB/Risk_sex_preservatives.png\" width = \"500\" height = '500'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A hierarchical extension of the BART model\n",
    "  \n",
    "  \n",
    "$$ \\mu_{\\gamma^{+}} \\sim \\text{Uniform}(0,10) $$\n",
    "$$ \\sigma_{\\gamma^{+}} \\sim \\text{Uniform}(0,10) $$\n",
    "$$ \\mu_{\\beta} \\sim \\text{Uniform}(0,10) $$\n",
    "$$ \\sigma_{\\beta} \\sim \\text{Uniform}(0,10) $$\n",
    "$$ \\gamma^{+}_i \\sim \\text{Gaussian}(\\mu_{\\gamma^{+}}, 1/\\sigma_{\\gamma^{+}}^2) $$\n",
    "$$ \\beta_i \\sim \\text{Gaussian}(\\mu_{\\beta}, 1/\\sigma_{\\beta}^2) $$\n",
    "$$ \\omega_i = -\\gamma^{+}_i \\,/\\,\\text{log}(1-p) $$\n",
    "$$ \\theta_{ijk} = \\frac{1} {1+e^{\\beta_i(k-\\omega_i)}} $$\n",
    "$$ d_{ijk} \\sim \\text{Bernoulli}(\\theta_{ijk}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Discusión en clase\n",
    "¿Cómo cambiaría el modelo gráfico de abajo para hacerlo jerárquico y siga las formulas anteriores?\n",
    "<center><img src=\"img/6_CB/model_BART.svg\" width = \"451\" height = '450'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hierarchical means we assume a common source for the parameters risk and consistency. The common source is the subject (e.g. George). The parameters change according to the alcohol level conditions\n",
    "<center><img src=\"img/6_CB/model_BART_hierarchical.svg\" width = \"500\" height = '500'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "p = 0.15  # (Belief of) bursting probability\n",
    "ntrials = 90  # Number of trials for the BART\n",
    "Ncond = 3\n",
    "extreme_npumps = 30 #some unlikely large number\n",
    "\n",
    "dall = np.full([Ncond, ntrials, extreme_npumps], np.nan)\n",
    "options = np.zeros((Ncond, ntrials))\n",
    "kall = np.full([Ncond, ntrials, extreme_npumps], np.nan)\n",
    "npumps_ = np.zeros((Ncond, ntrials))\n",
    "\n",
    "for icondi in range(Ncond):\n",
    "    if icondi == 0:\n",
    "        Data = pd.read_csv(\"data/6_CB/GeorgeSober.txt\", sep=\"\\t\")\n",
    "    elif icondi == 1:\n",
    "        Data = pd.read_csv(\"data/6_CB/GeorgeTipsy.txt\", sep=\"\\t\")\n",
    "    elif icondi == 2:\n",
    "        Data = pd.read_csv(\"data/6_CB/GeorgeDrunk.txt\", sep=\"\\t\")\n",
    "    # Data.head()\n",
    "    cash = np.asarray(Data[\"cash\"] != 0, dtype=int)\n",
    "    npumps = np.asarray(Data[\"pumps\"], dtype=int)\n",
    "    npumps_[icondi, :] = npumps\n",
    "    options[icondi, :] = cash + npumps\n",
    "    # response vector\n",
    "    for j, ipumps in enumerate(npumps):\n",
    "        inds = np.arange(options[icondi, j], dtype=int)\n",
    "        kall[icondi, j, inds] = inds + 1\n",
    "        if ipumps > 0:\n",
    "            dall[icondi, j, 0:ipumps] = 0\n",
    "        if cash[j] == 1:\n",
    "            dall[icondi, j, ipumps] = 1\n",
    "\n",
    "indexmask = np.isfinite(dall)\n",
    "dij = dall[indexmask]\n",
    "kij = kall[indexmask]\n",
    "condall = np.tile(np.arange(Ncond, dtype=int), (30, ntrials, 1))\n",
    "condall = np.swapaxes(condall, 0, 2)\n",
    "cij = condall[indexmask] #condition index (for dij and kij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "chains = 4\n",
    "with pm.Model() as model2:\n",
    "    #Hyperparameters for hierarchical priors\n",
    "    mu_g = pm.Uniform(\"mu_g\", lower=0, upper=10)\n",
    "    sigma_g = pm.Uniform(\"sigma_g\", lower=0, upper=10)\n",
    "    mu_b = pm.Uniform(\"mu_b\", lower=0, upper=10)\n",
    "    sigma_b = pm.Uniform(\"sigma_b\", lower=0, upper=10)\n",
    "    \n",
    "    #Priors (lower level)\n",
    "    gammap = pm.Normal(\"gammap\", mu=mu_g, sd=sigma_g, shape=Ncond)\n",
    "    beta = pm.Normal(\"beta\", mu=mu_b, sd=sigma_b, shape=Ncond)\n",
    "\n",
    "    #Likelihood\n",
    "    omega = -gammap[cij] / np.log(1 - p)\n",
    "    thetajk = 1 - pm.math.invlogit(-beta[cij] * (kij - omega))\n",
    "    \n",
    "    djk = pm.Bernoulli(\"djk\", p=thetajk, observed=dij)\n",
    "    \n",
    "    #get starting values with variational inference\n",
    "    approx = pm.fit(\n",
    "        n=100000, method=\"advi\", obj_optimizer=pm.adagrad_window\n",
    "    )  # type: pm.MeanField\n",
    "    start = approx.sample(draws=chains, include_transformed=True)\n",
    "    #sample\n",
    "    trace2 = pm.sample(\n",
    "         tune=2000, target_accept=0.95, chains=chains, \n",
    "        cores = 10, init=\"adapt_diag\", start=list(start)\n",
    "    )\n",
    "    data = az.from_pymc3(trace=trace2)\n",
    "\n",
    "\n",
    "\n",
    "az.plot_trace(data, var_names=[\"gammap\", \"beta\"], compact=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "gammaplus = trace2[\"gammap\"]\n",
    "beta = trace2[\"beta\"]\n",
    "ylabels = [\"Sober\", \"Tipsy\", \"Drunk\"]\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "for ic in range(Ncond):\n",
    "    ax0 = axes[ic, 0]\n",
    "    ax0.hist(npumps_[ic], bins=range(1, 10), rwidth=0.8, align=\"left\")\n",
    "    ax0.set_xlabel(\"Number of Pumps\", fontsize=12)\n",
    "    ax0.set_ylabel(ylabels[ic], fontsize=12)\n",
    "\n",
    "    ax1 = axes[ic, 1]\n",
    "    my_pdf1 = st.kde.gaussian_kde(gammaplus[:, ic])\n",
    "    x1 = np.linspace(0.5, 1.8, 200)\n",
    "    ax1.plot(x1, my_pdf1(x1), \"k\", lw=2.5, alpha=0.6)  # distribution function\n",
    "    ax1.set_xlim((0.5, 1.8))\n",
    "    ax1.set_xlabel(r\"$\\gamma^+$\", fontsize=15)\n",
    "    ax1.set_ylabel(\"Posterior Density\", fontsize=12)\n",
    "\n",
    "    ax2 = axes[ic, 2]\n",
    "    my_pdf2 = st.kde.gaussian_kde(beta[:, ic])\n",
    "    x2 = np.linspace(0.1, 1.5, 200)\n",
    "    ax2.plot(x2, my_pdf2(x2), \"k\", lw=2.5, alpha=0.6)  # distribution function\n",
    "    ax2.set_xlim((0.1, 1.5))\n",
    "    ax2.set_xlabel(r\"$\\beta$\", fontsize=15)\n",
    "    ax2.set_ylabel(\"Posterior Density\", fontsize=12)\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercises\n",
    "\n",
    "1) Apply the model to the data from the other subject, Bill. Does alcohol have the same effect on Bill as it did on George?\n",
    "\n",
    "2) Apply the non-hierarchical model to each of the six data files independently. Compare the results for the two parameters to those obtained from the hierarchical model, and explain any differences.\n",
    "\n",
    "3) The hierarchical model provides a structured relationship between the drinking conditions, but is still applied independently to each subject. Many of the applications of hierarchical modeling considered in our case studies, however, involve structured relationships between subjects, to capture individual differences. Develop a graphical model that extends the hierarchical model above to incorporate hierarchical structure both for drinking conditions and subjects. How could interactions between these two factors be modeled?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now let's apply a BART model to the counterfactual paper by FitzGibbon, et al (2021, Psych. Science). \n",
    "\n",
    "Let's model their hypothesis/finding that seeking counterfactual information affects emotions and behavior:\n",
    "\n",
    "<center><img src=\"img/6_CB/FitzGibbon9.png\" width = \"800\" height = '800'></center>\n",
    "\n",
    "<center><img src=\"img/6_CB/FitzGibbon8.png\" width = \"800\" height = '800'></center>\n",
    "\n",
    "Discussion/exercise\n",
    "\n",
    "How would you extend the non-hierarchical BART diagram to measure FitzGibbon hypothesis/finding? What does behavior adjustment mean? Risk $\\gamma^+$ or consistency $\\beta$? Both? Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Fitzgibbon suggest changes in risk: \"The worse participants felt, the riskier they became on the next trial\" pp. 9. \n",
    "\n",
    "Let's focus on that possibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Diagram done  in https://app.diagrams.net/. We model change in emotion in trial t and number of pumps in the trial t + 1.\n",
    "\n",
    "$\\delta \\ lim$ is missed opportunity i.e. n_pumps - balloon limit. \n",
    "\n",
    "$\\delta \\ emo$ is change in emotion\n",
    "\n",
    "$\\zeta$ will tell us how much risk attitudes ($\\gamma^+$) change with changes in emotion.\n",
    "\n",
    "<center><img src=\"img/6_CB/model_BART_counterfactuals.svg\" width = \"800\" height = '800'></center>\n",
    "\n",
    "# Class exercise \n",
    "Expand the non-hierarchical pymc code to reflect this diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "Data = pd.read_csv(\"data/6_CB/Counterfactual_Curiosity/E1.csv\")\n",
    "sID = Data['subject'].unique() \n",
    "for s in sID: #standardize emotion_change_rating by subject\n",
    "    idx = Data['subject'] == s\n",
    "    zscore = st.zscore(Data.loc[idx,'emotion_change_rating'])\n",
    "    Data.loc[idx,'emotion_change_rating'] = zscore\n",
    "s = 29 #subject to run or write 'all' for all subjects\n",
    "#representative subjects E1: \n",
    "#1,6,11,12,13,18,19,20,21,22,24,27 (negative slope emotion-risk on avg parameters), \n",
    "#2,3,4,5,7,14,17,23,25,26,28,29 (positive slope)\n",
    "# subject 27 E1 risk was highly affected by emotion according to the model\n",
    "if s == 'all':\n",
    "    idx = (Data['outcome'] == 'bank') &  (~Data['next_trial_n_pumps'].isna()) \n",
    "else:\n",
    "    idx = (Data['subject'] == sID[s]) & (Data['outcome'] == 'bank') &  (~Data['next_trial_n_pumps'].isna())\n",
    "Data = Data.loc[idx,:]\n",
    "print(Data.columns)\n",
    "#pump_value: how many points EACH pump gives\n",
    "#n_pumps: # of pumps participant asked the rabbit to do\n",
    "#limit: if n_pumps>limit then burst, else bank\n",
    "#information columns: if sought, participants saw the limit of the balloon\n",
    "#next_trial_points: pump_value_{trial+1}*n_pumps_{trial+1}\n",
    "Data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Non-hierarchical BART for FitzGibbon, et al, 2021 (see appendix for hierarchical version)\n",
    "p = 0.5\n",
    "npump_max = 12 #from the Fitzgibbon experimental design\n",
    "#emo_change_scale = [-200,200] #from the Fitzgibbon paper\n",
    "emo_change_scale = [-3,3] #standardized (zscore; see cell above)\n",
    "delta_emo_data = np.array(Data['emotion_change_rating'])\n",
    "info = np.array(Data['information_sought'])\n",
    "no_info = np.array(~np.array(Data['information_sought'], dtype = 'bool'), dtype = 'int')\n",
    "diff_limit = np.array(Data['diff_pumps_limit'])\n",
    "pump_value = np.array(Data['pump_value']) #including this does not make too much difference, it is uniform\n",
    "npumps_next_trial = np.array(Data['next_trial_n_pumps'])\n",
    "with pm.Model() as BART_CF:\n",
    "    #priors\n",
    "    kappas = pm.Uniform('kappas', \n",
    "                        lower = emo_change_scale[0], \n",
    "                        upper = emo_change_scale[1], shape = 4)\n",
    "    sigma_emo = pm.Uniform('sigma_emo', lower = 0 , upper = 50 )\n",
    "    maxgammap = pm.Uniform('maxgammap', lower = 0 , upper = 10)\n",
    "    zeta = pm.Uniform('zeta', lower = -1, upper = 1) #it worked with -1,1 \n",
    "    gammap = (maxgammap)/(1+tt.exp(-zeta*(delta_emo_data)))\n",
    "    #gammap = (maxgammap)/(1+tt.exp(-zeta*(delta_emo_data*pump_value)))\n",
    "    beta = pm.Normal(\"beta\", mu = 0.8, sd = 0.2) #Based on the previous BART\n",
    "    omega = pm.Deterministic(\"omega\", -gammap / np.log(1 - p))\n",
    "    \n",
    "    \n",
    "    #likelihood\n",
    "    mu_emo = (kappas[0] + kappas[1]*diff_limit)*no_info + (kappas[2] + kappas[3]*diff_limit)*info\n",
    "    delta_emoj = pm.Normal('delta_emo', mu = mu_emo, sigma = sigma_emo, \n",
    "                           observed = delta_emo_data)\n",
    "\n",
    "    \n",
    "    thetaj = 1 - pm.math.invlogit(-beta * (npumps_next_trial - omega)) \n",
    "    \n",
    "    dj = pm.Binomial(\"dj\", p=thetaj, n=npump_max, observed=npumps_next_trial)\n",
    "    \n",
    "    \n",
    "    trace = pm.sample(tune=2000, target_accept = 0.99, cores = 4, chains = 4)\n",
    "    \n",
    "    data = az.from_pymc3(trace=trace)\n",
    "    ppc = pm.sample_posterior_predictive(trace)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(s)\n",
    "az.plot_trace(data, var_names=[\"maxgammap\", 'zeta', 'kappas', \"beta\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Posterior plots\n",
    "gammaplus = trace[\"maxgammap\"]\n",
    "beta = trace[\"beta\"]\n",
    "zeta = trace[\"zeta\"]\n",
    "kappas = trace[\"kappas\"]\n",
    "\n",
    "def myplot_cf(posterior, ax, xlim, xlab):\n",
    "    my_pdf1 = st.kde.gaussian_kde(posterior)\n",
    "    x1 = np.linspace(xlim[0], xlim[1], 1000)\n",
    "    ax.plot(x1, my_pdf1(x1), \"k\", lw=2.5, alpha=0.6)  # distribution function\n",
    "    ax.set_xlabel(xlab, fontsize=15)\n",
    "    ax.set_ylabel(\"Posterior Density\", fontsize=12)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
    "myplot_cf(gammaplus, axes[0,0], [7.8,8.5], r\"$Max_{\\gamma^+}$\")\n",
    "myplot_cf(beta, axes[0,1], [0.38,0.45], r\"$\\beta$\")\n",
    "myplot_cf(zeta, axes[0,2], [-.05,.05], r\"$\\zeta$\")\n",
    "myplot_cf(kappas[:,0], axes[0,3], [0.3,0.8], r\"$\\kappa_0$ (intercept no info)\")\n",
    "myplot_cf(kappas[:,1], axes[1,0], [-.08,.04], r\"$\\kappa_1$ (slope no info)\")\n",
    "myplot_cf(kappas[:,2], axes[1,1], [0.9,1.35], r\"$\\kappa_2$ (intercept info)\")\n",
    "myplot_cf(kappas[:,3], axes[1,2], [-0.35,-0.23], r\"$\\kappa_3$ (slope  info)\")\n",
    "\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Posterior predictive checks\n",
    "#The model seems to qualitatively predict the data fairly well with some caveats.\n",
    "#The two peaks on observed delta_emo seem to be only on no info trials (see appendix, hierarchical)\n",
    "#Human subjects seem to overselect n pumps on the middle range (see dj observed)\n",
    "az.plot_ppc(az.from_pymc3(posterior_predictive=ppc, model=BART_CF));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model seems to qualitatively predict the data fairly well\n",
    "plt.figure(figsize=(5, 5)); \n",
    "idx = Data['information_sought']==1\n",
    "plt.plot(diff_limit[idx], delta_emo_data[idx], '.b',  label = 'Info')\n",
    "hdi_intercept = az.hdi(trace['kappas'][:,2]) #high density interval\n",
    "hdi_slope = az.hdi(trace['kappas'][:,3])\n",
    "x = np.linspace(-1,12)\n",
    "y = trace['kappas'][:,3].mean()*x + trace['kappas'][:,2].mean()\n",
    "plt.plot(x,y, color='blue')\n",
    "plt.fill_between(y1=hdi_slope[0]*x + hdi_intercept[0], \n",
    "                 y2= hdi_slope[1]*x + hdi_intercept[1],\n",
    "                 x=x, alpha=0.15, color='blue');\n",
    "plt.ylabel(\"Emotion-Change Rating\", fontsize=16); \n",
    "plt.xlabel(\"Missed Oppotunity\", fontsize=16);\n",
    "\n",
    "\n",
    "idx = Data['information_sought']==0\n",
    "plt.plot(diff_limit[idx], delta_emo_data[idx], '.r',  label = 'No info')\n",
    "hdi_intercept = az.hdi(trace['kappas'][:,0]) #high density interval\n",
    "hdi_slope = az.hdi(trace['kappas'][:,1])\n",
    "x = np.linspace(-1,12)\n",
    "y = trace['kappas'][:,1].mean()*x + trace['kappas'][:,0].mean()\n",
    "plt.plot(x,y, color='red')\n",
    "plt.fill_between(y1=hdi_slope[0]*x + hdi_intercept[0], \n",
    "                 y2= hdi_slope[1]*x + hdi_intercept[1],\n",
    "                 x=x, alpha=0.15, color='red');\n",
    "#plt.ylim(-250,250)\n",
    "plt.ylabel(\"Emotion-Change Rating\", fontsize=16); \n",
    "plt.xlabel(\"Missed Oppotunity\", fontsize=16);\n",
    "plt.legend(loc = 'upper left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On average parameters, risk attitudes change with \n",
    "# changes in emotion. But the effect size is tiny due to small zeta (see y scale).\n",
    "# The model suggests that risk attitudes don't change dramatically.\n",
    "# See appendix, for similar results with a hierarchical version (i.e. without logistics for gamma).\n",
    "# The hierarchical version didn't find dramatic consistency differences (beta).\n",
    "# In brief, either small changes in behavior explain FitzGibbon results, or\n",
    "# something else not accounted for in the model (e.g. a change in policy different than beta)\n",
    "# IMPORTANT: for all subjects in E1 including emotion change induced better waic and loo (see below). \n",
    "# i.e. is good to include it\n",
    "\n",
    "plt.figure(figsize=(5, 5)); \n",
    "emmmo_diff = np.linspace(-2,2,300)\n",
    "gammmma = gammaplus.mean()/(1+np.exp(-zeta.mean()*(emmmo_diff)))\n",
    "plt.plot(emmmo_diff,gammmma)\n",
    "plt.ylabel(\"$\\gamma^+$\", fontsize=16); \n",
    "plt.xlabel(\"Emotion-Change Rating\", fontsize=16);\n",
    "#plt.ylim(4.0850,4.0870)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Null model\n",
    "with pm.Model() as BART_CF_null:\n",
    "    #priors\n",
    "    kappas = pm.Uniform('kappas', \n",
    "                        lower = emo_change_scale[0], \n",
    "                        upper = emo_change_scale[1], shape = 4)\n",
    "    sigma_emo = pm.Uniform('sigma_emo', lower = 0 , upper = 50)\n",
    "    gammap = pm.Uniform('gammap', lower = 0 , upper = 10) \n",
    "    beta = pm.Normal(\"beta\", mu = 0.8, sd = 0.2) #Based on the previous BART\n",
    "    omega = pm.Deterministic(\"omega\", -gammap / np.log(1 - p))\n",
    "    \n",
    "    \n",
    "    #likelihood\n",
    "    mu_emo = (kappas[0] + kappas[1]*diff_limit)*no_info + (kappas[2] + kappas[3]*diff_limit)*info\n",
    "    delta_emoj = pm.Normal('delta_emo', mu = mu_emo, sigma = sigma_emo, \n",
    "                           observed = Data['emotion_change_rating'])\n",
    "    \n",
    "    thetaj = 1 - pm.math.invlogit(-beta * (npumps_next_trial - omega)) \n",
    "    \n",
    "    dj = pm.Binomial(\"dj\", p=thetaj, n=npump_max, observed=npumps_next_trial)\n",
    "\n",
    "    trace_null = pm.sample(tune=2000, target_accept = 0.99, cores=4, chains = 4)\n",
    "    data_null = az.from_pymc3(trace=trace_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(s)\n",
    "az.plot_trace(data_null, var_names=[\"gammap\", 'kappas', \"beta\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Model Comparison NOTE: run individual subjects for this cell (not enough ram).\n",
    "#https://discourse.pymc.io/t/calculating-waic-for-models-with-multiple-likelihood-functions/4834/5\n",
    "#The model comparison for individual subjects suggests that it is better to \n",
    "#include emotion as a modulator of risk (gamma)\n",
    "\n",
    "if s == 'all': #not enough ram!\n",
    "    print('OOOPS: due to ram limits, only individual subject models!!')    \n",
    "else:\n",
    "    data.sample_stats[\"log_likelihood\"] = data.log_likelihood['delta_emo'] + data.log_likelihood['dj']\n",
    "    data_null.sample_stats[\"log_likelihood\"] = data_null.log_likelihood['delta_emo'] + data_null.log_likelihood['dj']\n",
    "print(-2*az.waic(data)[0]) #lower values better\n",
    "print(-2*az.waic(data_null)[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = az.waic(data)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise\n",
    "\n",
    "<center><img src=\"img/6_CB/FitzGibbon10.png\" width = \"250\" height = '250'></center>\n",
    "\n",
    "1) There were 5 experiments in Fitzgibbon et al, 2021 paper. We just run data of Exp. 1. Run the previous model and analyses on Exp. 2 data (counterfactual information cost money). Before starting, do you expect different results? After running the model, are the results similar? Different?\n",
    "\n",
    "2) How would you extend the base diagram to a hierarchical one for all 5 experiments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align=\"center\" style = \"font-size: 70px\"> Prospect Theory </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Subjective value function prospect theory\n",
    "wAw = widgets.FloatSlider(min = 0, max = 1, step = 0.01, value=0.3,\n",
    "                        description = 'Alpha win')\n",
    "wAl = widgets.FloatSlider(min = 0, max = 1, step = 0.01, value=0.15,\n",
    "                        description = 'Alpha lose')\n",
    "wLA = widgets.FloatSlider(min = 0, max = 2, step = 0.01, value=2,\n",
    "                        description = 'Loss aversion')\n",
    "out = widgets.interactive_output(mf.slider_econ_beh_risk,\n",
    "                                 {'alpha_win': wAw,'alpha_lose': wAl, \n",
    "                                  'loss_aversion': wLA})\n",
    "left_widgets = VBox([wAw, wAl])\n",
    "right_widgets = VBox([wLA])\n",
    "top = HBox([left_widgets, right_widgets])\n",
    "VBox([top, out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\" style = \"font-size: 70px\"> Percepción de probabilidades </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Weighting function prospect theory\n",
    "wT = widgets.FloatSlider(min = 0, max = 2, step = 0.01, value=0.5,\n",
    "                        description = 'Theta')\n",
    "out = widgets.interactive_output(mf.slider_econ_beh_prob, {'theta': wT})\n",
    "VBox([wT, out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Algunos fenómenos psicológicos relacionados a probabilidades y riesgo\n",
    "* Optimismo (desmeritar riesgos negativos)\n",
    "* Gusto por el statu quo (decidir relativo a un referente)\n",
    "* Disponibilidad (prob. depende de recuerdos)\n",
    "* Hindsight (lo que ocurrió es lo probable)\n",
    "* Framing effects (la forma de presentar probabilidades importa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Los efectos se replican en varias poblaciones del mundo\n",
    "<center><img src=\"img/6_CB/Ruggeri1.svg\" width = \"700\" height = '700'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Y prospect theory puede explicar alrededor del 90% de problemas DESCRITOS (problemas experimentados es otra historia)\n",
    "<center><img src=\"img/6_CB/Ruggeri2.svg\" width = \"700\" height = '700'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Medición de framing effects \n",
    "#### Situacion 1:\n",
    "Imagine que le damos 1000 dólares. Ya son suyos ¿Qué opción prefiere?\n",
    " * 50% de ganar 1000 más, 50% de ganar 0 más\n",
    " * 100% de certeza gana 500 más"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Situación 2:\n",
    "Imagine que le damos 2000 dólares. Ya son suyos ¿Qué opción prefiere?\n",
    " * 50% de perder 1000, 50% de perder 0 \n",
    " * 100% de certeza pierde 500 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Ejercicio en clase y en grupos:\n",
    "\n",
    "Expliqué Framing Effects con teoría de prospectos. Use las gráficas/formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\" style = \"font-size: 70px\"> Bayes y Prospect Theory </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"img/6_CB/Nilsson1.png\" width = \"600\" height = '600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Cuál prefiere?\n",
    "<center><img src=\"exp/6_CB/img/GP_0.png\" width = \"500\" height = '500'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"exp/6_CB/img/GP_69.png\" width = \"500\" height = '500'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"exp/6_CB/img/GP_138.png\" width = \"500\" height = '500'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hagamos el experimento en Psychopy (Notebooks_Slides/exp/6_CB/CPT.psyexp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Intro:\n",
    "* Un acercamiento no bayesiano, como MLE, necesita muchas observaciones por individuo.\n",
    "* Un acercamiento no jerárquico asume que los individuos son independientes. Sin embargo, los humanos compartimos sesgos.\n",
    "* MLE obtiene estimativos puntuales. Con Bayes obtenemos toda la distribución. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Formulas de prospect theory\n",
    "\n",
    "Valor:\n",
    "\n",
    "$$\n",
    "v(x) = \\left\\{\n",
    "\\begin{aligned}\n",
    "    x^\\alpha \\; \\;\\; \\; &\\text{if} \\;\\; x\\ge 0 \\\\\n",
    "   -\\lambda(-x^\\beta)\\; \\;\\; \\; & \\text{if} \\;\\;  x< 0\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "<center><img src=\"img/6_CB/PT_val.svg\" width = \"250\" height = '250'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Probabilidades:\n",
    "$$ w(p_x) = \\frac{p_x^c}{(p_x^c - (1-p_x^c))^{1/c}}$$\n",
    "\n",
    "$$c = \\gamma \\text{ if gain, } c = \\delta \\text{ if loss}$$\n",
    "\n",
    "<center><img src=\"img/6_CB/PT_weight.svg\" width = \"250\" height = '250'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Valor Esperado\n",
    "$$V(x) = v(x)w(p_x)$$\n",
    "\n",
    "Decisión estocástica\n",
    "$$ p(A) = \\frac{1}{1+e^{\\phi(V(B)-V(A))}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Ejercicio en clase y en grupos:\n",
    "\n",
    "1. Ponga las formulas de prospect theory en un DAG (directed acyclical graph). Haga el DAG en papel y lapiz. Más abajo está una versión jerárquica pero no lo use. Este punto todo el mundo lo va a tener bien. El objetivo es que piense cómo expresar formulas en un DAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/6_CB/model_CPT.svg\" width = \"601\" height = '600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Nilsson et al encontraron que los indices de aversión al riesgo ($\\alpha$, $\\beta$) podían dejarse iguales para poder estimar aversión a las perdidas (más detalles en el paper de ellos). Vamos a estimar ese modelo restringido.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<center><img src=\"img/6_CB/Nilsson_Table1.svg\" width = \"901\" height = '900'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#REAL DATA. Load data and exp. info.\n",
    "gambles_A = pd.read_table(\"data/6_CB/GambleA.txt\", header=None)\n",
    "gambles_A.columns = ['Reward_1', 'Prob_1', 'Reward_2', 'Prob_2']\n",
    "gambles_A_win = gambles_A.loc[0:59,:].copy()\n",
    "gambles_A_loss = gambles_A.loc[60:119,:].copy()\n",
    "gambles_A_mix = gambles_A.loc[120:179,:].copy()\n",
    "gambles_B = pd.read_table(\"data/6_CB/GambleB.txt\", header=None)\n",
    "gambles_B.columns = ['Reward_1', 'Prob_1', 'Reward_2', 'Prob_2']\n",
    "gambles_B_win = gambles_B.loc[0:59,:].copy()\n",
    "gambles_B_loss = gambles_B.loc[60:119,:].copy()\n",
    "gambles_B_mix = gambles_B.loc[120:179,:].copy()\n",
    "Rieskamp_data = pd.read_table('data/6_CB/Rieskamp_data.txt', header=None) \n",
    "# 0: choice gamble A\n",
    "# 1: choice gamble B\n",
    "nsubjs_to_include = 10\n",
    "subjs_to_include = np.random.choice(a = Rieskamp_data.shape[1], \n",
    "                                    size = nsubjs_to_include, replace = False) #to improve speed of sampling during class hour\n",
    "Rieskamp_data = Rieskamp_data.iloc[:,subjs_to_include]\n",
    "Rieskamp_data_win = Rieskamp_data.loc[0:59,:].copy()\n",
    "Rieskamp_data_loss = Rieskamp_data.loc[60:119,:].copy()\n",
    "Rieskamp_data_mix = Rieskamp_data.loc[120:179,:].copy()\n",
    "ntrials = Rieskamp_data.shape[0]\n",
    "ntrials_by_type = int(ntrials/3)\n",
    "nsubj = Rieskamp_data.shape[1]\n",
    "print(ntrials, nsubj, Rieskamp_data.shape)\n",
    "#Rieskamp_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#SIMULATED DATA\n",
    "\n",
    "#CPT parameters\n",
    "alpha_sim = 0.88\n",
    "beta_sim = 0.88\n",
    "gamma_sim = 0.61\n",
    "delta_sim = 0.69\n",
    "lambda_sim = 2.25  \n",
    "luce_sim = 0.14 #0.04: high choice noise; 0.14: medium; 0.4: low \n",
    "\n",
    "#WIN TRIALS\n",
    "#Gamble A\n",
    "v = mf.value_fun(gambles_A_win[['Reward_1','Reward_2']], \n",
    "                 alpha_sim, beta_sim, lambda_sim)\n",
    "w = mf.weight_fun(gambles_A_win[['Reward_1','Reward_2']],\n",
    "                  gambles_A_win[['Prob_1','Prob_2']],\n",
    "                  gamma_sim, delta_sim)\n",
    "EV_A = mf.EV(v, w)\n",
    "\n",
    "#Gamble B\n",
    "v = mf.value_fun(gambles_B_win[['Reward_1','Reward_2']], \n",
    "                 alpha_sim, beta_sim, lambda_sim)\n",
    "w = mf.weight_fun(gambles_B_win[['Reward_1','Reward_2']],\n",
    "                  gambles_B_win[['Prob_1','Prob_2']],\n",
    "                  gamma_sim, delta_sim)\n",
    "EV_B = mf.EV(v, w)\n",
    "\n",
    "#Choice\n",
    "EV_both = pd.DataFrame(np.array([EV_A, EV_B]).transpose())\n",
    "choice_prob = np.tile(mf.choice_rule(EV_both,luce_sim),[nsubj,1]).transpose()\n",
    "choice_win = np.random.binomial(1,choice_prob).astype(int)\n",
    "\n",
    "\n",
    "#LOSS TRIALS\n",
    "#Gamble A\n",
    "v = mf.value_fun(gambles_A_loss[['Reward_1','Reward_2']], \n",
    "                 alpha_sim, beta_sim, lambda_sim)\n",
    "w = mf.weight_fun(gambles_A_loss[['Reward_1','Reward_2']],\n",
    "                  gambles_A_loss[['Prob_1','Prob_2']],\n",
    "                  gamma_sim, delta_sim)\n",
    "EV_A = mf.EV(v, w)\n",
    "\n",
    "#Gamble B\n",
    "v = mf.value_fun(gambles_B_loss[['Reward_1','Reward_2']], \n",
    "                 alpha_sim, beta_sim, lambda_sim)\n",
    "w = mf.weight_fun(gambles_B_loss[['Reward_1','Reward_2']],\n",
    "                  gambles_B_loss[['Prob_1','Prob_2']],\n",
    "                  gamma_sim, delta_sim)\n",
    "EV_B = mf.EV(v, w)\n",
    "\n",
    "#Choice\n",
    "EV_both = pd.DataFrame(np.array([EV_A, EV_B]).transpose())\n",
    "choice_prob = np.tile(mf.choice_rule(EV_both,luce_sim),[nsubj,1]).transpose()\n",
    "choice_loss = np.random.binomial(1,choice_prob).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "#MIX TRIALS\n",
    "#Gamble A\n",
    "v = mf.value_fun(gambles_A_mix[['Reward_1','Reward_2']], \n",
    "                 alpha_sim, beta_sim, lambda_sim)\n",
    "w = mf.weight_fun(gambles_A_mix[['Reward_1','Reward_2']],\n",
    "                  gambles_A_mix[['Prob_1','Prob_2']],\n",
    "                  gamma_sim, delta_sim)\n",
    "EV_A = mf.EV(v, w)\n",
    "\n",
    "#Gamble B\n",
    "v = mf.value_fun(gambles_B_mix[['Reward_1','Reward_2']], \n",
    "                 alpha_sim, beta_sim, lambda_sim)\n",
    "w = mf.weight_fun(gambles_B_mix[['Reward_1','Reward_2']],\n",
    "                  gambles_B_mix[['Prob_1','Prob_2']],\n",
    "                  gamma_sim, delta_sim)\n",
    "EV_B = mf.EV(v, w)\n",
    "\n",
    "#Choice\n",
    "EV_both = pd.DataFrame(np.array([EV_A, EV_B]).transpose())\n",
    "choice_prob = np.tile(mf.choice_rule(EV_both,luce_sim),[nsubj,1]).transpose()\n",
    "choice_mix = np.random.binomial(1,choice_prob).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#PyMC model\n",
    "def norm_cdf(x, mean=0, std=1):\n",
    "    return (1.0 + tt.erf((x-mean) / tt.sqrt(2.0*(std**2)))) / 2.0 #cdf; (x is a normal sample)\n",
    "    #return tt.sqrt(2)*tt.erfinv(2x-1) #Probit: inv. cdf standard norm (x is a prob.).   \n",
    "\n",
    "with pm.Model() as CPT:  \n",
    "    # Here priors for the hyperdistributions are defined:\n",
    "    ### alpha (risk attitude win)\n",
    "    mu_alpha_N = pm.Normal('mu_alpha_N', 0, 1)\n",
    "    sigma_alpha_N = pm.Uniform('sigma_alpha_N', 0, 5)\n",
    "    ### beta (risk attitude lose)\n",
    "    #mu_beta_N = pm.Normal('mu_beta_N', 0, 1)\n",
    "    #sigma_beta_N = pm.Uniform('sigma_beta_N', 0, 5)\n",
    "    ### gamma (non-linearity in prob. win)\n",
    "    mu_gamma_N = pm.Normal('mu_gamma_N', 0, 1)\n",
    "    sigma_gamma_N = pm.Uniform('sigma_gamma_N', 0, 5)\n",
    "    ### delta (non-linearity in prob. lose)\n",
    "    mu_delta_N = pm.Normal('mu_delta_N', 0, 1)\n",
    "    sigma_delta_N = pm.Uniform('sigma_delta_N', 0, 5)\n",
    "    ### lambda (loss aversion)\n",
    "    mu_l_lambda_N = pm.Uniform('mu_l_lambda_N', -2.3, 1.61)\n",
    "    sigma_l_lambda_N = pm.Uniform('sigma_l_lambda_N', 0, 1.13)\n",
    "    ### luce (temperature of softmax)\n",
    "    mu_l_luce_N = pm.Uniform('mu_l_luce_N', -2.3, 1.61)\n",
    "    sigma_l_luce_N = pm.Uniform('sigma_l_luce_N', 0, 1.13)\n",
    "    \n",
    "    ## We put group-level normal's on the individual parameters.\n",
    "    ## This models alpha, beta, gamma, and delta as probitized parameters. \n",
    "    ## That is, it models parameters on the probit scale and then \n",
    "    ## puts them back to the range 0-1 with the CDF.\n",
    "    ## Lambda and luce are positive and modeled in log scale.\n",
    "    ## Each participant has unique parameter-values: \n",
    "    ## alpha, beta, gamma, delta, lambda, and luce\n",
    "    alpha_N = pm.TruncatedNormal('alpha_N', mu_alpha_N, sigma_alpha_N,\n",
    "                                 lower = -3, upper = 3,\n",
    "                                 shape = nsubj)\n",
    "    #beta_N = pm.TruncatedNormal('beta_N', mu_beta_N, sigma_beta_N,\n",
    "    #                            lower = -3, upper = 3,\n",
    "    #                            shape = nsubj)\n",
    "    gamma_N = pm.TruncatedNormal('gamma_N', mu_gamma_N, sigma_gamma_N,\n",
    "                                 lower = -3, upper = 3,\n",
    "                                 shape = nsubj)\n",
    "    delta_N = pm.TruncatedNormal('delta_N', mu_delta_N, sigma_delta_N,\n",
    "                                 lower = -3, upper = 3,\n",
    "                                 shape = nsubj)\n",
    "    lambda_N = pm.Normal('lambda_N', mu_l_lambda_N, sigma_l_lambda_N,\n",
    "                        shape = nsubj)\n",
    "    luce_N = pm.Normal('luce_N', mu_l_luce_N, sigma_l_luce_N,\n",
    "                       shape = nsubj)\n",
    "    \n",
    "    ### Put everything in the desired scale\n",
    "    ## We use cdf to bound some parameters to be in 0-1\n",
    "    alpha = pm.Deterministic('alpha', norm_cdf(alpha_N))\n",
    "    #beta = pm.Deterministic('beta', norm_cdf(beta_N))\n",
    "    beta = pm.Deterministic('beta', alpha)\n",
    "    gamma = pm.Deterministic('gamma', norm_cdf(gamma_N))\n",
    "    delta = pm.Deterministic('delta', norm_cdf(delta_N))\n",
    "    ## We exp because we assume a log. scale\n",
    "    lambd = pm.Deterministic('lambbda', tt.exp(lambda_N))\n",
    "    luce = pm.Deterministic('luce', tt.exp(luce_N))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # It is now time to define how the model should be fit to data.\n",
    "    ############ WIN TRIALS ############\n",
    "    gambless_A = gambles_A_win\n",
    "    gambless_B = gambles_B_win\n",
    "    ##GAMBLE A\n",
    "    ## subjective value of outcomes x & y in gamble A\n",
    "    reward_1 = np.tile(np.array(gambless_A['Reward_1']),(nsubj,1)).transpose()\n",
    "    reward_2 = np.tile(np.array(gambless_A['Reward_2']),(nsubj,1)).transpose()\n",
    "    v_x_a = pm.Deterministic('v_x_a', reward_1**tt.tile(alpha,(ntrials_by_type,1)))\n",
    "    v_y_a = pm.Deterministic('v_y_a', reward_2**tt.tile(alpha,(ntrials_by_type,1)))\n",
    "    \n",
    "    ## subjective prob. of outcomes x & y in gamble A\n",
    "    prob_1 = np.tile(np.array(gambless_A['Prob_1']),(nsubj,1)).transpose()\n",
    "    prob_2 = np.tile(np.array(gambless_A['Prob_2']),(nsubj,1)).transpose()\n",
    "    z_a = pm.Deterministic('z_a', prob_1**tt.tile(gamma,(ntrials_by_type,1)) + prob_2**tt.tile(gamma,(ntrials_by_type,1)))\n",
    "    den_a = pm.Deterministic('den_a', z_a**(1/tt.tile(gamma,(ntrials_by_type,1))))\n",
    "    num_x_a = pm.Deterministic('num_x_a', prob_1**tt.tile(gamma,(ntrials_by_type,1)))\n",
    "    w_x_a =  pm.Deterministic('w_x_a', num_x_a / den_a)  \n",
    "    num_y_a = pm.Deterministic('num_y_a', prob_2**tt.tile(gamma,(ntrials_by_type,1)))\n",
    "    w_y_a =  pm.Deterministic('w_y_a', num_y_a / den_a) \n",
    "       \n",
    "    ##subjective value of gamble A\n",
    "    Vf_a = pm.Deterministic('Vf_a', w_x_a * v_x_a + w_y_a * v_y_a)\n",
    "   \n",
    "\n",
    "    #GAMBLE B\n",
    "    ## subjective value of outcomes x & y in gamble B\n",
    "    reward_1 = np.tile(np.array(gambless_B['Reward_1']),(nsubj,1)).transpose()\n",
    "    reward_2 = np.tile(np.array(gambless_B['Reward_2']),(nsubj,1)).transpose()\n",
    "    v_x_b = pm.Deterministic('v_x_b', reward_1**tt.tile(alpha,(ntrials_by_type,1)))\n",
    "    v_y_b = pm.Deterministic('v_y_b', reward_2**tt.tile(alpha,(ntrials_by_type,1)))\n",
    "    \n",
    "    ## subjective prob. of outcomes x & y in gamble B\n",
    "    prob_1 = np.tile(np.array(gambless_B['Prob_1']),(nsubj,1)).transpose()\n",
    "    prob_2 = np.tile(np.array(gambless_B['Prob_2']),(nsubj,1)).transpose()\n",
    "    z_b = pm.Deterministic('z_b', prob_1**tt.tile(gamma,(ntrials_by_type,1)) + prob_2**tt.tile(gamma,(ntrials_by_type,1)))\n",
    "    den_b = pm.Deterministic('den_b', z_b**(1/tt.tile(gamma,(ntrials_by_type,1))))\n",
    "    num_x_b = pm.Deterministic('num_x_b', prob_1**tt.tile(gamma,(ntrials_by_type,1)))\n",
    "    w_x_b =  pm.Deterministic('w_x_b', num_x_b / den_b)  \n",
    "    num_y_b = pm.Deterministic('num_y_b', prob_2**tt.tile(gamma,(ntrials_by_type,1)))\n",
    "    w_y_b =  pm.Deterministic('w_y_b', num_y_b / den_b)   \n",
    "    \n",
    "    ##subjective value of gamble B\n",
    "    Vf_b = pm.Deterministic('Vf_b', w_x_b * v_x_b + w_y_b * v_y_b)\n",
    "    \n",
    "    ## Difference in value\n",
    "    #print(den)\n",
    "    dv = pm.Deterministic('D', (Vf_a - Vf_b))\n",
    "    ##LIKELIHOOD \n",
    "    ## choice for gamble-pair is a Bernoulli-distribution \n",
    "    ## with p = binval \n",
    "    ## binval is luce's choice rule (akin to a softmax) \n",
    "    binval = pm.Deterministic('binval', 1/(1+tt.exp((tt.tile(luce,(ntrials_by_type,1))) * (dv)))) #prob. of B\n",
    "    datta = pm.Data(\"data_win\", np.array(choice_win)) \n",
    "    win_obs = pm.Bernoulli('win_obs', p = binval, observed = datta)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ############ LOSS TRIALS ############\n",
    "    gambless_A = gambles_A_loss\n",
    "    gambless_B = gambles_B_loss\n",
    "    ##GAMBLE A\n",
    "    ## subjective value of outcomes x & y in gamble A\n",
    "    reward_1 = np.tile(np.array(gambless_A['Reward_1']),(nsubj,1)).transpose()\n",
    "    reward_2 = np.tile(np.array(gambless_A['Reward_2']),(nsubj,1)).transpose()\n",
    "    v_x_a_l = pm.Deterministic('v_x_a_l', (-1)*((-reward_1)**tt.tile(beta,(ntrials_by_type,1))))\n",
    "    v_y_a_l = pm.Deterministic('v_y_a_l', (-1)*((-reward_2)**tt.tile(beta,(ntrials_by_type,1))))\n",
    "\n",
    "    ## subjective prob. of outcomes x & y in gamble A\n",
    "    prob_1 = np.tile(np.array(gambless_A['Prob_1']),(nsubj,1)).transpose()\n",
    "    prob_2 = np.tile(np.array(gambless_A['Prob_2']),(nsubj,1)).transpose()\n",
    "    z_a_l = pm.Deterministic('z_a_l', prob_1**tt.tile(delta,(ntrials_by_type,1)) + prob_2**tt.tile(delta,(ntrials_by_type,1)))\n",
    "    den_a_l = pm.Deterministic('den_a_l', z_a_l**(1/tt.tile(delta,(ntrials_by_type,1))))\n",
    "    num_x_a_l = pm.Deterministic('num_x_a_l', prob_1**tt.tile(delta,(ntrials_by_type,1)))\n",
    "    w_x_a_l =  pm.Deterministic('w_x_a_l', num_x_a_l / den_a_l)  \n",
    "    num_y_a_l = pm.Deterministic('num_y_a_l', prob_2**tt.tile(delta,(ntrials_by_type,1)))\n",
    "    w_y_a_l =  pm.Deterministic('w_y_a_l', num_y_a_l / den_a_l) \n",
    "    \n",
    "    ##subjective value of gamble A\n",
    "    Vf_a_l = pm.Deterministic('Vf_a_l', w_x_a_l * v_x_a_l + w_y_a_l * v_y_a_l)\n",
    "    \n",
    "    \n",
    "    #GAMBLE B\n",
    "    ## subjective value of outcomes x & y in gamble B\n",
    "    reward_1 = np.tile(np.array(gambless_B['Reward_1']),(nsubj,1)).transpose()\n",
    "    reward_2 = np.tile(np.array(gambless_B['Reward_2']),(nsubj,1)).transpose()\n",
    "    v_x_b_l = pm.Deterministic('v_x_b_l', (-1)*((-reward_1)**tt.tile(beta,(ntrials_by_type,1))))\n",
    "    v_y_b_l = pm.Deterministic('v_y_b_l', (-1)*((-reward_2)**tt.tile(beta,(ntrials_by_type,1))))\n",
    "    \n",
    "    ## subjective prob. of outcomes x & y in gamble B\n",
    "    prob_1 = np.tile(np.array(gambless_B['Prob_1']),(nsubj,1)).transpose()\n",
    "    prob_2 = np.tile(np.array(gambless_B['Prob_2']),(nsubj,1)).transpose()\n",
    "    z_b_l = pm.Deterministic('z_b_l', prob_1**tt.tile(delta,(ntrials_by_type,1)) + prob_2**tt.tile(delta,(ntrials_by_type,1)))\n",
    "    den_b_l = pm.Deterministic('den_b_l', z_b_l**(1/tt.tile(delta, (ntrials_by_type,1))))\n",
    "    num_x_b_l = pm.Deterministic('num_x_b_l', prob_1**tt.tile(delta, (ntrials_by_type,1)))\n",
    "    w_x_b_l =  pm.Deterministic('w_x_b_l', num_x_b_l / den_b_l)  \n",
    "    num_y_b_l = pm.Deterministic('num_y_b_l', prob_2**tt.tile(delta, (ntrials_by_type,1)))\n",
    "    w_y_b_l =  pm.Deterministic('w_y_b_l', num_y_b_l / den_b_l)   \n",
    "\n",
    "    ##subjective value of gamble B\n",
    "    Vf_b_l = pm.Deterministic('Vf_b_l', w_x_b_l * v_x_b_l + w_y_b_l * v_y_b_l)\n",
    "    \n",
    "    ## Difference in value\n",
    "    #print(den)\n",
    "    dv_l = pm.Deterministic('D_l', (Vf_a_l - Vf_b_l))\n",
    "    ##likelihood \n",
    "    ## choice for gamble-pair is a Bernoulli-distribution \n",
    "    ## with p = binval \n",
    "    ## binval is luce's choice rule (akin to a softmax) \n",
    "    binval_l = pm.Deterministic('binval_l', 1/(1+tt.exp((tt.tile(luce,(ntrials_by_type,1))) * (dv_l)))) #prob. of B\n",
    "    datta_l = pm.Data(\"data_loss\", np.array(choice_loss))\n",
    "    loss_obs = pm.Bernoulli('loss_obs', p = binval_l, observed = datta_l)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ############ MIX TRIALS ############\n",
    "    gambless_A = gambles_A_mix\n",
    "    gambless_B = gambles_B_mix\n",
    "    ##GAMBLE A\n",
    "    ## subjective value of outcomes x & y in gamble A\n",
    "    reward_1 = np.tile(np.array(gambless_A['Reward_1']),(nsubj,1)).transpose()\n",
    "    reward_2 = np.tile(np.array(gambless_A['Reward_2']),(nsubj,1)).transpose()\n",
    "    v_x_a_m = pm.Deterministic('v_x_a_m', reward_1**tt.tile(alpha,(ntrials_by_type,1)))\n",
    "    v_y_a_m = pm.Deterministic('v_y_a_m', (-1*tt.tile(lambd,(ntrials_by_type,1)))*((-reward_2)**tt.tile(beta,(ntrials_by_type,1))))\n",
    "\n",
    "    ## subjective prob. of outcomes x & y in gamble A\n",
    "    prob_1 = np.tile(np.array(gambless_A['Prob_1']),(nsubj,1)).transpose()\n",
    "    prob_2 = np.tile(np.array(gambless_A['Prob_2']),(nsubj,1)).transpose()\n",
    "    z_a_m = pm.Deterministic('z_a_m', prob_1**tt.tile(gamma,(ntrials_by_type,1)) + prob_2**tt.tile(delta,(ntrials_by_type,1)))\n",
    "    den_a1_m = pm.Deterministic('den_a1_m', z_a_m**(1/tt.tile(gamma,(ntrials_by_type,1))))\n",
    "    den_a2_m = pm.Deterministic('den_a2_m', z_a_m**(1/tt.tile(delta,(ntrials_by_type,1))))\n",
    "    num_x_a_m = pm.Deterministic('num_x_a_m', prob_1**tt.tile(gamma,(ntrials_by_type,1)))\n",
    "    w_x_a_m =  pm.Deterministic('w_x_a_m', num_x_a_m / den_a1_m)  \n",
    "    num_y_a_m = pm.Deterministic('num_y_a_m', prob_2**tt.tile(delta,(ntrials_by_type,1)))\n",
    "    w_y_a_m =  pm.Deterministic('w_y_a_m', num_y_a_m / den_a2_m) \n",
    "    \n",
    "    ##subjective value of gamble A\n",
    "    Vf_a_m = pm.Deterministic('Vf_a_m', w_x_a_m * v_x_a_m + w_y_a_m * v_y_a_m)\n",
    "    \n",
    "    \n",
    "    ##GAMBLE B\n",
    "    ## subjective value of outcomes x & y in gamble B\n",
    "    reward_1 = np.tile(np.array(gambless_B['Reward_1']),(nsubj,1)).transpose()\n",
    "    reward_2 = np.tile(np.array(gambless_B['Reward_2']),(nsubj,1)).transpose()\n",
    "    v_x_b_m = pm.Deterministic('v_x_b_m', reward_1**tt.tile(alpha,(ntrials_by_type,1)))\n",
    "    v_y_b_m = pm.Deterministic('v_y_b_m', (-1*tt.tile(lambd,(ntrials_by_type,1)))*((-reward_2)**tt.tile(beta,(ntrials_by_type,1))))\n",
    "\n",
    "    ## subjective prob. of outcomes x & y in gamble B\n",
    "    prob_1 = np.tile(np.array(gambless_B['Prob_1']),(nsubj,1)).transpose()\n",
    "    prob_2 = np.tile(np.array(gambless_B['Prob_2']),(nsubj,1)).transpose()\n",
    "    z_b_m = pm.Deterministic('z_b_m', prob_1**tt.tile(gamma,(ntrials_by_type,1)) + prob_2**tt.tile(delta,(ntrials_by_type,1)))\n",
    "    den_b1_m = pm.Deterministic('den_b1_m', z_b_m**(1/tt.tile(gamma,(ntrials_by_type,1))))\n",
    "    den_b2_m = pm.Deterministic('den_b2_m', z_b_m**(1/tt.tile(delta,(ntrials_by_type,1))))\n",
    "    num_x_b_m = pm.Deterministic('num_x_b_m', prob_1**tt.tile(gamma,(ntrials_by_type,1)))\n",
    "    w_x_b_m =  pm.Deterministic('w_x_b_m', num_x_b_m / den_b1_m)  \n",
    "    num_y_b_m = pm.Deterministic('num_y_b_m', prob_2**tt.tile(delta,(ntrials_by_type,1)))\n",
    "    w_y_b_m =  pm.Deterministic('w_y_b_m', num_y_b_m / den_b2_m) \n",
    "    \n",
    "    ##subjective value of gamble B\n",
    "    Vf_b_m = pm.Deterministic('Vf_b_m', w_x_b_m * v_x_b_m + w_y_b_m * v_y_b_m)\n",
    "    \n",
    "        \n",
    "    ## Difference in value\n",
    "    #print(den)\n",
    "    dv_m = pm.Deterministic('D_m', (Vf_a_m - Vf_b_m))\n",
    "    ##likelihood \n",
    "    ## choice for gamble-pair is a Bernoulli-distribution \n",
    "    ## with p = binval \n",
    "    ## binval is luce's choice rule (akin to a softmax) \n",
    "    binval_m = pm.Deterministic('binval_m', 1/(1+tt.exp((tt.tile(luce,(ntrials_by_type,1))) * (dv_m)))) #prob. of B\n",
    "    datta_m = pm.Data(\"data_mix\", np.array(choice_mix))\n",
    "    mix_obs = pm.Bernoulli('mix_obs', p = binval_m, observed = datta_m)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##############  Sampling  ##############\n",
    "    trace = pm.sample(1000, tune = 1500, init='adapt_diag', target_accept = 0.95)\n",
    "    #step = pm.Metropolis()\n",
    "    #trace = pm.sample(50000, tune = 5000, step=step)\n",
    "    rhat = pm.rhat(trace, var_names = ['alpha', 'beta', 'gamma', 'delta', 'lambbda', 'luce'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Revisar convergencia. \n",
    "# Rhat <1.1 usualmente son aceptados como convergencia\n",
    "print(rhat.alpha[0:nsubj].mean())\n",
    "print(rhat.beta[0:nsubj].mean())\n",
    "print(rhat.gamma[0:nsubj].mean())\n",
    "print(rhat.delta[0:nsubj].mean())\n",
    "print(rhat.lambbda[0:nsubj].mean())\n",
    "print(rhat.luce[0:nsubj].mean())\n",
    "for RV in CPT.basic_RVs: #None should be inf or -inf\n",
    "    print(RV.name, RV.logp(CPT.test_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Visualizaciones de convergencia\n",
    "pars = [('mu_alpha_N','mu_alpha_N'), ('sigma_alpha_N','sigma_alpha_N'), \n",
    "        #('mu_beta_N','mu_beta_N'),('sigma_beta_N','sigma_beta_N'), \n",
    "        ('mu_gamma_N','mu_gamma_N'), ('sigma_gamma_N','sigma_gamma_N'),\n",
    "        ('mu_delta_N','mu_delta_N'), ('sigma_delta_N','sigma_delta_N'), \n",
    "        ('mu_l_lambda_N','mu_l_lambda_N'),('sigma_l_lambda_N','sigma_l_lambda_N'), \n",
    "        ('mu_l_luce_N','mu_l_luce_N'), ('sigma_l_luce_N','sigma_l_luce_N'),\n",
    "       ('alpha_N','alpha_N'), ('beta_N','beta_N'), ('gamma_N','gamma_N'),\n",
    "        ('delta_N','delta_N'), ('lambda_N','lambda_N'), ('luce_N','luce_N'), \n",
    "        ('alpha','alpha'), ('beta','beta'),\n",
    "        ('gamma','gamma'), ('delta','delta'), \n",
    "        ('lambda','lambbda'), ('luce','luce')]\n",
    "wD = widgets.Dropdown(options=pars,\n",
    "                        value='alpha', description='Param.: ')\n",
    "out = widgets.interactive_output(mf.dropdown_convergence,{'param': wD, \n",
    "                                                       'trace': fixed(trace)})\n",
    "VBox([wD, out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(['alpha',np.median(np.median(trace['alpha'], axis = 0)),\n",
    "       np.median(trace['alpha'], axis = 0).std(), alpha_sim]) #columns in trace are subjects, rows samples\n",
    "print(['beta',np.median(np.median(trace['beta'], axis = 0)),\n",
    "       np.median(trace['beta'], axis = 0).std(), beta_sim])\n",
    "print(['gamma',np.median(np.median(trace['gamma'], axis = 0)),\n",
    "       np.median(trace['gamma'], axis = 0).std(), gamma_sim])\n",
    "print(['delta',np.median(np.median(trace['delta'], axis = 0)),\n",
    "       np.median(trace['delta'], axis = 0).std(), delta_sim])\n",
    "print(['lambbda',np.median(np.median(trace['lambbda'], axis = 0)),\n",
    "       np.median(trace['lambbda'], axis = 0).std(), lambda_sim])\n",
    "print(['luce',np.median(np.median(trace['luce'], axis = 0)),\n",
    "       np.median(trace['luce'], axis = 0).std(), luce_sim])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Posterior plots of 6 CPT parameters (group-level)\n",
    "data = az.from_pymc3(trace=trace, model=CPT)\n",
    "fig, ax = plt.subplots(2,2, figsize = (8,8))\n",
    "az.plot_density([st.norm.cdf(trace['mu_alpha_N'])],\n",
    "                data_labels=['Alpha'],\n",
    "                hdi_prob=1, ax = ax[0,0]);\n",
    "az.plot_density([st.norm.cdf(trace['mu_gamma_N']),\n",
    "                st.norm.cdf(trace['mu_delta_N'])],\n",
    "                data_labels=['Gamma' , 'Delta'],\n",
    "                hdi_prob=1, ax = ax[0,1]);\n",
    "az.plot_density(np.exp(trace['mu_l_lambda_N']),\n",
    "                data_labels=['Lambda'],\n",
    "                hdi_prob=1, ax = ax[1,0]);\n",
    "az.plot_density(np.exp(trace['mu_l_luce_N']),\n",
    "                data_labels=['Luce'],\n",
    "                hdi_prob=1, ax = ax[1,1]);\n",
    "ax[0,0].set_title('Alpha \\n risk aversion')\n",
    "ax[0,1].set_title('Prob. non-linearities')\n",
    "ax[1,0].set_title('Lambda \\n loss aversion')\n",
    "ax[1,1].set_title('Luce \\n choice noise')\n",
    "plt.tight_layout()\n",
    "fig.savefig('img/6_CB/Nilsson_Fig3.svg')\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Now run the model with real data\n",
    "with CPT:\n",
    "    #Change data \n",
    "    pm.set_data({\"data_win\": np.array(Rieskamp_data_win),\n",
    "                 'data_loss': np.array(Rieskamp_data_loss),\n",
    "                 'data_mix': np.array(Rieskamp_data_mix)\n",
    "                })\n",
    "    \n",
    "    ##############  Sampling  ##############\n",
    "    trace = pm.sample(1000, tune = 1500, init='adapt_diag', target_accept = 0.95)\n",
    "    rhat = pm.rhat(trace, var_names = ['alpha', 'beta', 'gamma', 'delta', 'lambbda', 'luce'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Revisar convergencia. \n",
    "# Rhat <1.1 usualmente son aceptados como convergencia\n",
    "print(rhat.alpha[0:nsubj].mean())\n",
    "print(rhat.beta[0:nsubj].mean())\n",
    "print(rhat.gamma[0:nsubj].mean())\n",
    "print(rhat.delta[0:nsubj].mean())\n",
    "print(rhat.lambbda[0:nsubj].mean())\n",
    "print(rhat.luce[0:nsubj].mean())\n",
    "for RV in CPT.basic_RVs: #None should be inf or -inf\n",
    "    print(RV.name, RV.logp(CPT.test_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(['alpha',np.median(np.median(trace['alpha'], axis = 0)),\n",
    "       np.median(trace['alpha'], axis = 0).std()]) #columns in trace are subjects, rows samples\n",
    "print(['beta',np.median(np.median(trace['beta'], axis = 0)),\n",
    "       np.median(trace['beta'], axis = 0).std()])\n",
    "print(['gamma',np.median(np.median(trace['gamma'], axis = 0)),\n",
    "       np.median(trace['gamma'], axis = 0).std()])\n",
    "print(['delta',np.median(np.median(trace['delta'], axis = 0)),\n",
    "       np.median(trace['delta'], axis = 0).std()])\n",
    "print(['lambbda',np.median(np.median(trace['lambbda'], axis = 0)),\n",
    "       np.median(trace['lambbda'], axis = 0).std()])\n",
    "print(['luce',np.median(np.median(trace['luce'], axis = 0)),\n",
    "       np.median(trace['luce'], axis = 0).std()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Posterior plots of 6 CPT parameters (group-level)\n",
    "data = az.from_pymc3(trace=trace, model=CPT)\n",
    "fig, ax = plt.subplots(2,2, figsize = (8,8))\n",
    "az.plot_density([st.norm.cdf(trace['mu_alpha_N'])],\n",
    "                data_labels=['Alpha'],\n",
    "                hdi_prob=1, ax = ax[0,0]);\n",
    "az.plot_density([st.norm.cdf(trace['mu_gamma_N']),\n",
    "                st.norm.cdf(trace['mu_delta_N'])],\n",
    "                data_labels=['Gamma' , 'Delta'],\n",
    "                hdi_prob=1, ax = ax[0,1]);\n",
    "az.plot_density(np.exp(trace['mu_l_lambda_N']),\n",
    "                data_labels=['Lambda'],\n",
    "                hdi_prob=1, ax = ax[1,0]);\n",
    "az.plot_density(np.exp(trace['mu_l_luce_N']),\n",
    "                data_labels=['Luce'],\n",
    "                hdi_prob=1, ax = ax[1,1]);\n",
    "ax[0,0].set_title('Alpha \\n risk aversion')\n",
    "ax[0,1].set_title('Prob. non-linearities')\n",
    "ax[1,0].set_title('Lambda \\n loss aversion')\n",
    "ax[1,1].set_title('Luce \\n choice noise')\n",
    "plt.tight_layout()\n",
    "fig.savefig('img/6_CB/Nilsson_Fig4.svg')\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios\n",
    "\n",
    "Corra el modelo con sus datos e interprete los párametros obtenidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Conclusión\n",
    "\n",
    "* Medir riesgo en individuos con BART o loterias\n",
    "* Modelar hipótesis:\n",
    "    * FitzGibbon, et al, 2021, contrafactuales, emociones y riesgo\n",
    "    * Kahneman & Tversy, prospect theory:\n",
    "        * Dificultad en recuperar parametros (especificación de Nilsson, et al, 2011, igualaba preferencias de riesgo ($\\alpha=\\beta$) para recuperar aversión a perdidas ($\\lambda$)) \n",
    "* Jerárquico:\n",
    "    * Usa información de grupo\n",
    "* No jerárquico:\n",
    "    * Pooling (participantes son idénticos)\n",
    "    * Independence (participantes son únicos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to slides --SlidesExporter.reveal_theme='solarized' --SlidesExporter.reveal_transition='none' --SlidesExporter.reveal_scroll=True 6_Decisiones_Riesgo.ipynb #Saves slide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "Para salvar las diapositivas a PDF (en Chrome), correr nbconvert para que abra las diapositivas en un servidor local (la transition y el theme son opcionales):\n",
    "\n",
    "!jupyter nbconvert --to slides --SlidesExporter.reveal_theme='solarized' --SlidesExporter.reveal_transition='convex' nombre_de_mi_notebook.ipynb --post serve\n",
    "\n",
    "Luego, a la dirección añadirle ?print-pdf después del .html:\n",
    "\n",
    "http://127.0.0.1:8000/nombre_de_mi_notebook.slides.html?print-pdf\n",
    "\n",
    "Y luego, imprimir y darle salvar como pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Para salvar a pdf\n",
    "!jupyter nbconvert --to slides --SlidesExporter.reveal_theme='solarized' --SlidesExporter.reveal_transition='none' 6_Decisiones_Riesgo.ipynb --post serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Anexos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#BART\n",
    "dot_text = 'digraph G {rankdir=TB; compound=true; newrank=true; labelloc=\"t\";\\\n",
    "           label=\"BART\";\\\n",
    "           /* general properties*/\\\n",
    "           node [margin=0, fixedsize=true, shape=plaintext,\\\n",
    "                 height=0.7, width=0.7, lblstyle=\"font=\\\\small\"];\\\n",
    "           /* links */\\\n",
    "           burst_prob -> number_pumps;\\\n",
    "           risk_taking -> number_pumps;\\\n",
    "           number_pumps -> logistic;\\\n",
    "           beh_consist -> logistic;\\\n",
    "           logistic -> decision;\\\n",
    "           subgraph cluster0 {\\\n",
    "               margin = 10; labeljust=l; lblstyle=\"font=\\\\small\";\\\n",
    "               style = rounded;\\\n",
    "               label = \"$Choice_k$\";\\\n",
    "               subgraph cluster1 {\\\n",
    "                   margin = 10; labeljust=l; lblstyle=\"font=\\\\small\";\\\n",
    "                   style = rounded;\\\n",
    "                   label = \"$Trial_j$\";\\\n",
    "                   logistic;\\\n",
    "                   decision;\\\n",
    "               }\\\n",
    "           }\\\n",
    "           /* nodes */\\\n",
    "           burst_prob [label = \"$p$\", fillcolor = gray, style = filled, shape = circle];\\\n",
    "           number_pumps [label = \"$omega$\", shape = circle, peripheries = 2];\\\n",
    "           risk_taking [label = \"$gamma^{+}$\", shape = circle];\\\n",
    "           logistic [label = \"$theta_{jk}$\", shape = circle, peripheries = 2];\\\n",
    "           beh_consist [label = \"$beta$\", shape = circle];\\\n",
    "           decision [label = \"$d_{jk}$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           }' #warning: use single quote at start and end; double quotes for labels\n",
    "s = Source(dot_text, filename=\"img/6_CB/model_BART.gv\", format=\"svg\") #THIS IS NOT THE FINAL ONE\n",
    "s.view()\n",
    "\n",
    "#To typeset latex stuff on the image: \n",
    "#1) open svg in inkscape and write latex formulas. Export as pdf (click the one that says latex)\n",
    "#   to change fontsize of latex in inkscape write before the expression: \n",
    "#        \\fontsize{34pt}{1em} $latex expression$ ... change #pt for size\n",
    "#2) go to overleaf or latex editor of choice and do this (https://castel.dev/post/lecture-notes-2/):\n",
    "#   2.1) In the preamble:\n",
    "#  \\usepackage{import}\n",
    "#  \\usepackage{xifthen}\n",
    "#  \\usepackage{pdfpages}\n",
    "#  \\usepackage{transparent}\n",
    "#  \\usepackage{graphics} \n",
    "\n",
    "#  \\newcommand{\\incfig}[1]{%\n",
    "#      \\def\\svgwidth{\\columnwidth}\n",
    "#      \\import{./figures/}{#1.pdf_tex} %PUT the inkscape .pdf_tex AND .pdf in a local folder called figures\n",
    "#  }\n",
    "#   2.2)In the body:\n",
    "#  \\begin{figure}[ht]\n",
    "#      \\centering\n",
    "#      \\scalebox{.65}{\\incfig{your_inkscape.pdf_tex}} #change scalebox proportion to rescale\n",
    "#      \\caption{Riemmans theorem}\n",
    "#      \\label{fig:riemmans-theorem}\n",
    "#  \\end{figure}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#BART hierarchical\n",
    "dot_text = 'digraph G {rankdir=TB; compound=true; newrank=true; labelloc=\"t\";\\\n",
    "           label=\"BART\";\\\n",
    "           /* general properties*/\\\n",
    "           node [margin=0, fixedsize=true, shape=plaintext,\\\n",
    "                 height=0.7, width=0.7, lblstyle=\"font=\\\\small\"];\\\n",
    "           /* links */\\\n",
    "           burst_prob -> number_pumps;\\\n",
    "           risk_taking -> number_pumps;\\\n",
    "           mu_risk_taking -> risk_taking;\\\n",
    "           sigma_risk_taking -> risk_taking;\\\n",
    "           number_pumps -> logistic;\\\n",
    "           beh_consist -> logistic;\\\n",
    "           mu_beh_consist -> beh_consist;\\\n",
    "           sigma_beh_consist -> beh_consist;\\\n",
    "           logistic -> decision;\\\n",
    "           subgraph cluster0 {\\\n",
    "               margin = 10; labeljust=l; lblstyle=\"font=\\\\small\";\\\n",
    "               style = rounded;\\\n",
    "               label = \"$Conditions_i$\";\\\n",
    "               risk_taking;\\\n",
    "               beh_consist;\\\n",
    "               number_pumps;\\\n",
    "               subgraph cluster1 {\\\n",
    "                   margin = 10; labeljust=l; lblstyle=\"font=\\\\small\";\\\n",
    "                   style = rounded;\\\n",
    "                   label = \"$Choice_k$\";\\\n",
    "                   subgraph cluster2 {\\\n",
    "                       margin = 10; labeljust=l; lblstyle=\"font=\\\\small\";\\\n",
    "                       style = rounded;\\\n",
    "                       label = \"$Trial_j$\";\\\n",
    "                       logistic;\\\n",
    "                       decision;\\\n",
    "                   }\\\n",
    "               }\\\n",
    "           }\\\n",
    "           /* nodes */\\\n",
    "           burst_prob [label = \"$p$\", fillcolor = gray, style = filled, shape = circle];\\\n",
    "           number_pumps [label = \"$omega_i$\", shape = circle, peripheries = 2];\\\n",
    "           risk_taking [label = \"$gamma_i^{+}$\", shape = circle];\\\n",
    "           mu_risk_taking [label = \"$mu_{gamma^{+}}$\", shape = circle];\\\n",
    "           sigma_risk_taking [label = \"$sigma_{gamma^{+}}$\", shape = circle];\\\n",
    "           logistic [label = \"$theta_{ijk}$\", shape = circle, peripheries = 2];\\\n",
    "           beh_consist [label = \"$beta_i$\", shape = circle];\\\n",
    "           mu_beh_consist [label = \"$mu_{beta}$\", shape = circle];\\\n",
    "           sigma_beh_consist [label = \"$sigma_{beta}$\", shape = circle];\\\n",
    "           decision [label = \"$d_{ijk}$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           }' #warning: use single quote at start and end; double quotes for labels\n",
    "s = Source(dot_text, filename=\"img/6_CB/model_BART_hierarchical.gv\", format=\"svg\") #THIS IS NOT THE FINAL ONE\n",
    "s.view()\n",
    "\n",
    "#\\mu_{\\gamma^{+}} \\sim Uniform(0,10)\n",
    "#\\sigma_{\\gamma^{+}} \\sim Uniform(0,10)\n",
    "#\\mu_{\\beta} \\sim Uniform(0,10)\n",
    "#\\sigma_{\\beta} \\sim Uniform(0,10)\n",
    "#\\gamma_i^{+} \\sim Normal(\\mu_{\\gamma^{+}}, \\sigma_{\\gamma^{+}})\n",
    "#\\beta_i \\sim Normal(\\mu_{\\beta}, \\sigma_{\\beta})\n",
    "#\\omega_i = -frac{\\gamma_i^{+}}{log(1-p)}\n",
    "#\\theta_{ijk} = \\frac{1}{1 + e^{\\beta_i (k-\\omega_i)}}\n",
    "#\\d_{ijk} \\sim Bernoulli(\\theta_{ijk})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#CPT\n",
    "dot_text = 'digraph G {rankdir=TB; compound=true; newrank=true; labelloc=\"t\";\\\n",
    "           label=\"Cumulative Prospect Theory\";\\\n",
    "           /* general properties*/\\\n",
    "           node [margin=0, fixedsize=true, shape=plaintext,\\\n",
    "                 height=0.55, width=0.55, lblstyle=\"font=\\\\small\"];\\\n",
    "           /* links */\\\n",
    "           hyper_mu_sd -> alpha_normal;\\\n",
    "           hyper_mu_sd -> beta_normal;\\\n",
    "           hyper_mu_sd -> gamma_normal;\\\n",
    "           hyper_mu_sd -> delta_normal;\\\n",
    "           hyper_mu_sd_exp -> lambda_normal;\\\n",
    "           hyper_mu_sd_exp -> phi_normal;\\\n",
    "           subgraph cluster0 {\\\n",
    "               margin = 10; labeljust=l; lblstyle=\"font=\\\\small\";\\\n",
    "               style = rounded;\\\n",
    "               label = \" \"; texlbl = \"$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; i$ Individuo\";\\\n",
    "               alpha_normal -> alpha;\\\n",
    "               beta_normal -> beta;\\\n",
    "               gamma_normal -> gamma;\\\n",
    "               delta_normal -> delta;\\\n",
    "               phi_normal -> phi;\\\n",
    "               lambda_normal -> lambd;\\\n",
    "               alpha -> val;\\\n",
    "               lambd -> val;\\\n",
    "               beta -> val;\\\n",
    "               gamma -> w_prob;\\\n",
    "               delta -> w_prob;\\\n",
    "               phi -> choice;\\\n",
    "               subgraph cluster1 {\\\n",
    "                   margin = 10; labeljust=l; lblstyle=\"font=\\\\small\";\\\n",
    "                   style = rounded;\\\n",
    "                   label = \" \"; texlbl = \"$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; t$ Turno\";\\\n",
    "                   x -> val;\\\n",
    "                   x -> c;\\\n",
    "                   c -> w_prob;\\\n",
    "                   p -> w_prob;\\\n",
    "                   val -> EV;\\\n",
    "                   w_prob -> EV;\\\n",
    "                   EV -> choice;\\\n",
    "                   choice -> choice_data;\\\n",
    "               }\\\n",
    "           }\\\n",
    "           choice_data -> choice_data_dist -> hyper_mu_dist -> hyper_sd_dist -> hyper_mu_dist_exp -> hyper_sd_dist_exp -> alpha_normal_dist ->  alpha_dist -> beta_normal_dist -> beta_dist -> gamma_normal_dist -> gamma_dist -> delta_normal_dist -> delta_dist -> lambda_normal_dist -> lambda_dist -> phi_normal_dist -> phi_dist -> val_dist -> c_dist -> w_prob_dist -> EV_dist -> choice_dist [style = invis];\\\n",
    "           /* nodes */\\\n",
    "           hyper_mu_sd [texlbl = \"$(\\\\mu,\\\\sigma)$\", shape = circle];\\\n",
    "           hyper_mu_dist[texlbl = \"$\\\\mu \\sim Normal(0,1)$\"];\\\n",
    "           hyper_sd_dist [texlbl = \"$\\\\sigma \\sim Uniform(0,10)$\"];\\\n",
    "           hyper_mu_sd_exp [texlbl = \"$(\\\\mu_l,\\\\sigma_l)$\", shape = circle];\\\n",
    "           hyper_mu_dist_exp [texlbl = \"$\\\\mu_l \\sim Uniform(-2.3,1.61)$\"];\\\n",
    "           hyper_sd_dist_exp  [texlbl = \"$\\\\sigma_l \\sim Uniform(0,1.13)$\"];\\\n",
    "           alpha_normal [texlbl = \"$\\\\alpha_{N_i}$\", shape = circle];\\\n",
    "           alpha_normal_dist [texlbl = \"$\\\\alpha_{N_i} \\sim Normal(\\\\mu,\\\\sigma)[-3,3]$\"];\\\n",
    "           alpha [texlbl = \"$\\\\alpha_i$\", shape = circle, peripheries = 2];\\\n",
    "           alpha_dist  [texlbl = \"$\\\\alpha_i \\sim Std-Normal_{CDF}(\\\\alpha_{N_i})$\"];\\\n",
    "           beta_normal [texlbl = \"$\\\\beta_{N_i}$\", shape = circle];\\\n",
    "           beta_normal_dist [texlbl = \"$\\\\beta_{N_i} \\sim Normal(\\\\mu,\\\\sigma)[-3,3]$\"];\\\n",
    "           beta [texlbl = \"$\\\\beta_i$\", shape = circle, peripheries = 2];\\\n",
    "           beta_dist  [texlbl = \"$\\\\beta_i \\sim Std-Normal_{CDF}(\\\\beta_{N_i})$\"];\\\n",
    "           gamma_normal [texlbl = \"$\\\\gamma_{N_i}$\", shape = circle];\\\n",
    "           gamma_normal_dist [texlbl = \"$\\\\gamma_{N_i} \\sim Normal(\\\\mu,\\\\sigma)[-3,3]$\"];\\\n",
    "           gamma [texlbl = \"$\\\\gamma_i$\", shape = circle, peripheries = 2];\\\n",
    "           gamma_dist  [texlbl = \"$\\\\gamma_i \\sim Std-Normal_{CDF}(\\\\gamma_{N_i})$\"];\\\n",
    "           delta_normal [texlbl = \"$\\\\delta_{N_i}$\", shape = circle];\\\n",
    "           delta_normal_dist [texlbl = \"$\\\\delta_{N_i} \\sim Normal(\\\\mu,\\\\sigma)[-3,3]$\"];\\\n",
    "           delta [texlbl = \"$\\\\delta_i$\", shape = circle, peripheries = 2];\\\n",
    "           delta_dist  [texlbl = \"$\\\\delta_i \\sim Std-Normal_{CDF}(\\\\delta_{N_i})$\"];\\\n",
    "           lambda_normal [texlbl = \"$\\\\lambda_{N_i}$\", shape = circle];\\\n",
    "           lambda_normal_dist [texlbl = \"$\\\\lambda_{N_i} \\sim Normal(\\\\mu_l,\\\\sigma_l)$\"];\\\n",
    "           lambd [texlbl = \"$\\\\lambda_i$\", shape = circle, peripheries = 2];\\\n",
    "           lambda_dist  [texlbl = \"$\\\\lambda_i \\sim e^{\\\\lambda_{N_i}}$\"];\\\n",
    "           phi_normal [texlbl = \"$\\\\phi_{N_i}$\", shape = circle];\\\n",
    "           phi_normal_dist [texlbl = \"$\\\\phi_{N_i} \\sim Normal(\\\\mu_l,\\\\sigma_l)$\"];\\\n",
    "           phi [texlbl = \"$\\\\phi_i$\", shape = circle, peripheries = 2];\\\n",
    "           phi_dist  [texlbl = \"$\\\\phi_i \\sim e^{\\\\phi_{N_i}}$\"];\\\n",
    "           x [texlbl = \"$x_{it}$\", shape = circle, fillcolor = gray, style = filled];\\\n",
    "           p [texlbl = \"$p_{it}$\", shape = circle, fillcolor = gray, style = filled];\\\n",
    "           c [texlbl = \"$c_{it}$\", shape = circle];\\\n",
    "           c_dist [texlbl = \"$c_{it} = \\\\left\\\\lbrace \\\\parbox{7cm}{$\\\\gamma_i \\;\\;\\;\\;\\;\\; x_{it} \\ge 0$ \\\\\\ \\\\\\ $\\\\delta_i \\;\\;\\;\\;\\;\\; x_{it} <0 $} \\\\right.$\"];\\\n",
    "           w_prob [texlbl = \"$\\\\pi(p_{it})$\", shape = circle];\\\n",
    "           w_prob_dist [texlbl = \"$\\\\pi(p_{it})= \\\\frac{p_{it}^c}{(p_{it}^c-(1-p_{it}^c))^{1/c}}$\"];\\\n",
    "           val [texlbl = \"$v(x_{it})$\", shape = circle];\\\n",
    "           val_dist [texlbl = \"$v(x_{it}) = \\\\left\\\\lbrace \\\\parbox{7cm}{$x_{it}^{\\\\alpha_i} \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; x_{it} \\ge 0$ \\\\\\ \\\\\\ $-\\\\lambda_i(-x_{it})^{\\\\beta} \\;\\;\\;\\; x_{it} <0 $} \\\\right.$\"];\\\n",
    "           EV [texlbl = \"$V(O)$\", shape = circle, peripheries = 2];\\\n",
    "           EV_dist [texlbl = \"$V(O) = \\pi(p_{it_A})v(x_{it_A}) + \\pi(p_{it_B})v(x_{it_B})$\"];\\\n",
    "           choice [texlbl = \"$p_{it}(A,B)$\", shape = circle];\\\n",
    "           choice_dist [texlbl = \"$p_{it}(A,B) = \\\\frac{1}{1+e^{\\\\phi (V(B_t)-V(A_t))}}$\"];\\\n",
    "           choice_data [texlbl = \"$Choice_{it}$\", shape = square, style = filled, fillcolor = gray];\\\n",
    "           choice_data_dist [texlbl = \"$Choice_{it} \\sim Bernoulli(p_{it}(A,B))$\"];\\\n",
    "           }' #warning: use single quote at start and end; double quotes for labels\n",
    "tex = d2t.dot2tex(dot_text, format='tikz', preproc = True) #makes sure it looks good in tex\n",
    "tex = d2t.dot2tex(dot_text, texmode = 'verbatim', crop=True) #crop: the page size equal to the model\n",
    "diagram_tex = open('img/6_CB/model_CPT.tex', 'w')\n",
    "diagram_tex.write(tex) \n",
    "diagram_tex.close()\n",
    "\n",
    "# this builds a pdf-file inside a directory\n",
    "pdf = build_pdf(tex)\n",
    "pdf.save_to('img/6_CB/model_CPT.pdf') #convertir a svg y pulir/editar posiciones en inkscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# CPT images for trials\n",
    "# Gambles shown to participants. \n",
    "# A and B appeared on the left or right randomly\n",
    "gambles_A = pd.read_table(\"data/6_CB/GambleA.txt\", header=None)\n",
    "gambles_A.columns = ['Reward_1', 'Prob_1', 'Reward_2', 'Prob_2']\n",
    "gambles_B = pd.read_table(\"data/6_CB/GambleB.txt\", header=None)\n",
    "gambles_B.columns = ['Reward_1', 'Prob_1', 'Reward_2', 'Prob_2']\n",
    "ntrials = gambles_B.shape[0]\n",
    "file_address_psychopy = []\n",
    "for n in range(ntrials):\n",
    "    if n<60: #win trials\n",
    "        colors = ['#228B22','#20B2AA']\n",
    "        title = 'GANAR'\n",
    "    elif n>59 and n<120:\n",
    "        colors = [\"#8B0000\", '#FF0000']\n",
    "        title = 'PERDER'\n",
    "    else:\n",
    "        colors = [\"#228B22\", '#FF0000']\n",
    "        title = 'GANAR & PERDER'\n",
    "        \n",
    "    # Creating plot\n",
    "    fig = plt.figure(figsize=(12,5), constrained_layout=True)\n",
    "    spec = GridSpec(ncols=28, nrows=1, figure=fig)\n",
    "    ax0 = fig.add_subplot(spec[0, 0:1])\n",
    "    ax1 = fig.add_subplot(spec[0, 2:12])\n",
    "    ax2 = fig.add_subplot(spec[0, 13:14])\n",
    "    ax3 = fig.add_subplot(spec[0, 15:25])\n",
    "    ax4 = fig.add_subplot(spec[0, 26:27])\n",
    "    #fig, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=(9,3)) #ax1,ax2 refer to your two pies\n",
    "    \n",
    "    labels = gambles_A.loc[n,['Reward_1', 'Reward_2']]*1000\n",
    "    labels = [\"${:,.0f}\".format(labels[0]), \"${:,.0f}\".format(labels[1])]\n",
    "    values = gambles_A.loc[n,['Prob_1', 'Prob_2']]\n",
    "    _, texts, autotexts = ax1.pie(values, labels = labels,\n",
    "                              colors = colors,autopct = '%1.1f%%', radius=1.5) #plot first pie\n",
    "    [ _.set_fontsize(16) for _ in texts ]\n",
    "    [ _.set_fontsize(12) for _ in autotexts ]    \n",
    "    #ax1.axis('equal')\n",
    "    ax1.set_ylim(-1,2)\n",
    "    ax1.set_title(title, fontsize = 18)\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "\n",
    "    ax0.set_axis_off()\n",
    "    ax1.set_axis_off()\n",
    "    ax2.set_axis_off()\n",
    "    ax3.set_axis_off()\n",
    "    ax4.set_axis_off()\n",
    "\n",
    "    labels = gambles_B.loc[n,['Reward_1', 'Reward_2']]*1000\n",
    "    labels = [\"${:,.0f}\".format(labels[0]), \"${:,.0f}\".format(labels[1])]\n",
    "    values = gambles_B.loc[n,['Prob_1', 'Prob_2']]\n",
    "    _, texts, autotexts = ax3.pie(values, labels = labels, \n",
    "                              colors = colors,autopct = '%1.1f%%', radius=1.5) #plot first pie\n",
    "    [ _.set_fontsize(16) for _ in texts ]\n",
    "    [ _.set_fontsize(12) for _ in autotexts ]\n",
    "    ax3.set_ylim(-1,2)\n",
    "    #ax3.axis('equal')\n",
    "    ax3.set_title(title, fontsize = 18)\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "    file_address_psychopy.append('img/GP_' + str(n) + '.png')\n",
    "    fig.savefig('exp/6_CB/img/GP_' + str(n) + '.png')\n",
    "    plt.close()     \n",
    "#bbox_inches= \"tight\"\n",
    "f.to_csv('exp/6_CB/img_links.csv')\n",
    "      \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Hierarchical BART (FitzGibbon, et al, 2021)\n",
    "p = 0.5\n",
    "npump_max = 12 #from the Fitzgibbon experimental design\n",
    "#emo_change_scale = [-200,200] #from the Fitzgibbon paper\n",
    "emo_change_scale = [-3,3] #standardized (zscore; see cell above)\n",
    "delta_emo_data = np.array(Data['emotion_change_rating'])\n",
    "info = np.array(Data['information_sought'])\n",
    "info_boolean = np.array(info, dtype = 'bool')\n",
    "diff_limit = np.array(Data['diff_pumps_limit'])\n",
    "npumps_next_trial = np.array(Data['next_trial_n_pumps'])\n",
    "k = 2; #info and no info\n",
    "chains = 4\n",
    "with pm.Model() as BART_CF:\n",
    "    #priors\n",
    "    mu_g = pm.Uniform(\"mu_g\", lower = 0, upper = 10)\n",
    "    sigma_g = pm.Uniform(\"sigma_g\", lower = 0, upper = 10)\n",
    "    mu_b = pm.Uniform(\"mu_b\", lower = 0, upper = 1)\n",
    "    sigma_b = pm.Uniform(\"sigma_b\", lower = 0, upper = 3)\n",
    "    mu_intercept = pm.Uniform(\"mu_i\", lower = -3, upper = 3, shape = 2)\n",
    "    mu_slope = pm.Uniform(\"mu_s\", lower = -2, upper = 2, shape = 2)\n",
    "    sigma_intercept = pm.Uniform(\"sigma_i\", lower = 0, upper = 3, shape = 2)\n",
    "    sigma_slope = pm.Uniform(\"sigma_s\", lower = 0, upper = 3, shape = 2)\n",
    "    sigma_emo = pm.Uniform('sigma_emo', lower = 0 , upper = 3, shape = 2)\n",
    "\n",
    "    gammap = pm.Normal(\"gammap\", mu=mu_g, sd=sigma_g, shape=k)\n",
    "    beta = pm.Normal(\"beta\", mu=mu_b, sd=sigma_b, shape=k)\n",
    "\n",
    "    omega = -gammap[info] / np.log(1 - p)\n",
    "    \n",
    "    thetajk = 1 - pm.math.invlogit(-beta[info] * (npumps_next_trial - omega))\n",
    "    \n",
    "    intercept = pm.Normal('intercept', \n",
    "                          mu = mu_intercept, \n",
    "                          sd = sigma_intercept, shape = 2)\n",
    "    slope = pm.Normal('slope', \n",
    "                      mu = mu_slope, \n",
    "                      sd = sigma_slope, shape = 2)\n",
    "    \n",
    "    #likelihood\n",
    "    mu_emo = intercept[0] + slope[0]*diff_limit[~info_boolean]\n",
    "    delta_emo_no_info = pm.Normal('delta_emo_no_info', mu = mu_emo, sigma = sigma_emo[0], \n",
    "                                  observed = delta_emo_data[~info_boolean])\n",
    "    \n",
    "    mu_emo = intercept[1] + slope[1]*diff_limit[info_boolean]\n",
    "    delta_emo_info = pm.Normal('delta_emo_info', mu = mu_emo, sigma = sigma_emo[1], \n",
    "                           observed = delta_emo_data[info_boolean])\n",
    "    \n",
    "    dj = pm.Binomial(\"dj\", p=thetajk, n=npump_max, observed=npumps_next_trial)\n",
    "    \n",
    "    #get starting values with variational inference\n",
    "    approx = pm.fit(\n",
    "        n=10000, method=\"advi\", obj_optimizer=pm.adagrad_window\n",
    "    )  # type: pm.MeanField\n",
    "    start = approx.sample(draws=chains, include_transformed=True)\n",
    "    #sample\n",
    "    trace = pm.sample(\n",
    "         tune=2000, target_accept=0.99, chains=chains, cores = 4, \n",
    "        init=\"adapt_diag\", start=list(start)\n",
    "    )\n",
    "\n",
    "    data = az.from_pymc3(trace=trace)\n",
    "    ppc = pm.sample_posterior_predictive(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print(s)\n",
    "az.plot_trace(data, var_names=[\"gammap\", \"beta\", 'intercept', 'slope']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "az.plot_ppc(az.from_pymc3(posterior_predictive=ppc, model=BART_CF));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#The model seems to qualitatively predict the data fairly well\n",
    "plt.figure(figsize=(5, 5)); \n",
    "idx = Data['information_sought']==1\n",
    "plt.plot(diff_limit[idx], delta_emo_data[idx], '.b',  label = 'Info')\n",
    "hdi_intercept = az.hdi(trace['intercept'][:,1]) #high density interval\n",
    "hdi_slope = az.hdi(trace['slope'][:,1])\n",
    "x = np.linspace(-1,12)\n",
    "y = trace['slope'][:,1].mean()*x + trace['intercept'][:,1].mean()\n",
    "plt.plot(x,y, color='blue')\n",
    "plt.fill_between(y1=hdi_slope[0]*x + hdi_intercept[0], \n",
    "                 y2= hdi_slope[1]*x + hdi_intercept[1],\n",
    "                 x=x, alpha=0.15, color='blue');\n",
    "plt.ylabel(\"Emotion-Change Rating\", fontsize=16); \n",
    "plt.xlabel(\"Missed Oppotunity\", fontsize=16);\n",
    "\n",
    "\n",
    "idx = Data['information_sought']==0\n",
    "plt.plot(diff_limit[idx], delta_emo_data[idx], '.r',  label = 'No info')\n",
    "hdi_intercept = az.hdi(trace['intercept'][:,0]) #high density interval\n",
    "hdi_slope = az.hdi(trace['slope'][:,0])\n",
    "x = np.linspace(-1,12)\n",
    "y = trace['slope'][:,0].mean()*x + trace['intercept'][:,0].mean()\n",
    "plt.plot(x,y, color='red')\n",
    "plt.fill_between(y1=hdi_slope[0]*x + hdi_intercept[0], \n",
    "                 y2= hdi_slope[1]*x + hdi_intercept[1],\n",
    "                 x=x, alpha=0.15, color='red');\n",
    "#plt.ylim(-250,250)\n",
    "plt.ylabel(\"Emotion-Change Rating\", fontsize=16); \n",
    "plt.xlabel(\"Missed Oppotunity\", fontsize=16);\n",
    "plt.legend(loc = 'upper left');"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "rise": {
   "chalkboard": {
    "color": [
     "rgb(250, 0, 0)",
     "rgb(0, 250, 250)"
    ]
   },
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "simple",
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
