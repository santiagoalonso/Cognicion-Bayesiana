{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f7e45b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on PyMC v5.1.0\n",
      "Running on Pytensor v2.10.1\n"
     ]
    }
   ],
   "source": [
    "#This session is mostly (99.9%) from pymc website: https://www.pymc.io/projects/examples/en/latest/case_studies/reinforcement_learning.html\n",
    "#Also in archive: https://archive.ph/Srci7\n",
    "\n",
    "import os\n",
    "#from IPython.display import HTML\n",
    "\n",
    "#Tables and matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Stats\n",
    "import scipy.stats as st\n",
    "from scipy.optimize import fmin\n",
    "from scipy import integrate\n",
    "from scipy.stats.mstats import mquantiles\n",
    "import statistics \n",
    "\n",
    "\n",
    "#Probabilistic programs\n",
    "#!pip install pymc==5.0.2\n",
    "#!pip install pytensor\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt\n",
    "import pytensor \n",
    "#import aesara.tensor as at\n",
    "print('Running on PyMC v{}'.format(pm.__version__))\n",
    "print('Running on Pytensor v{}'.format(pytensor.__version__))\n",
    "\n",
    "#Graphs\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import altair as alt\n",
    "#from altair_saver import save #ademas instalar en terminal: brew cask install chromedriver\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, HBox, VBox, Layout\n",
    "#from graphviz import Source, Digraph\n",
    "#import dot2tex as d2t\n",
    "#from latex import build_pdf\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import arviz as az\n",
    "\n",
    "#User-defined functions (in the same folder as the notebook)\n",
    "import my_fun as mf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986f1fda",
   "metadata": {},
   "source": [
    "# Reinforced Learning\n",
    "\n",
    "Santiago Alonso-DÃ­az"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef59976",
   "metadata": {},
   "source": [
    "<center><img src=\"img/11_CB/SuttonBarto.png\" width = \"600\" height = '600'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75ef0477",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"620\" height=\"540\" controls>\n",
       "  <source src=\"img/11_CB/ReinforcedLearningVideogames.mov\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"620\" height=\"540\" controls>\n",
    "  <source src=\"img/11_CB/ReinforcedLearningVideogames.mov\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a5d52b",
   "metadata": {},
   "source": [
    "<center><img src=\"img/11_CB/Mnih2015.png\" width = \"600\" height = '600'></center>\n",
    "Mnih et al, 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b096808",
   "metadata": {},
   "source": [
    "<center><img src=\"img/11_CB/Triqui1.png\" width = \"400\" height = '400'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec278d2",
   "metadata": {},
   "source": [
    "In this state S, there are only two available actions (G, H)\n",
    "<center><img src=\"img/11_CB/Triqui2.png\" width = \"400\" height = '400'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450c6398",
   "metadata": {},
   "source": [
    "Which of this states is more valuable under a random policy? Green/X plays\n",
    "<center><img src=\"img/11_CB/Triqui3.png\" width = \"600\" height = '600'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892ebd3b",
   "metadata": {},
   "source": [
    "One way to learn the value of a state (V(s)) after doing action (a) at time t+1 is reinforced learning? For instance,\n",
    "<br><br>\n",
    "$$V(s)_{t+1} = V(s)_{t} + \\alpha (V(s)_{t} - R(s)_{t+1}) $$\n",
    "<br>\n",
    "$\\alpha$: Learning rate\n",
    "$R$: Reward \n",
    "$t$: Time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e86d556",
   "metadata": {},
   "source": [
    "More generally, reinforced learning rules usually take this abstract form:\n",
    "\n",
    "$$ New \\; Estimate \\leftarrow Old \\; Estimate  + StepSize [Target - Old \\; Estimate] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc4a3a4",
   "metadata": {},
   "source": [
    "# Case: Multiarmed Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0a3cfd",
   "metadata": {},
   "source": [
    "<center><img src=\"img/11_CB/MAB1.png\" width = \"600\" height = '600'></center>\n",
    "Daw, et al, 2006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7bac1b",
   "metadata": {},
   "source": [
    "$q(a)$ is an idealized quantity i.e. the actual expected value of doing action $a$:\n",
    "$$q(a) \\dot{=} \\mathbb{E}[R_t | A_t = a] $$\n",
    "<br>\n",
    "One should pick the arm with the highest $q(a)$:\n",
    "$$Argmax_a q(a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468cf631",
   "metadata": {},
   "source": [
    "Problem: we do not know $q(a)$. Thus, we symbolize our current estimate at time $t$ as:\n",
    "$$ Q_t (a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f99eb6",
   "metadata": {},
   "source": [
    "One possibility for $Q_t(a)$ is the mean of the rewards $R$ up to time $t-1$ when choosing $a$:\n",
    "\n",
    "$$ Q_t(a) = \\frac{\\sum_{i=1}^{t-1}R_i*\\mathbb{1}_{A_i=a}}{\\sum_{i=1}^{t-1}\\mathbb{1}_{A_i=a}}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76bda0d8",
   "metadata": {},
   "source": [
    "We don't need perfect memory, we can obtain the average through reinforced learning:\n",
    "\n",
    "<center><img src=\"img/11_CB/MAB5.png\" width = \"600\" height = '600'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5467dd77",
   "metadata": {},
   "source": [
    "<H2 style=\"text-align: center;\">Dilemma</H2>\n",
    "<p style=\"text-align: center;\">Exploit ($Argmax_a Q_t(a)$)</p>\n",
    "<p style=\"text-align: center;\">vs</p>\n",
    "<p style=\"text-align: center;\">Explore (other non $Argmax$)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27917cc8",
   "metadata": {},
   "source": [
    "Algorithm examples:\n",
    "* Greedy ($Argmax_a Q_t(a)$)\n",
    "* $\\epsilon$-greedy ($Argmax_a Q_t(a)$ with probability $1-\\epsilon$, uniform with probability $\\epsilon$)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9a5c44",
   "metadata": {},
   "source": [
    "Which one to choose with a greedy policy?\n",
    "<center><img src=\"img/11_CB/MAB2.png\" width = \"500\" height = '500'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a863320",
   "metadata": {},
   "source": [
    "What is the probability of choosing 143 with $\\epsilon=0.5$?\n",
    "<center><img src=\"img/11_CB/MAB2.png\" width = \"500\" height = '500'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f20f4ee",
   "metadata": {},
   "source": [
    "The greedy algorithm only exploits, while $\\epsilon$-greedy allows for exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458602bd",
   "metadata": {},
   "source": [
    "Which policy is better? It depends\n",
    "<center><img src=\"img/11_CB/MAB3.png\" width = \"500\" height = '500'></center>\n",
    "Sutton & Barto, 2020\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77359a1",
   "metadata": {},
   "source": [
    "Some simulations:\n",
    "\n",
    "<center><img src=\"img/11_CB/MAB4.png\" width = \"500\" height = '500'></center>\n",
    "\n",
    "Sutton & Barto, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7397e9",
   "metadata": {},
   "source": [
    "Which policy is better? It depends ... e.g. on the variance of rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cb602c",
   "metadata": {},
   "source": [
    "A popular policy is to use a softmax function that maps a vector of real values to the interval [0,1]:\n",
    "\n",
    "$$Pr\\{A_t=a\\} = \\pi_t(a) \\dot{=}  \\frac{e^{H_t(a)}}{\\sum_{b=1}^k e^{H_t(b)}}$$\n",
    "\n",
    "$H_t(a)$: Strength of preference towards option $a$ (e.g. a linear transformation of $Q_t(a)$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c858f10",
   "metadata": {},
   "source": [
    "# Bayesian Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0536bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
