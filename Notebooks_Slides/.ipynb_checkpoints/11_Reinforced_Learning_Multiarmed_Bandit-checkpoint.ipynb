{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f0aa70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on PyMC v5.1.0\n",
      "Running on Pytensor v2.10.1\n"
     ]
    }
   ],
   "source": [
    "#This session is mostly (99.9%) from pymc website: https://www.pymc.io/projects/examples/en/latest/case_studies/reinforcement_learning.html\n",
    "#Also in archive: https://archive.ph/Srci7\n",
    "\n",
    "import os\n",
    "#from IPython.display import HTML\n",
    "\n",
    "#Tables and matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Stats\n",
    "import scipy.stats as st\n",
    "from scipy.optimize import fmin\n",
    "from scipy import integrate\n",
    "from scipy.stats.mstats import mquantiles\n",
    "import statistics \n",
    "\n",
    "\n",
    "#Probabilistic programs\n",
    "#!pip install pymc==5.0.2\n",
    "#!pip install pytensor\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt\n",
    "import pytensor \n",
    "#import aesara.tensor as at\n",
    "print('Running on PyMC v{}'.format(pm.__version__))\n",
    "print('Running on Pytensor v{}'.format(pytensor.__version__))\n",
    "\n",
    "#Graphs\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import altair as alt\n",
    "#from altair_saver import save #ademas instalar en terminal: brew cask install chromedriver\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, HBox, VBox, Layout\n",
    "#from graphviz import Source, Digraph\n",
    "#import dot2tex as d2t\n",
    "#from latex import build_pdf\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import arviz as az\n",
    "\n",
    "#User-defined functions (in the same folder as the notebook)\n",
    "import my_fun as mf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e4f9ea",
   "metadata": {},
   "source": [
    "# Reinforced Learning\n",
    "\n",
    "Santiago Alonso-DÃ­az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "445ac8c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"620\" height=\"540\" controls>\n",
       "  <source src=\"img/11_CB/ReinforcedLearningVideogames.mov\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"620\" height=\"540\" controls>\n",
    "  <source src=\"img/11_CB/ReinforcedLearningVideogames.mov\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2913cd",
   "metadata": {},
   "source": [
    "<center><img src=\"img/11_CB/Mnih2015.png\" width = \"600\" height = '600'></center>\n",
    "Mnih et al, 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d54059",
   "metadata": {},
   "source": [
    "<center><img src=\"img/11_CB/Triqui1.png\" width = \"400\" height = '400'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af1d757",
   "metadata": {},
   "source": [
    "In this state S, there are only two available actions (G, H)\n",
    "<center><img src=\"img/11_CB/Triqui2.png\" width = \"400\" height = '400'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb65c0d5",
   "metadata": {},
   "source": [
    "Which of this states is more valuable under a random policy? Green/X plays\n",
    "<center><img src=\"img/11_CB/Triqui3.png\" width = \"600\" height = '600'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bdde5b",
   "metadata": {},
   "source": [
    "One way to learn the value of a state (V(s)) after doing action (a) at time t+1 is reinforced learning? For instance,\n",
    "<br><br>\n",
    "$$V(s)_{t+1} = V(s)_{t} + \\alpha (V(s)_{t} - R(s)_{t+1}) $$\n",
    "<br>\n",
    "$\\alpha$: Learning rate\n",
    "$R$: Reward \n",
    "$t$: Time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e0eb2a",
   "metadata": {},
   "source": [
    "# Case: Multiarmed Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe3608a",
   "metadata": {},
   "source": [
    "<center><img src=\"img/11_CB/MAB1.png\" width = \"600\" height = '600'></center>\n",
    "Daw, et al, 2006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61df7bd6",
   "metadata": {},
   "source": [
    "$$q(a) \\dot{=} \\mathbb{E}[R_t | A_t = a] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feede029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
