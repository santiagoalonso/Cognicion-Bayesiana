{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on PyMC v3.11.4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "from __future__ import print_function\n",
    "\n",
    "#Tables and matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Stats\n",
    "import scipy.stats as st\n",
    "from scipy.optimize import fmin\n",
    "from scipy import integrate\n",
    "from scipy.stats.mstats import mquantiles\n",
    "import statistics \n",
    "import pyreadr\n",
    "import scipy.io as sio\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Probabilistic programs\n",
    "#!pip install pymc3\n",
    "#!pip install pymc==4.0.0b1\n",
    "import pymc3 as pm\n",
    "#import pymc as pm\n",
    "import theano.tensor as tt \n",
    "from theano.compile.ops import as_op\n",
    "print('Running on PyMC v{}'.format(pm.__version__))\n",
    "#import hddm\n",
    "#print (\"hddm version: \", hddm.__version__)\n",
    "\n",
    "#Graphs\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import altair as alt\n",
    "from altair_saver import save #ademas instalar en terminal: brew cask install chromedriver\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, HBox, VBox, Layout\n",
    "from graphviz import Source, Digraph\n",
    "import dot2tex as d2t\n",
    "from latex import build_pdf\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import arviz as az\n",
    "import colorsys\n",
    "\n",
    "# Image processing stuff\n",
    "#!pip install opencv-python\n",
    "import cv2\n",
    "\n",
    "#User-defined functions (in the same folder as the notebook)\n",
    "import my_fun as mf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision criterion\n",
    "\n",
    "Santiago Alonso-Díaz, PhD <br>\n",
    "Universidad Javeriana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Drift diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Enigma & Turing [recomended video](https://www.youtube.com/watch?v=G2_Q9FoD-oQ) <br><br>\n",
    "\n",
    "\n",
    "<center><img src=\"img/9_CB/DDM1.png\" width = \"801\" height = '800'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The code was probabilistic (pseudo-uniform, the Germans never used the same letter in the code and the message) IF the position of the rotors was not known\n",
    "\n",
    "<center><img src=\"img/9_CB/DDM2.png\" width = \"601\" height = '600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The machines were similar in how they produced random codes (mechanical system with rotors in initial positions; similar to random seeds today) <br> <br>\n",
    "\n",
    "<center><img src=\"img/9_CB/DDM5.png\" width = \"701\" height = '700'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Si dos códigos interceptados compartían letra en la misma posición era probable que fuera la misma maquina/posición de rotores. <br><br>\n",
    "\n",
    "<center><img src=\"img/9_CB/DDM3.png\" width = \"501\" height = '500'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Podemos acumular información ruidosa hasta un umbral para decidir si en efecto son la misma maquina (umbral superior) o diferente (umbral inferior) y empezar a decodificar <br><br>\n",
    "\n",
    "<center><img src=\"img/9_CB/DDM4.png\" width = \"501\" height = '500'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Qué tiene que ver esto con cognición bayesiana y decisiones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La actividad cerebral es ruidosa. Antes de decidir, LIP acumula actividad de MT hasta un umbral para confirmar. <br><br>\n",
    "\n",
    "<center><img src=\"img/9_CB/DDM6.svg\" width = \"701\" height = '700'></center>\n",
    "Roitman & Shadlen (2002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Los parametros usuales son drift rate, bound levels, starting point, and non-decision times. <br>\n",
    "En muchas versiones del DDM (no todas), se acumula el likelihood ratio (asume prior uniforme): $\\frac{p(evidencia|decisión \\ 1)}{p(evidencia|decisión \\ 2)}$ <br>\n",
    "En otras versiones, se acumula una variable de decision que se puede inferir dadas las acciones y tiempos de respuesta <br><br>\n",
    "\n",
    "\n",
    "<center><img src=\"img/9_CB/DDM7.svg\" width = \"801\" height = '800'></center>\n",
    "Shadlen & Kiani (2002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Cuál es el rojo más brillante? <br><br><br>\n",
    "<center><img src=\"img/9_CB/DDM8.svg\" width = \"501\" height = '500'></center>\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Cuál es el rojo más brillante? <br><br><br>\n",
    "<center><img src=\"img/9_CB/DDM9.svg\" width = \"501\" height = '500'></center>\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Cuál es el rojo más brillante? <br><br><br>\n",
    "<center><img src=\"img/9_CB/DDM10.svg\" width = \"501\" height = '500'></center>\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿En cuál turno fueron más lentos? En ese turno la evidencia se acumuló más lento (los otros parametros constantes) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El framework de acumulación de evidencia se ha aplicado en (reviews en Ratcliff, et al, 2016; Shadlen & Kiani, 2013):\n",
    "\n",
    "* Numerosidad\n",
    "* Memoria\n",
    "* Percepción de movimiento\n",
    "* Lenguaje\n",
    "* Categorización\n",
    "* Marketing\n",
    "* Altruismo\n",
    "* Descuento intertemporal\n",
    "* Otros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hay varios algoritmos para estimar los parámetros del modelo (Tabla de Shinn, et al, 2020). Acá nos centramos en HDDM que es bayesiano (Wiecki, et al, 2013). <br><br>\n",
    "\n",
    "<center><img src=\"img/9_CB/DDM11.svg\" width = \"601\" height = '600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "v: drift rate <br>\n",
    "a: decision threshold <br>\n",
    "z: starting point of accumulation <br>\n",
    "ndt: non-decision times (e.g. encoding) <br>\n",
    "$s_v, \\ s_z, \\ s_{ndt}$: inter-trial variability of parameters <br>\n",
    "x: response time and choice <br>\n",
    "\n",
    "\n",
    "<center><img src=\"img/9_CB/model_HDDM.svg\" width = \"601\" height = '600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Vamos a usar el paquete HDDM (Wiecki, Sofer, and Frank, 2013) y el demo en http://ski.clps.brown.edu/hddm_docs/tutorial_python.html\n",
    "\n",
    "El paquete está basado en PyMC. Sin embargo, requiere definiciones de densidades especiales para $x_{ji}$ por lo que mejor usamos el paquete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Usaremos el experimento de Cavanagh, et al, (2011). Subthalamic nucleus stimulation reverses mediofrontal influence over decision threshold.\n",
    "\n",
    "<center><img src=\"img/9_CB/Cav1.png\" width = \"601\" height = '600'></center>\n",
    "\n",
    "4 items. A: gana 100%; B: 0%; C: 75%; D: 25%. Probabilidades de ganar se aprenden via ensayo y error <br>\n",
    "4 tipos de turno:\n",
    "* High conflict WW win-win (A vs C); \n",
    "* High conflict LL lose-lose (B vs D); \n",
    "* Low-conflict WL (A vs D; C vs B). \n",
    "\n",
    "Hipótesis: High conflict trials aumentan el threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data = hddm.load_csv('data/9_CB/HDDM_data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Veamos los datos de los sujetos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data = hddm.utils.flip_errors(data) #pone los tiempos de respuesta de errores en negativo.\n",
    "\n",
    "fig = plt.figure(figsize = (10,6))\n",
    "ax = fig.add_subplot(111, xlabel='RT (s)', ylabel='count', title='RT distributions')\n",
    "for i, subj_data in data.groupby('subj_idx'):\n",
    "    subj_data.rt.hist(bins=20, histtype='step', ax=ax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ahora ajustar el modelo más simple donde los parametros no varían por condición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate model object passing it our data (no need to call flip_errors() before passing it).\n",
    "# This will tailor an individual hierarchical DDM around your dataset.\n",
    "#m = hddm.HDDM(data, include=('z', 'sv', 'st', 'sz')) #If you want to include inter-trial variability. SLOW\n",
    "m = hddm.HDDM(data)\n",
    "# find a good starting point which helps with the convergence.\n",
    "m.find_starting_values()\n",
    "# start drawing 7000 samples and discarding 5000 as burn-in\n",
    "m.sample(2000, burn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ahora veamos el parametro grupal umbral ($\\mu_a$) y su variabilidad ($\\sigma_a$), y su valor para un sujeto ($a_1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "stats = m.gen_stats()\n",
    "stats[stats.index.isin(['a', 'a_std', 'a_subj.0'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Ahora el plot de los traces, autocorrelaciones (si van a cero indica buen sampling), e histogramas de algunos parámetros\n",
    "m.plot_posteriors(['a', 't', 'v', 'a_std']) #group level parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Para correr varias cadenas hay que correr varias veces el comando. Con varias cadenas se puede calcular rhat (<1.1 convergio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "nchains = 4\n",
    "for i in range(nchains): \n",
    "    print(\"Cadena: \", i)\n",
    "    m = hddm.HDDM(data)\n",
    "    m.find_starting_values()\n",
    "    m.sample(5000, burn=20)\n",
    "    models.append(m)\n",
    "\n",
    "hddm.analyze.gelman_rubin(models) #rhat<1.1 convergio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Plot posterior predictive (red data, blue model) (HDDM plot_posterior_predictive function was not working)\n",
    "def _parents_to_random_posterior_sample(bottom_node, pos=None):\n",
    "    \"\"\"Walks through parents and sets them to pos sample.\"\"\"\n",
    "    for i, parent in enumerate(bottom_node.extended_parents):\n",
    "        #if not isinstance(parent, pm.Node): # Skip non-stochastic nodes\n",
    "        #    continue\n",
    "\n",
    "        if pos is None:\n",
    "            # Set to random posterior position\n",
    "            pos = np.random.randint(0, len(parent.trace()))\n",
    "\n",
    "        assert len(parent.trace()) >= pos, \"pos larger than posterior sample size\"\n",
    "        parent.value = parent.trace()[pos]\n",
    "        \n",
    "def _plot_posterior_pdf_node(bottom_node, axis, value_range=None, samples=10, bins=100):\n",
    "    \"\"\"Calculate posterior predictive for a certain bottom node.\n",
    "    :Arguments:\n",
    "        bottom_node : pymc.stochastic\n",
    "            Bottom node to compute posterior over. Bottom refers to individual level parameters.\n",
    "        axis : matplotlib.axis\n",
    "            Axis to plot into.\n",
    "        value_range : numpy.ndarray\n",
    "            Range over which to evaluate the likelihood.\n",
    "    :Optional:\n",
    "        samples : int (default=10)\n",
    "            Number of posterior samples to use.\n",
    "        bins : int (default=100)\n",
    "            Number of bins to compute histogram over.\n",
    "    \"\"\"\n",
    "\n",
    "    if value_range is None:\n",
    "        # Infer from data by finding the min and max from the nodes\n",
    "        #raise NotImplementedError(\"value_range keyword argument must be supplied.\")\n",
    "        value_range = np.linspace(-5, 5, 100) #in seconds\n",
    "\n",
    "    like = np.empty((samples, len(value_range)), dtype=np.float32)\n",
    "    for sample in range(samples):\n",
    "        _parents_to_random_posterior_sample(bottom_node) #this obtains random parameters (a, v, ndt)\n",
    "        # IMPORTANT ONE: Generate likelihood \n",
    "        like[sample,:] = bottom_node.pdf(value_range) #Basically, this is Navarro & Fuss (2009) likelihood\n",
    "\n",
    "    y = like.mean(axis=0) #The mean of the posterior predictive\n",
    "    try:\n",
    "        y_std = like.std(axis=0)\n",
    "    except FloatingPointError:\n",
    "        print(\"WARNING! %s threw FloatingPointError over std computation. Setting to 0 and continuing.\" % bottom_node.__name__)\n",
    "        y_std = np.zeros_like(y)\n",
    "\n",
    "    # Plot pp\n",
    "    axis.plot(value_range, y, label='post pred', color='b')\n",
    "    axis.fill_between(value_range, y-y_std, y+y_std, color='b', alpha=.8)\n",
    "\n",
    "    # Plot data\n",
    "    if len(bottom_node.value) != 0:\n",
    "        axis.hist(bottom_node.value.values, density=True, color='r',\n",
    "                  range=(value_range[0], value_range[-1]), label='data',\n",
    "                  bins=bins, histtype='step', lw=2.)\n",
    "\n",
    "    axis.set_ylim(bottom=0) # Likelihood and histogram can only be positive\n",
    "\n",
    "\n",
    "observeds = m.get_observeds()\n",
    "max_items = max([len(i[1]) for i in observeds.groupby('tag').groups.items()])\n",
    "columns = min(3, max_items)\n",
    "num_subjs = observeds.shape[0]\n",
    "for tag, nodes in observeds.groupby('tag'): #nodes are parameters e.g. bottom-node is a non-group level parameter\n",
    "    fig = plt.figure(figsize=(10,30))\n",
    "    #fig.suptitle(tag, fontsize=12)\n",
    "    fig.subplots_adjust(top=0.9, hspace=.4, wspace=.3)\n",
    "\n",
    "    nrows = num_subjs or len(nodes)/columns\n",
    "\n",
    "    if len(nodes) - int(nrows * columns) > 0:\n",
    "        nrows += 1\n",
    "\n",
    "    # Plot individual subjects (if present)\n",
    "    i = 0\n",
    "    for subj_i, (node_name, bottom_node) in enumerate(nodes.iterrows()):\n",
    "        i += 1\n",
    "        \n",
    "        ax = fig.add_subplot(int(np.ceil(nrows)), int(columns), int(subj_i+1))\n",
    "        if 'subj_idx' in bottom_node:\n",
    "            ax.set_title('Subj. ' + str(bottom_node['subj_idx']))\n",
    "\n",
    "        _plot_posterior_pdf_node(bottom_node['node'], ax)\n",
    "\n",
    "        if i >= np.ceil(nrows) * columns:\n",
    "            warnings.warn('Too many nodes. Consider increasing number of columns.')\n",
    "            break\n",
    "\n",
    "        if num_subjs is not None and i >= num_subjs:\n",
    "            break\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Con HDDM podemos calcular parametros por condicion (WW, LL, WL) usando la opción depends_on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "m_stim = hddm.HDDM(data, depends_on={'v': 'stim', 'a': 'stim'}) # en depends_on, el value del diccionario es la columna en la data con la condición experimental\n",
    "m_stim.find_starting_values()\n",
    "m_stim.sample(4500, burn=1000)\n",
    "\n",
    "#the sampled parameters are in .nodes_db.node. It is a pandas dataframe\n",
    "v_WW, v_LL, v_WL = m_stim.nodes_db.node[['v(WW)', 'v(LL)', 'v(WL)']]\n",
    "a_WW, a_LL, a_WL = m_stim.nodes_db.node[['a(WW)', 'a(LL)', 'a(WL)']]\n",
    "ndt = m_stim.nodes_db.node['t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hddm.analyze.plot_posterior_nodes([v_WW, v_LL, v_WL]);\n",
    "plt.xlabel('drift-rate');\n",
    "plt.ylabel('Posterior probability');\n",
    "plt.title('Posterior of drift-rate group-level means');\n",
    "\n",
    "plt.figure();\n",
    "hddm.analyze.plot_posterior_nodes([a_WW, a_LL, a_WL]);\n",
    "plt.xlabel('bounds');\n",
    "plt.ylabel('Posterior probability');\n",
    "plt.title('Posterior of bounds group-level means');\n",
    "\n",
    "\n",
    "plt.figure();\n",
    "hddm.analyze.plot_posterior_nodes([ndt]);\n",
    "plt.xlabel('non-decision times');\n",
    "plt.ylabel('Posterior probability');\n",
    "plt.title('Posterior of ndt group-level means');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Model comparison via DIC (lower is better)\n",
    "print(\"Lumped model DIC: %f\" % m.dic)\n",
    "print(\"Stimulus model DIC: %f\" % m_stim.dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El paquete HDDM también puede calcular los parámetros con información externa via regresiones (e.g. que el drift dependa de la edad o actividad cerebral). Detalles en https://github.com/hddm-devs/hddm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El paquete HDDM no simula, usa formulas, sin embargo veamos aproximádamente una simulación para obtener la variable de decisión (la traza). Esto puede ser útil, por ejemplo, para calcular confianza en la decisión.\n",
    "<br><br>\n",
    "<center><img src=\"img/9_CB/Wiecki1.svg\" width = \"601\" height = '600'></center>\n",
    "Imagen: Wiecki, et al, 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulated DV (decision variable)\n",
    "a = 20*a_WL.trace().mean() # bound\n",
    "v = v_WL.trace().mean() # drift\n",
    "t = ndt.trace().mean() # non-decision time\n",
    "n_ttrrials = 200\n",
    "sampling_rate = 1e-2 #arbitrary units \n",
    "\n",
    "#simulated_data, params = hddm.generate.gen_rand_data(params={'a': a, 'v': v, 't': t}, size=n_ttrrials) #No se estimo st, sv, sz\n",
    "#print([simulated_data['rt'].mean() - t, simulated_data['response'].mean(), v,a,t])\n",
    "\n",
    "z = np.random.uniform(-a/4,a/4)#Starting point\n",
    "DVdt = np.append(np.array([z]), np.random.normal(v*sampling_rate, 1, 3000))\n",
    "print(a, v, t, z, DV)\n",
    "DV = np.cumsum(DVdt)\n",
    "idx_below = np.argmax(DV<-a) #bottom threshold\n",
    "idx_above = np.argmax(DV>a) #upper threshold\n",
    "if min(idx_below, idx_above) == 0:\n",
    "    idx_choice_time = max(idx_below, idx_above)\n",
    "else:\n",
    "    idx_choice_time = min(idx_below, idx_above)\n",
    "choicee = 'correct'\n",
    "if idx_choice_time==0: #never passed a threshold\n",
    "    choicee = 'No threshold hit'\n",
    "    idx_choice_time = len(DV)-1\n",
    "elif idx_choice_time == idx_below: \n",
    "    choicee = 'wrong'\n",
    "    \n",
    "    \n",
    "DV = DV[0:idx_choice_time]\n",
    "#DV = DV[(DV<(a*sampling_rate)) & (DV>(-a*sampling_rate))]\n",
    "plt.plot(DV);\n",
    "print(v*sampling_rate, DV.shape[0], idx_choice_time, choicee)\n",
    "#DV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Definamos confianza como p(choice=correct|DV). Implementemos esta idea en un ejercicio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Ejercicio \n",
    "\n",
    "* Simule el DV de un turno correcto e infiera la probabilidad de que DV es mayor a cero. Es decir, asuma que su data es el vector DV y quiere calcular si el posterior del promedio de DV es mayor a cero. Use un likelihood normal y un prior uniforme para el promedio y desviación estándar de DV (recuerde que la desviación estándar debe ser mayor a cero i.e. el prior va de [0, K]). \n",
    "    * Bono: hacerlo de dos formas, a) con pymc3 y b) con la formula para la densidad de una posterior con likelihood normal y priors uniformes.  \n",
    "\n",
    "* Haga lo anterior (i.e. simule el DV) para 5 turnos WW, 5 turnos LL, y 5 turnos WL. ¿Cuál condición genera mayor confianza?\n",
    "\n",
    "* ¿Qué pasa si baja el umbral? Hagalo solo con turnos WL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Signal detection theory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Le pedimos a un par de personas que detecte varias veces si hay un tono/señal/voz del más alla en un ambiente lleno de ruido. <br><br>\n",
    "En la figura, Laurie dice \"sí\" más veces. ¿Es Laurie más sensible al tono/señal/voz del más alla que Chris?\n",
    "\n",
    "<center><img src=\"img/9_CB/Goldstein1.png\" width = \"501\" height = '500'></center>\n",
    "Fuente: Goldestein, 1996, Sensation & Perception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Laurie dice sí más y puede que genere más hits pero también más false alarms.<br><br>\n",
    "\n",
    "|              | Signal + Noise trial |    Noise Trial    |\n",
    "|--------------|:--------------------:|:-----------------:|\n",
    "| Yes response |          Hit         |    False alarm    |\n",
    "| No response  |         Miss         | Correct rejection |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "eje x: fuerza de actividad interna (supuesto: ruido produce actividad más débil)<br>\n",
    "$d$: distancia entre ruido y señal <br>\n",
    "$d/2$: criterio de decisión optimo si turnos de ruido y señal ocurren con igual probabilidad <br>\n",
    "$k$: criterio de decisión usado por la persona (e.g. si fuerza actividad>$k$, decir sí) <br>\n",
    "$c$: sesgo i.e. criterio usado ($k$) menos criterio óptimo ($d/2$)<br>\n",
    "$\\theta^h$: probabilidad de hit <br>\n",
    "$\\theta^f$: probabilidad de false alarm <br>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center><img src=\"img/9_CB/Lee_Wagenmakers1.svg\" width = \"501\" height = '500'></center>\n",
    "Fuente: Lee & Wagenmakers (2013)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Ejercicio\n",
    "\n",
    "* En la anterior figura, sombree el área que correspondería a \"Miss\" y cuál a \"Correct Rejection\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Volviendo a Laurie y Chris, hay por lo menos dos hipótesis para estos datos:\n",
    "* Laurie y Chris difieren en d (sensibilidad)\n",
    "* Laurie y Chris difieren en k (criterio)\n",
    "<br><br>\n",
    "<center><img src=\"img/9_CB/Goldstein1.png\" width = \"501\" height = '500'></center>\n",
    "Fuente: Goldestein, 1996, Sensation & Perception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "No solo aplica para fenómenos perceptuales. Cualquier toma de decisión dependerá de la discriminibilidad y umbrales que se pongan.\n",
    "\n",
    "<center><img src=\"img/9_CB/Krajbich1.svg\" width = \"451\" height = '450'></center>\n",
    "Fuente: Krajbich, et al, 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Krajbich, et al, 2015, de hecho encuentran que una vez controlando por discriminabilidad, la idea de thinking fast and slow (barras) desaparece (scatter plots).\n",
    "\n",
    "| Dictator Game | Intertemporal Discounting |\n",
    "|:-------------:|:-------------------------:|\n",
    "|      <center><img src=\"img/9_CB/Krajbich2.svg\" width = \"451\" height = '450'></center>      |        <center><img src=\"img/9_CB/Krajbich3.svg\" width = \"451\" height = '450'></center>       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "En un marco que integre valor esperado y signal detection theory, los criterios óptimos podrían considerar valor esperado (Lynn, Wormwood, Barret, & Quigley, 2015). Acá, la decisión de cicla o carro depende tanto del valor esperado del método de transporte y la probabilidad de lluvia.\n",
    "<br><br>\n",
    "\n",
    "<center><img src=\"img/9_CB/Lynn1.jpg\" width = \"551\" height = '550'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Veamos ahora cómo implementar un modelo simple de signal detection theory (SDT) en PyMC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$c_i:$ sesgo <br>\n",
    "$d_i:$ distancia entre las distribuciones señal y ruido <br>\n",
    "$\\theta_i^f:$ Tasa de false alarm <br>\n",
    "$\\theta_i^h:$ Tasa de hit rate <br>\n",
    "$h_i, \\ f_i:$ Hits and false alarms observados, respectivamente <br>\n",
    "$s_i, \\ n_i:$ Número de signal y noise trials, respectivamente <br><br>\n",
    "\n",
    "\n",
    "<center><img src=\"img/9_CB/model_SDT.svg\" width = \"551\" height = '550'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Modelos basados en https://github.com/junpenglao/Bayesian-Cognitive-Modeling-in-Pymc3\n",
    "#  Load data\n",
    "dataset = 1\n",
    "if dataset == 1:  # Demo\n",
    "    k = 3  # number of cases\n",
    "    data = np.array([70, 50, 30, 50, 7, 5, 3, 5, 10, 0, 0, 10]).reshape(k, -1)\n",
    "else:  # Lehrner et al. (1995) data\n",
    "    k = 3  # number of cases\n",
    "    data = np.array([148, 29, 32, 151, 150, 40, 30, 140, 150, 51, 40, 139]).reshape(\n",
    "        k, -1\n",
    "    )\n",
    "\n",
    "# Number of ...\n",
    "h = data[:, 0] #hits (each row is a subject)\n",
    "f = data[:, 1] #false alarm\n",
    "MI = data[:, 2] #miss\n",
    "CR = data[:, 3] #correct rejection\n",
    "s = h + MI #signal trials\n",
    "n = f + CR #noise trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def Phi(x):\n",
    "    #'Cumulative distribution function for the standard normal distribution'\n",
    "    # Also it is the probit transform\n",
    "    return 0.5 + 0.5 * pm.math.erf(x / pm.math.sqrt(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model1:\n",
    "    #Priors\n",
    "    di = pm.Normal(\"Discriminability\", mu=0, tau=0.5, shape=k) #d\n",
    "    ci = pm.Normal(\"Bias\", mu=0, tau=2, shape=k) #c\n",
    "\n",
    "    #Likelihood\n",
    "    thetah = pm.Deterministic(\"Hit Rate\", Phi(di / 2 - ci))\n",
    "    thetaf = pm.Deterministic(\"False Alarm Rate\", Phi(-di / 2 - ci))\n",
    "    \n",
    "    hi = pm.Binomial(\"hi\", p=thetah, n=s, observed=h)\n",
    "    fi = pm.Binomial(\"fi\", p=thetaf, n=n, observed=f)\n",
    "    \n",
    "    #Sampling\n",
    "    trace1 = pm.sample()\n",
    "    data = az.from_pymc3(trace=trace1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "az.plot_trace(\n",
    "    data, var_names=[\"Discriminability\", \"Bias\", \"Hit Rate\", \"False Alarm Rate\"], compact=True\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Ejercicios\n",
    "\n",
    "* ¿Qué significa qué los priors de discriminabilidad y sesgo estén centrados en cero? ¿Piensa que los priors de discriminabilidad y bias son los apropiados? Trate otros parámetros para la normal u otras formas funcionales y comente si observa (o no) cambios. \n",
    "\n",
    "* Analicé la siguiente tabla con signal detection theory (SDT). Use el modelo anterior de PyMC. En la tabla se muestra datos de un experimento con tres poblaciones y su capacidad de reconocer olores. Control: no COVID (n=18), Grupo 1: COVID severo (n=18), Grupo 2: COVID leve (n=19). El experimento era en tres fases. Primera: memorizar 10 olores. Segunda: esperar 15 minutos. Tercera: de 20 olores reconocer si son nuevos o viejos (10 eran nuevos, 10 viejos). La tabla tiene la suma agregada por grupo.\n",
    "    * ¿Cuál es la discriminabilidad y sesgo de cada grupo?\n",
    "    * ¿Qué conclusiones saca?\n",
    "    * ¿Se puede decir algo de diferencias individuales en un mismo grupo?\n",
    "\n",
    "|          |  Control |  Control | Group I  | Group I  | Group II | Group II |\n",
    "|----------|:--------:|:--------:|----------|----------|----------|----------|\n",
    "|          | Old odor | New odor | Old odor | New odor | Old odor | New odor |\n",
    "| Old resp | 148      | 29       | 150      | 40       | 150      | 51       |\n",
    "| New resp | 32       | 151      | 30       | 140      | 40       | 139      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ahora veamos una versión jerárquica de SDT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "En los modelos jerárquicos asumimos que los parámetros, en este caso discriminabilidad y sesgos, varían entre sujetos, y esa variación viene de una distribución poblacional, en este caso representada por los prior $\\mu_c, \\ \\mu_d, \\ \\lambda_c, \\ \\lambda_d$ <br><br>\n",
    "\n",
    "<center><img src=\"img/9_CB/model_hSDT.svg\" width = \"551\" height = '550'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La data viene de un experimento de Heit & Rotello (2005) que trata de evaluar la conjetura que la diferencia entre razonamiento inductivo (particular a general; Juan, Sofia, Heriberto, Karla, son mortales; todos los hombres son mortales) y deductivo (general a particular; todos los hombres son mortales; Pepe es mortal) se puede enmarcar en signal detection theory.\n",
    "\n",
    "Especificamente, la hipótesis es que el criterio de decision deductivo es mayor al inductivo (Rips, 2001)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Características del experimento:\n",
    "* 80 participantes.  \n",
    "* 8 argumentos\n",
    "* Diseño between-subject \n",
    "* Los del grupo inductivo (n=40) respondían preguntas tipo \"¿qué tan plausible?\"\n",
    "* Los del grupo deductivo (n=40) respondían preguntas tipo \"¿es necesariamente verdad?\"\n",
    "* Se contaron los hits, false alarms, miss, y correct rejections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Las preguntas siempre empezaban con: Asumiendo que las oraciones arriba de la línea son ciertas ... <br>\n",
    "En la condición inductiva las preguntas eran del tipo: ¿qué tan plausible? ¿qué tan convincente? <br>\n",
    "En la condición deductiva las preguntas eran del tipo: ¿Necesariamente implica? ¿Puede estar segura? <br>\n",
    "<center><img src=\"img/9_CB/rips1.svg\" width = \"501\" height = '500'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/9_CB/rips2.svg\" width = \"501\" height = '500'></center>\n",
    "Heit & Rotello (2005) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "std_d = pd.read_csv(\"data/9_CB/heit_rotello_std_d.csv\") # deduction data\n",
    "std_i = pd.read_csv(\"data/9_CB/heit_rotello_std_i.csv\") # induction data\n",
    "\n",
    "#Induction\n",
    "h1  = np.array(std_i['V1']) #hits\n",
    "f1  = np.array(std_i['V2']) #false alarm\n",
    "MI1 = np.array(std_i['V3']) #miss\n",
    "CR1 = np.array(std_i['V4']) #correct rejection\n",
    "s1 = h1 + MI1 #signal trials\n",
    "n1 = f1 + CR1 #noise trials\n",
    "\n",
    "#Deduction\n",
    "h2  = np.array(std_d['V1'])\n",
    "f2  = np.array(std_d['V2'])\n",
    "MI2 = np.array(std_d['V3'])\n",
    "CR2 = np.array(std_d['V4'])\n",
    "s2 = h2 + MI2\n",
    "n2 = f2 + CR2\n",
    "\n",
    "k = len(h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Inductive model\n",
    "with pm.Model() as model2i:\n",
    "    #Priors\n",
    "    mud = pm.Normal(\"mud\", mu=0, tau=0.001)\n",
    "    muc = pm.Normal(\"muc\", mu=0, tau=0.001)\n",
    "    lambdad = pm.Gamma(\"lambdad\", alpha=0.001, beta=0.001)\n",
    "    lambdac = pm.Gamma(\"lambdac\", alpha=0.001, beta=0.001)\n",
    "\n",
    "    di = pm.Normal(\"di\", mu=mud, tau=lambdad, shape=k)\n",
    "    ci = pm.Normal(\"ci\", mu=muc, tau=lambdac, shape=k)\n",
    "    \n",
    "    #Likelihood\n",
    "    thetah = pm.Deterministic(\"Hit Rate\", Phi(di / 2 - ci))\n",
    "    thetaf = pm.Deterministic(\"False Alarm Rate\", Phi(-di / 2 - ci))\n",
    "\n",
    "    hi = pm.Binomial(\"hi\", p=thetah, n=s1, observed=h1)\n",
    "    fi = pm.Binomial(\"fi\", p=thetaf, n=n1, observed=f1)\n",
    "    \n",
    "    trace_i = pm.sample(target_accept=0.95)\n",
    "    data_i = az.from_pymc3(trace=trace_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model2d:\n",
    "    mud = pm.Normal(\"mud\", mu=0, tau=0.001)\n",
    "    muc = pm.Normal(\"muc\", mu=0, tau=0.001)\n",
    "    lambdad = pm.Gamma(\"lambdad\", alpha=0.001, beta=0.001)\n",
    "    lambdac = pm.Gamma(\"lambdac\", alpha=0.001, beta=0.001)\n",
    "\n",
    "    di = pm.Normal(\"di\", mu=mud, tau=lambdad, shape=k)\n",
    "    ci = pm.Normal(\"ci\", mu=muc, tau=lambdac, shape=k)\n",
    "\n",
    "    thetah = pm.Deterministic(\"Hit Rate\", Phi(di / 2 - ci))\n",
    "    thetaf = pm.Deterministic(\"False Alarm Rate\", Phi(-di / 2 - ci))\n",
    "\n",
    "    hi = pm.Binomial(\"hi\", p=thetah, n=s2, observed=h2)\n",
    "    fi = pm.Binomial(\"fi\", p=thetaf, n=n2, observed=f2)\n",
    "    \n",
    "    trace_d = pm.sample(target_accept=0.95)\n",
    "    data_d = az.from_pymc3(trace=trace_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(18,6))\n",
    "az.plot_pair(data_i, var_names=[\"mud\", \"muc\"], scatter_kwargs={\"alpha\": 0.1, \"color\": \"red\"}, \n",
    "                    figsize = (8,8), ax = ax[0])\n",
    "az.plot_pair(data_d, var_names=[\"mud\", \"muc\"], scatter_kwargs={\"alpha\": 0.1, \"color\": \"green\"}, ax=ax[0])\n",
    "ax[0].set_xlim((0, 6))\n",
    "ax[0].set_ylim((-3, 2))\n",
    "ax[0].set_xlabel(r\"$\\mu_d$\")\n",
    "ax[0].set_ylabel(r\"$\\mu_c$\");\n",
    "az.plot_dist(trace_i['mud'], ax = ax[1], color = 'red', label = 'Inductive')\n",
    "az.plot_dist(trace_d['mud'], ax = ax[1], color = 'green', label = 'Deductive')\n",
    "ax[1].set_xlabel(r\"$\\mu_d$\", fontsize = 20);\n",
    "ax[1].set_title('Group-level discriminability')\n",
    "az.plot_dist(trace_i['muc'], ax = ax[2], color = 'red')\n",
    "az.plot_dist(trace_d['muc'], ax = ax[2], color = 'green')\n",
    "ax[2].set_xlabel(r\"$\\mu_c$\", fontsize = 20);\n",
    "ax[2].set_title('Group level criterion');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Ejercicios\n",
    "\n",
    "* Basados en la posterior del criterion ¿tiene sentido la conjetura que la deducción humana es simplemente requerir argumentos más fuertes?\n",
    "\n",
    "* Basados en la posterior de discriminability ¿qué tanto cambia su respuesta al punto anterior? ¿qué tan importante es para la conjetura que la discriminability fuera igual?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Hierarchical Drift Diffusion Model (HDDM)\n",
    "dot_text = 'digraph G {rankdir=TB; compound=true; newrank=true; labelloc=\"t\";\\\n",
    "           label=\"Hierarchical Drift Diffusion Model (HDDM)\";\\\n",
    "           /* general properties*/\\\n",
    "           node [margin=0, fixedsize=true, shape=plaintext,\\\n",
    "                 height=0.3, width=0.3, lblstyle=\"font=\\\\small\"];\\\n",
    "           /* links */\\\n",
    "           mu_v -> v;\\\n",
    "           sigma_v -> v;\\\n",
    "           mu_a -> a;\\\n",
    "           sigma_a -> a;\\\n",
    "           mu_z -> z;\\\n",
    "           sigma_z -> z;\\\n",
    "           mu_ndt -> ndt;\\\n",
    "           sigma_ndt -> ndt;\\\n",
    "           v -> x;\\\n",
    "           a -> x;\\\n",
    "           z -> x;\\\n",
    "           ndt -> x;\\\n",
    "           sv -> x;\\\n",
    "           sz -> x;\\\n",
    "           sndt -> x;\\\n",
    "           subgraph cluster0 {\\\n",
    "               margin = 5; labeljust=l; lblstyle=\"font=\\\\small\";\\\n",
    "               style = rounded;\\\n",
    "               label = \"$j subject$\";\\\n",
    "               v;\\\n",
    "               a;\\\n",
    "               z;\\\n",
    "               ndt;\\\n",
    "               subgraph cluster1 {\\\n",
    "                   margin = 5; labeljust=l; lblstyle=\"font=\\\\small\";\\\n",
    "                   style = rounded;\\\n",
    "                   label = \"$i trial$\";\\\n",
    "                   x;\\\n",
    "               }\\\n",
    "           }\\\n",
    "           /* nodes */\\\n",
    "           v [label = \"$v_j$\", shape = circle];\\\n",
    "           mu_v [label = \"$mu_v$\", shape = circle];\\\n",
    "           sigma_v [label = \"$sig_v$\", shape = circle];\\\n",
    "           a [label = \"$a_j$\", shape = circle];\\\n",
    "           mu_a [label = \"$mu_a$\", shape = circle];\\\n",
    "           sigma_a [label = \"$sig_a$\", shape = circle];\\\n",
    "           z [label = \"$z_j$\", shape = circle];\\\n",
    "           mu_z [label = \"$mu_z$\", shape = circle];\\\n",
    "           sigma_z [label = \"$sig_z$\", shape = circle];\\\n",
    "           ndt [label = \"$ndt_j$\", shape = circle];\\\n",
    "           mu_ndt [label = \"$mu_{ndt}$\", shape = circle];\\\n",
    "           sigma_ndt [label = \"$sig_{ndt}$\", shape = circle];\\\n",
    "           sv [label = \"$s_v$\", shape = circle];\\\n",
    "           sz [label = \"$s_z$\", shape = circle];\\\n",
    "           sndt [label = \"$s_{ndt}$\", shape = circle];\\\n",
    "           x [label = \"$x_{ji}$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           }' #warning: use single quote at start and end; double quotes for labels\n",
    "s = Source(dot_text, filename=\"img/9_CB/model_HDDM.gv\", format=\"svg\") #THIS IS NOT THE FINAL ONE\n",
    "s.view()\n",
    "\n",
    "#distributions:\n",
    "# \\mu_v \\sim Normal(2,3)\n",
    "# \\sigma_v \\sim Half \\ Normal(0,2)\n",
    "# \\mu_a \\sim Normal(1.5,0.75)\n",
    "# \\sigma_a \\sim Half \\ Normal(0,0.1)\n",
    "# \\mu_z \\sim Normal(0.5,0.5)\n",
    "# \\sigma_z \\sim Half \\ Normal(0,0.05)\n",
    "# \\mu_{ndt} \\sim Normal(0.4,0.2)\n",
    "# \\sigma_{ndt} \\sim Half \\ Normal(0,1)\n",
    "# s_v \\sim Half \\ Normal(0,2)\n",
    "# s_z \\sim Beta(1,3)\n",
    "# s_{ndt} \\sim Half \\ Normal(0,0.3)\n",
    "# a_j \\sim \\Gamma(\\mu_a, \\sigma_a^2)\n",
    "# v_j \\sim Normal(\\mu_v, \\sigma_v^2)\n",
    "# z_j \\sim invlogit(Normal(\\mu_z, \\sigma_z^2))\n",
    "# ndt_j \\sim Normal(\\mu_{ndt}, \\sigma_{ndt}^2)\n",
    "# x_{ji} \\sim \\frac{\\pi}{a_j^2} e^{\\left( -v_j a_j z_j - \\frac{v_j^2x}{2} \\right)} \\times \\sum_{k=1}^{\\infty} k e^{\\left(-\\frac{k^2\\pi^2x}{2a_j^2}\\right)} sin(k \\pi z_j)\n",
    "\n",
    "\n",
    "#To typeset latex stuff on the image: \n",
    "#1) open svg in inkscape and write latex formulas. Export as pdf (click the one that says latex)\n",
    "#   to change fontsize of latex in inkscape write before the expression: \n",
    "#        \\fontsize{34pt}{1em} $latex expression$ ... change #pt for size\n",
    "#2) go to overleaf or latex editor of choice and do this (https://castel.dev/post/lecture-notes-2/):\n",
    "#   2.1) In the preamble:\n",
    "#  \\usepackage{import}\n",
    "#  \\usepackage{xifthen}\n",
    "#  \\usepackage{pdfpages}\n",
    "#  \\usepackage{transparent}\n",
    "#  \\usepackage{graphics} \n",
    "\n",
    "#  \\newcommand{\\incfig}[1]{%\n",
    "#      \\def\\svgwidth{\\columnwidth}\n",
    "#      \\import{./figures/}{#1.pdf_tex} %PUT the inkscape .pdf_tex AND .pdf in a local folder called figures\n",
    "#  }\n",
    "#   2.2)In the body:\n",
    "#  \\begin{figure}[ht]\n",
    "#      \\centering\n",
    "#      \\scalebox{.65}{\\incfig{your_inkscape.pdf_tex}} #change scalebox proportion to rescale\n",
    "#      \\caption{Riemmans theorem}\n",
    "#      \\label{fig:riemmans-theorem}\n",
    "#  \\end{figure}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Signal detection theory\n",
    "dot_text = 'digraph G {rankdir=TB; compound=true; newrank=true; labelloc=\"t\";\\\n",
    "           label=\"Signal Detection Theory (SDT)\";\\\n",
    "           /* general properties*/\\\n",
    "           node [margin=0, fixedsize=true, shape=plaintext,\\\n",
    "                 height=0.3, width=0.3, lblstyle=\"font=\\\\small\"];\\\n",
    "           /* links */\\\n",
    "           c -> theta_h;\\\n",
    "           c -> theta_f;\\\n",
    "           d -> theta_h;\\\n",
    "           d -> theta_f;\\\n",
    "           theta_f -> f;\\\n",
    "           theta_h -> h;\\\n",
    "           s -> h;\\\n",
    "           n -> f;\\\n",
    "           subgraph cluster0 {\\\n",
    "               margin = 5; labeljust=l; lblstyle=\"font=\\\\small\";\\\n",
    "               style = rounded;\\\n",
    "               label = \"$i data sets$\";\\\n",
    "               c;\\\n",
    "               d;\\\n",
    "               s;\\\n",
    "               n;\\\n",
    "               h;\\\n",
    "               f;\\\n",
    "               theta_f;\\\n",
    "               theta_h;\\\n",
    "           }\\\n",
    "           /* nodes */\\\n",
    "           c [label = \"$c_i$\", shape = circle];\\\n",
    "           d [label = \"$d_i$\", shape = circle];\\\n",
    "           s [label = \"$s_i$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           n [label = \"$n_i$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           h [label = \"$h_i$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           f [label = \"$f_i$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           theta_f [label = \"$theta_i^f$\", shape = circle, peripheries = 2];\\\n",
    "           theta_h [label = \"$theta_i^h$\", shape = circle, peripheries = 2];\\\n",
    "           }' #warning: use single quote at start and end; double quotes for labels\n",
    "s = Source(dot_text, filename=\"img/9_CB/model_SDT.gv\", format=\"svg\") #THIS IS NOT THE FINAL ONE\n",
    "s.view()\n",
    "\n",
    "#d_i \\sim Normal(0, \\frac{1}{2})\n",
    "#c_i \\sim Normal(0,2)\n",
    "#\\theta_i^h = \\Phi(\\frac{1}{2}d_i - c_i)\n",
    "#\\theta_i^f = \\Phi(-\\frac{1}{2}d_i - c_i)\n",
    "#h_i \\sim Binomial(\\theta_i^h, s_i)\n",
    "#f_i \\sim Binomial(\\theta_i^f, n_i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Hierarchical signal detection theory\n",
    "dot_text = 'digraph G {rankdir=TB; compound=true; newrank=true; labelloc=\"t\";\\\n",
    "           label=\"Hierarchical Signal Detection Theory (SDT)\";\\\n",
    "           /* general properties*/\\\n",
    "           node [margin=0, fixedsize=true, shape=plaintext,\\\n",
    "                 height=0.3, width=0.3, lblstyle=\"font=\\\\small\"];\\\n",
    "           /* links */\\\n",
    "           lambda_c -> c;\\\n",
    "           mu_c -> c;\\\n",
    "           lambda_d -> d;\\\n",
    "           mu_d -> d;\\\n",
    "           c -> theta_h;\\\n",
    "           c -> theta_f;\\\n",
    "           d -> theta_h;\\\n",
    "           d -> theta_f;\\\n",
    "           theta_f -> f;\\\n",
    "           theta_h -> h;\\\n",
    "           s -> h;\\\n",
    "           n -> f;\\\n",
    "           subgraph cluster0 {\\\n",
    "               margin = 5; labeljust=l; lblstyle=\"font=\\\\small\";\\\n",
    "               style = rounded;\\\n",
    "               label = \"$i people$\";\\\n",
    "               c;\\\n",
    "               d;\\\n",
    "               s;\\\n",
    "               n;\\\n",
    "               h;\\\n",
    "               f;\\\n",
    "               theta_f;\\\n",
    "               theta_h;\\\n",
    "           }\\\n",
    "           /* nodes */\\\n",
    "           lambda_c [label = \"$lam_c$\", shape = circle];\\\n",
    "           lambda_d [label = \"$lam_d$\", shape = circle];\\\n",
    "           mu_c [label = \"$mu_c$\", shape = circle];\\\n",
    "           mu_d [label = \"$mu_d$\", shape = circle];\\\n",
    "           c [label = \"$c_i$\", shape = circle];\\\n",
    "           d [label = \"$d_i$\", shape = circle];\\\n",
    "           s [label = \"$s_i$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           n [label = \"$n_i$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           h [label = \"$h_i$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           f [label = \"$f_i$\", fillcolor = gray, style = filled, shape = square];\\\n",
    "           theta_f [label = \"$theta_i^f$\", shape = circle, peripheries = 2];\\\n",
    "           theta_h [label = \"$theta_i^h$\", shape = circle, peripheries = 2];\\\n",
    "           }' #warning: use single quote at start and end; double quotes for labels\n",
    "s = Source(dot_text, filename=\"img/9_CB/model_hSDT.gv\", format=\"svg\") #THIS IS NOT THE FINAL ONE\n",
    "s.view()\n",
    "\n",
    "#\\mu_c, \\ \\mu_d \\sim Normal(0,0.001)\n",
    "#\\lambda_c, \\ \\lambda_d \\sim Normal(0.001,0.001)\n",
    "#d_i \\sim Normal(\\mu_d, \\lambda_d)\n",
    "#c_i \\sim Normal(\\mu_c, \\lambda_c)\n",
    "#\\theta_i^h = \\Phi(\\frac{1}{2}d_i - c_i)\n",
    "#\\theta_i^f = \\Phi(-\\frac{1}{2}d_i - c_i)\n",
    "#h_i \\sim Binomial(\\theta_i^h, s_i)\n",
    "#f_i \\sim Binomial(\\theta_i^f, n_i)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "rise": {
   "chalkboard": {
    "color": [
     "rgb(250, 0, 0)",
     "rgb(0, 250, 250)"
    ]
   },
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "simple",
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
